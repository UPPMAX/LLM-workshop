{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ace267-148d-4962-8ab5-e7b819597463",
   "metadata": {},
   "source": [
    "# Excercise: RandomSearch with Optuna\n",
    "In this exercise we will make a very simple finetuning example of a tiny language model. The focus here is on hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58043a-38b7-42dd-bd15-9f09082625b3",
   "metadata": {},
   "source": [
    "## Base model\n",
    "We will use [roneneldan/TinyStories-1M](https://huggingface.co/roneneldan/TinyStories-1M), a 1 million parameter model trained to write short children stories in a simple English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d7206-9dc3-4372-9a9e-86918a630254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "def get_model_and_tokenizer() -> tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    model_name = \"roneneldan/TinyStories-1M\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17110c48-e8d4-4e7f-bd10-6a44eaf1c562",
   "metadata": {},
   "source": [
    "## Dataset for supervised finetuning\n",
    "A dataset has been prepared in the project directory called NamedTinyStories dataset. The dataset is based on asking a bigger TinyStories model generate stories based on a certain name which is pulled from the most popular names in the United States of the last century."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a3934-2dc2-4974-8276-8674641bc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import Sequence\n",
    "from collections import defaultdict\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def get_dataset() -> Dataset:\n",
    "    return load_from_disk(\"/mimer/NOBACKUP/groups/llm-workshop/datasets/NamedTinyStories/dataset\")\n",
    "\n",
    "def collate_fn(examples: list[dict], tokenizer: PreTrainedTokenizer):\n",
    "    newline_ids = tokenizer(\"\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "    batch = defaultdict(list)\n",
    "    for example in examples:\n",
    "        story_ids = tokenizer(example[\"story\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        name_ids = tokenizer(example[\"name\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        name_ids += [tokenizer.eos_token_id]\n",
    "\n",
    "        input_ids = story_ids + newline_ids + name_ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = [-100] * (len(story_ids) + len(newline_ids)) + name_ids\n",
    "\n",
    "\n",
    "        batch[\"input_ids\"].append(input_ids)\n",
    "        batch[\"attention_mask\"].append(attention_mask)\n",
    "        batch[\"labels\"].append(labels)\n",
    "\n",
    "    # Pad labels manually (unknown for tokenizer)\n",
    "    max_seq_len = max(len(l) for l in batch[\"labels\"])\n",
    "    batch[\"labels\"] = [[-100] * (max_seq_len - len(l)) + l for l in batch[\"labels\"]]\n",
    "    batch = tokenizer.pad(batch, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76826ee-742f-4659-86a5-2e5574b8cb22",
   "metadata": {},
   "source": [
    "## Add metrics of interest\n",
    "This is optional as we will be using the loss in this case, but cross entropy loss is not very human readable so let's add some metrics that can be easier to follow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981f786-597a-4627-b5a2-4266b14bbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction, TrainerCallback, pipeline\n",
    "\n",
    "class ComputeMetrics:\n",
    "    def __init__(self, batchwise: bool):\n",
    "        self.batchwise = batchwise\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def __call__(self, eval_pred: EvalPrediction, compute_result: bool = False):\n",
    "        logits, labels = eval_pred\n",
    "\n",
    "        # Get most likely prediction and shift to match prediction with label\n",
    "        preds = logits.argmax(dim=-1)[:, :-1]\n",
    "        labels = labels[:, 1:]\n",
    "\n",
    "        # Add to counters\n",
    "        self.correct += ((preds == labels) | (labels == -100)).all(dim=1).sum()\n",
    "        self.total += preds.size(0)\n",
    "        \n",
    "        if compute_result or not self.batchwise:\n",
    "            # Return result\n",
    "            accuracy = self.correct / self.total\n",
    "            self.reset()\n",
    "            return {\"accuracy\": float(accuracy)}\n",
    "\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7b225-2300-4174-b10f-d9e039d60775",
   "metadata": {},
   "source": [
    "## Enable Optuna pruning\n",
    "Optuna can optionally use pruning methods. For this work with HuggingFace trainer we will implement a callback that can abort a session based on the evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcf1fc-d721-4008-b544-5a380d5d607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import TrialPruned\n",
    "from optuna.trial import Trial\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class OptunaPruningCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that reports metrics to Optuna and enables pruning of unpromising trials.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial: Trial, metric_to_optimize: str):\n",
    "        super().__init__()\n",
    "        self.trial = trial\n",
    "        self.metric_to_optimize = metric_to_optimize\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Reports the main metric to Optuna and triggers pruning if necessary.\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            return control\n",
    "\n",
    "        value = metrics[self.metric_to_optimize]\n",
    "        step = int(state.global_step)\n",
    "        self.trial.report(value, step=step)\n",
    "\n",
    "        # Check if this trial should be pruned\n",
    "        if self.trial.should_prune():\n",
    "            # Raise Optuna's pruning exception to halt training gracefully\n",
    "            raise TrialPruned(f\"Trial was pruned at step {step}.\")\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636e8ae-5208-48cb-aa35-a6693da76239",
   "metadata": {},
   "source": [
    "## Specifying the objective of the hyperparameter optimization\n",
    "Now we want to have something to track when do our hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8ab77-9be2-447d-9063-acc50a2ca96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from optuna.trial import Trial\n",
    "from torch import Generator\n",
    "from torch.utils.data import random_split\n",
    "from transformers import PreTrainedModel, Trainer, TrainingArguments\n",
    "\n",
    "def finetune_stories(trial: Trial) -> float:\n",
    "    model, tokenizer = get_model_and_tokenizer()\n",
    "    \n",
    "    # HyperParameters\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    # Not tuned, but let's log them to Optuna anyway as they impact performance\n",
    "    trial.set_user_attr(\"batch_size\", 16)\n",
    "    trial.set_user_attr(\"train_dataset_size\", 0.8)\n",
    "    trial.set_user_attr(\"eval_dataset_size\", 0.2)\n",
    "    trial.set_user_attr(\"train_eval_split_seed\", 10371)\n",
    "    trial.set_user_attr(\"num_epochs\", 30)\n",
    "    trial.set_user_attr(\"metric_to_optimize\", \"eval_loss\")\n",
    "\n",
    "    # Initialize datasets\n",
    "    ds_rng = Generator(device=\"cpu\")\n",
    "    ds_rng.manual_seed(trial.user_attrs[\"train_eval_split_seed\"])\n",
    "    train_dataset, eval_dataset = random_split(\n",
    "        get_dataset(),\n",
    "        lengths=[\n",
    "            trial.user_attrs[\"train_dataset_size\"],\n",
    "            trial.user_attrs[\"eval_dataset_size\"],\n",
    "        ],\n",
    "        generator=ds_rng,\n",
    "    )\n",
    "    #train_dataset = NamedStoryDataset(tokenizer=tokenizer, length=trial.user_attrs[\"train_dataset_size\"])\n",
    "    #eval_dataset = NamedStoryDataset(tokenizer=tokenizer, length=trial.user_attrs[\"eval_dataset_size\"])\n",
    "    \n",
    "    # Define training parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{trial.study.study_name}_trial{trial.number:03d}\",\n",
    "        learning_rate=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        per_device_train_batch_size=trial.user_attrs[\"batch_size\"],\n",
    "        per_device_eval_batch_size=trial.user_attrs[\"batch_size\"],\n",
    "        dataloader_drop_last=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        batch_eval_metrics=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=trial.user_attrs[\"num_epochs\"],\n",
    "        remove_unused_columns=False,  # is done by custom collator instead\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        save_strategy=\"no\",\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,\n",
    "        report_to=[],\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=partial(collate_fn, tokenizer=tokenizer),\n",
    "        compute_metrics=ComputeMetrics(batchwise=training_args.batch_eval_metrics),\n",
    "        callbacks=[OptunaPruningCallback(trial, metric_to_optimize=trial.user_attrs[\"metric_to_optimize\"])],\n",
    "    )\n",
    "\n",
    "    # Run training\n",
    "    trainer.train()\n",
    "\n",
    "    # Get metric to optimize\n",
    "    best_metrics = sorted(\n",
    "        [\n",
    "            log_entry for log_entry in trainer.state.log_history\n",
    "            if isinstance(log_entry, dict)\n",
    "        ],\n",
    "        key=lambda metrics: metrics.get(trial.user_attrs[\"metric_to_optimize\"], float(\"inf\"))\n",
    "    )[0]\n",
    "    trial.set_user_attr(\"best_metrics\", best_metrics)\n",
    "    return best_metrics[trial.user_attrs[\"metric_to_optimize\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee1086-75a9-41ae-82e6-ca40d14263d8",
   "metadata": {},
   "source": [
    "## Run the hyperparameter optimization\n",
    "Only one thing left to do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab82a00-79a0-47af-b1ac-3073c968a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study, samplers, pruners\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "storage = JournalStorage(\n",
    "    JournalFileBackend(\"./optuna_journal_storage.log\"),\n",
    "    #JournalFileBackend(\"/mimer/NOBACKUP/groups/...\")\n",
    ")\n",
    "\n",
    "study = create_study(\n",
    "    study_name=\"NamedStories_v0.1\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=samplers.RandomSampler(),  # https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "    pruner=pruners.NopPruner(),  # https://optuna.readthedocs.io/en/stable/reference/pruners.html\n",
    "    direction=\"minimize\",\n",
    ")\n",
    "study.optimize(finetune_stories, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeaf0d6-e715-4da3-a3c7-c1aaef59f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is just to stop execution at this point if not running in Jupyter\n",
    "import sys\n",
    "\n",
    "from IPython import get_ipython\n",
    "from ipykernel.zmqshell import ZMQInteractiveShell\n",
    "\n",
    "if not isinstance(get_ipython(), ZMQInteractiveShell):\n",
    "    get_ipython().ask_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1da97-0fa7-48c4-8e90-c4549238d403",
   "metadata": {},
   "source": [
    "## Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d3f3a-edcc-4faf-a2cf-ad69203fec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import load_study\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "storage = JournalStorage(\n",
    "    JournalFileBackend(\"./optuna_journal_storage.log\"),\n",
    "    #JournalFileBackend(\"/mimer/NOBACKUP/groups/...\")\n",
    ")\n",
    "\n",
    "study = load_study(\n",
    "    study_name=\"NamedStories_v0.1\",\n",
    "    storage=storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e71850-f6ca-4517-b200-32a469d000f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284e2f5-a356-4b0a-a18e-26d6ee516e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "\n",
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5a885-aacf-4355-a8d0-0b36917a8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f6c30-d443-40ba-9803-d4e501d3f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(\"inf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

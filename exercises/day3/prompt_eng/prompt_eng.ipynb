{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf587844-1209-4324-946e-89d4e9bdc797",
   "metadata": {},
   "source": [
    "### Steps to Connect to an already running vLLM server:  \n",
    "We have to Local port-forward from this machine to the machine where vLLM is running:\n",
    "1. On a terminal login to alvis\n",
    "2. ssh to this machine, ie. where this notebook is running. say, `ssh alvis10-11`\n",
    "3. Perform local port-forwarding to vLLM serving machine (say `alvis5-09`): do the following:  \n",
    "   `ssh -L <vllm_api_port>:localhost:<vllm_api_port> alvis5-09`\n",
    "4. Now run the following cells\n",
    "\n",
    "For teacher: Run an vLLM server like so:\n",
    "```\n",
    "apptainer exec /apps/containers/vLLM/vllm-0.11.0.sif vllm serve /mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775 --port=$(find_ports) --host 0.0.0.0 > vllm.out 2> vllm.err &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1c997-52a5-49ee-bf3c-47f7028fc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and model wrapper\n",
    "import os, json, re, textwrap\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_model(prompt, temperature=0.0, max_tokens=300):\n",
    "    \"\"\"Gets response from a vLLM hosted model, given that it is already running on a port.\"\"\"\n",
    "\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    VLLM_PORT = 8000\n",
    "    openai_api_base = f\"http://localhost:{VLLM_PORT}/v1\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "\n",
    "    models = client.models.list()\n",
    "    model = models.data[0].id\n",
    "    STREAM = False\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=STREAM,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def score_response(response, expected_keywords):\n",
    "    \"\"\"Simple scoring: counts how many expected keywords appear in the returned text.\"\"\"\n",
    "    \n",
    "    text = response.lower()\n",
    "    hits = sum(1 for k in expected_keywords if k.lower() in text)\n",
    "    return hits, len(expected_keywords), hits/len(expected_keywords) if expected_keywords else 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee32c1-de7f-44df-94a1-f439c95d0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(field, technique_name, naive_prompt, engineered_prompt, expected_keywords, temperature=0.0):\n",
    "    print('\\n' + '='*80)\n",
    "    print(f\"Field: {field} â€” Technique: {technique_name}\\n\")\n",
    "    print('Naive prompt:\\n', naive_prompt, '\\n')\n",
    "    r1 = call_model(naive_prompt, temperature=temperature)\n",
    "    print('Naive response:\\n', r1, '\\n')\n",
    "    s1 = score_response(r1, expected_keywords)\n",
    "    print(f\"Naive score: {s1[0]}/{s1[1]} ({s1[2]*100:.0f}%)\\n\")\n",
    "    print('Engineered prompt:\\n', engineered_prompt, '\\n')\n",
    "    r2 = call_model(engineered_prompt, temperature=temperature)\n",
    "    print('Engineered response:\\n', r2, '\\n')\n",
    "    s2 = score_response(r2, expected_keywords)\n",
    "    print(f\"Engineered score: {s2[0]}/{s2[1]} ({s2[2]*100:.0f}%)\")\n",
    "    improvement = (s2[2] - s1[2]) if s1[1]>0 else s2[2]\n",
    "    print('\\nImprovement (fractional):', round(improvement, 3))\n",
    "    return {'naive':(r1,s1), 'engineered':(r2,s2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515862bc",
   "metadata": {},
   "source": [
    "### Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fa3f5-113a-4eeb-b7b4-18cda21ff607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Philology example (few-shot)\n",
    "naive = \"Given this excerpt, what clues tell you about its date? Excerpt: '...incipit missae... long s...'.\"\n",
    "engineered = textwrap.dedent('''\n",
    "    Below are two examples of how to extract dating clues from short manuscript excerpts.\n",
    "    Example 1:\n",
    "    Excerpt: \"...in nomine...\" -> Clues: script = Caroline minuscule; material = parchment; probable date = 9th century.\n",
    "    Example 2:\n",
    "    Excerpt: \"...long s...\" -> Clues: long s indicates later medieval scribal practice; probable date = 14th-16th c.\n",
    "    Now follow the same pattern for this excerpt: '...incipit missae... long s...'\n",
    "    Output: list 'Clues' and 'Probable date range'.\n",
    "    ''')\n",
    "expected = ['script', 'parchment', 'probable date', 'long s']\n",
    "run_example('Philology', 'Few-shot prompting', naive, engineered, expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2c143",
   "metadata": {},
   "source": [
    "### Assigning roles\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "        engineered = textwrap.dedent('''\n",
    "        You are a physics professor teaching an introductory quantum mechanics course to school students who have basic knowledge of classical physics but no prior quantum mechanics background.\n",
    "    \n",
    "        Explain quantum entanglement in a way that:\n",
    "        1. Uses an accessible analogy\n",
    "        2. Addresses common misconceptions\n",
    "        3. Explains why it's significant for quantum computing\n",
    "        4. Keeps the explanation under 200 words\n",
    "        ''')\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6170dbf-5a34-4d03-821a-b43008a2ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics example (assign role)\n",
    "naive = textwrap.dedent('''\n",
    "    Explain quantum entanglement.\n",
    "    ''')\n",
    "engineered = textwrap.dedent('''\n",
    "    ___\n",
    "    ''') #FILL_HERE\n",
    "\n",
    "expected = ['imagine', 'candy', 'friend', 'computers']\n",
    "run_example('Physics', 'Assigning roles', naive, engineered, expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231b4ae",
   "metadata": {},
   "source": [
    "### Separating data and instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b111f-842e-498e-b701-5a1990f44eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peace & Conflict example (separating data and instructions)\n",
    "naive = \"Read this case and list drivers of conflict: 'In Village X, new dam reduced water, unemployment rose, local militia formed.'\"\n",
    "engineered = textwrap.dedent('''\n",
    "    Instruction: Identify and rank up to 5 primary drivers of conflict in the provided case. For each driver, give a one-sentence rationale and suggest one short mitigation action.\n",
    "    ---DATA---\n",
    "    Case description: \"In Village X, construction of a new upstream dam reduced local water access; seasonal crops failed; unemployment rose from 12% to 28% in two years; a local militia formed and there were two clashes with police last year.\"\n",
    "    ---END DATA---\n",
    "    Output format (markdown numbered list):\n",
    "    1. <Driver>: <one-sentence rationale>. Mitigation: <action>.\n",
    "    ''')\n",
    "expected = ['water', 'unemployment', 'militia', 'mitigation', 'rationale']\n",
    "run_example('Peace & Conflict', 'Separating data and instructions', naive, engineered, expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5dada",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989c781-94e8-4762-a35e-b3694201054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sociology example (formatted JSON output)\n",
    "naive = \"Give me variables and why they're important for studying neighborhood cohesion.\"\n",
    "engineered = textwrap.dedent('''\n",
    "    Return a JSON array where each element is an object with keys:\n",
    "    - \"variable\": short variable name\n",
    "    - \"why\": one-sentence justification\n",
    "    - \"measurement\": suggested question and response type\n",
    "    Provide exactly 5 variables. Do not include additional text outside the JSON.\n",
    "    Field context: sociology research on neighborhood cohesion.\n",
    "    ''')\n",
    "expected = ['json', 'variable', 'why', 'measurement']\n",
    "run_example('Sociology', 'Formatting output', naive, engineered, expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b774d-cf1b-4a6e-bc06-580de4987f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75232b-7a64-4ad7-b066-2082ade31c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063720e-6f6f-4e99-b718-c5e20612d346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

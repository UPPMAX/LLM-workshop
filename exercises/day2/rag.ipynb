{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ff931f",
   "metadata": {},
   "source": [
    "# Build a RAG agent with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0605e5c",
   "metadata": {},
   "source": [
    "Derived from [Langchain docs](https://docs.langchain.com/oss/python/langchain/rag)\n",
    "\n",
    "### Objective\n",
    "\n",
    "You will start a **vLLM engine** to serve a local LLM and implement a RAG (Retrieval-Augmented Generation) agent that queries a blog post. \n",
    "\n",
    "The documents will be vectorized using sentence embeddings and stored in **Milvus**, a vector database, so the agent can retrieve relevant context for answering user queries.\n",
    "\n",
    "### Instructions to Start the VLLM Inference Engine\n",
    "\n",
    "NOTE: This notebook should be run on a GPU node (A40 or above). Also `cd` into your respective project folder since this creates logs files and database. \n",
    "\n",
    "1. **SSH into the compute node** that is running this Jupyter server.\n",
    "2. **Start the VLLM inference engine** using the following command:\n",
    "    ```bash\n",
    "    apptainer exec /mimer/NOBACKUP/groups/llm-workshop/containers/rag/rag.sif vllm serve /mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1 --port=$(find_ports) --gpu-memory-utilization 0.6 --enable_auto_tool_choice --tool_call_parser=hermes > vllm.out 2> vllm.err &\n",
    "    ```\n",
    "3. **Monitor the logs**:\n",
    "    - `vllm.out` for standard output.\n",
    "    - `vllm.err` for error logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbce2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddca59-8a5c-4e31-b369-aa8f154a7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize local sentence-transformers embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acb60a-464f-40e4-85f6-4730c89ca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "URI = \"./milvus_example.db\"\n",
    "\n",
    "# Initialize Milvus vector store with local embeddings\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd830cc-81a4-4f40-a2ef-04c252da64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "VLLM_PORT=46219 # REPLACE WITH YOUR VLLM PORT\n",
    "LLM_MODEL=\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1\"\n",
    "\n",
    "# Initialize local vLLM chat model\n",
    "model = init_chat_model(\n",
    "    model=LLM_MODEL,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=f\"http://localhost:{VLLM_PORT}/v1\",\n",
    "    api_key=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd7581",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef1841",
   "metadata": {},
   "source": [
    "### Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924fbf4-9312-45b9-9bc1-201b154f02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# Load a blog post from Lilian Weng's blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ab1b4-d884-4fe2-af41-a457b2c7fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 500 characters of the loaded document\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4232f1d",
   "metadata": {},
   "source": [
    "### Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ea318-63b2-48ca-82c3-6b7c254230bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split document into smaller chunks for indexing\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55809e",
   "metadata": {},
   "source": [
    "### Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511f1c3-99a7-4d5c-8a4b-b03918c2c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add document chunks to Milvus vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77054a2a",
   "metadata": {},
   "source": [
    "## Retrieval and Generation\n",
    "\n",
    "Even retrieving from a vector database can be considered a tool. This is just to demo tool usage.\n",
    "\n",
    "### Tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961857a-3aca-448d-80fc-21520da48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# Define a retrieval tool that fetches relevant documents from the vector store\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31e98-d067-4b01-8c00-7f8fbedaed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create an agent with the retrieval tool\n",
    "tools = [retrieve_context]\n",
    "# NOTE: we are disabling reasoning/thinking to force it to use tools\n",
    "prompt = (\n",
    "    \"/no_think You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcc103-eacf-48fc-94ce-f3f11c8084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query to the agent\n",
    "# NOTE: We construct a multi-part query to demonstrate retrieval and follow-up\n",
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea2fcd",
   "metadata": {},
   "source": [
    "### No tool usage:\n",
    "\n",
    "Instead of using a tool, we can directly send the retrived results based on the original query to the LLM. The agent in this case will have no tool but we inject the extra retrived context as a middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b461-ae28-480a-87e6-2f180f8f3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "# Define a dynamic prompt middleware to inject context\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"/no_think You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "    print(system_message)\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3f0d6-adb8-4468-bb99-e94a196c2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f90e9-08c7-4009-bb0f-70d90a2b398d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

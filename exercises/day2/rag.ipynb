{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ff931f",
   "metadata": {},
   "source": [
    "# Build a RAG agent with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0605e5c",
   "metadata": {},
   "source": [
    "Derived from [Langchain docs](https://docs.langchain.com/oss/python/langchain/rag)\n",
    "\n",
    "### Objective\n",
    "\n",
    "You will start a **vLLM engine** to serve a local LLM and implement a RAG (Retrieval-Augmented Generation) agent that queries a blog post. \n",
    "\n",
    "The documents will be vectorized using sentence embeddings and stored in **Milvus**, a vector database, so the agent can retrieve relevant context for answering user queries.\n",
    "\n",
    "### Instructions to Start the VLLM Inference Engine\n",
    "\n",
    "NOTE: This notebook should be run on a GPU node (A40 or above). Also `cd` into your respective project folder since this creates logs files and database.\n",
    "\n",
    "1. **SSH into the compute node** that is running this Jupyter server.\n",
    "2. **Start the VLLM inference engine** using the following command:\n",
    "    ```bash\n",
    "    apptainer exec /mimer/NOBACKUP/groups/llm-workshop/containers/rag/rag.sif vllm serve /mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1 --port=$(find_ports) --gpu-memory-utilization 0.6 --enable_auto_tool_choice --tool_call_parser=hermes > vllm.out 2> vllm.err &\n",
    "    ```\n",
    "3. **Monitor the logs**:\n",
    "    - `vllm.out` for standard output.\n",
    "    - `vllm.err` for error logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbce2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddca59-8a5c-4e31-b369-aa8f154a7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize local sentence-transformers embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acb60a-464f-40e4-85f6-4730c89ca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "URI = \"./milvus_example.db\"\n",
    "\n",
    "# Initialize Milvus vector store with local embeddings\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd830cc-81a4-4f40-a2ef-04c252da64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "VLLM_PORT=46219 # REPLACE WITH YOUR VLLM PORT\n",
    "LLM_MODEL=\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1\"\n",
    "\n",
    "# Initialize local vLLM chat model\n",
    "model = init_chat_model(\n",
    "    model=LLM_MODEL,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=f\"http://localhost:{VLLM_PORT}/v1\",\n",
    "    api_key=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd7581",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef1841",
   "metadata": {},
   "source": [
    "### Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924fbf4-9312-45b9-9bc1-201b154f02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# Load a blog post from Lilian Weng's blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ab1b4-d884-4fe2-af41-a457b2c7fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# Print the first 500 characters of the loaded document\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4232f1d",
   "metadata": {},
   "source": [
    "### Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ea318-63b2-48ca-82c3-6b7c254230bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split document into smaller chunks for indexing\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55809e",
   "metadata": {},
   "source": [
    "### Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511f1c3-99a7-4d5c-8a4b-b03918c2c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/langchain_milvus/vectorstores/milvus.py:1214: UserWarning: No ids provided and auto_id is False. Setting auto_id to True automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[462128758045802496, 462128758045802497, 462128758045802498]\n"
     ]
    }
   ],
   "source": [
    "# Add document chunks to Milvus vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77054a2a",
   "metadata": {},
   "source": [
    "## Retrieval and Generation\n",
    "\n",
    "Even retrieving from a vector database can be considered a tool. This is just to demo tool usage.\n",
    "\n",
    "### Tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961857a-3aca-448d-80fc-21520da48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# Define a retrieval tool that fetches relevant documents from the vector store\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31e98-d067-4b01-8c00-7f8fbedaed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create an agent with the retrieval tool\n",
    "tools = [retrieve_context]\n",
    "# NOTE: we are disabling reasoning/thinking to force it to use tools\n",
    "prompt = (\n",
    "    \"/no_think You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcc103-eacf-48fc-94ce-f3f11c8084c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (chatcmpl-tool-f63a3ef127f74dd2945e559b15a62fb3)\n",
      " Call ID: chatcmpl-tool-f63a3ef127f74dd2945e559b15a62fb3\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'pk': 462128758045802499, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'pk': 462128758045802498, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition involves several approaches:\n",
      "\n",
      "1. **Simple Prompting**: LLM can be prompted with simple instructions like \"Steps for XYZ. 1.\", \"What are the subgoals for achieving XYZ?\" to help decompose a task into smaller steps.\n",
      "\n",
      "2. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline.\" for writing a novel, helps decompose the task into manageable parts.\n",
      "\n",
      "3. **External Classical Planners**: The LLM+P approach (Liu et al. 2023) uses an external classical planner to handle long-horizon planning. This involves translating the problem into PDDL, requesting a planner to generate a PDDL plan, and then translating the plan back into natural language. This method is particularly useful in robotic domains where domain-specific PDDL and suitable planners are available.\n",
      "\n",
      "4. **Chain of Thought (CoT)**: CoT is a prompting technique that helps LLM decompose complex tasks into smaller steps by thinking step-by-step. This method enhances model performance on complex tasks by utilizing more test-time computation.\n",
      "\n",
      "5. **Tree of Thoughts**: This approach extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. The search process can be BFS or DFS, with each state evaluated by a classifier or majority vote.\n",
      "\n",
      "These methods offer different ways to decompose tasks, from simple prompting to using external planners and exploring multiple reasoning paths.\n"
     ]
    }
   ],
   "source": [
    "# Example query to the agent\n",
    "# NOTE: We construct a multi-part query to demonstrate retrieval and follow-up\n",
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea2fcd",
   "metadata": {},
   "source": [
    "### No tool usage:\n",
    "\n",
    "Instead of using a tool, we can directly send the retrived results based on the original query to the LLM. The agent in this case will have no tool but we inject the extra retrived context as a middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b461-ae28-480a-87e6-2f180f8f3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "# Define a dynamic prompt middleware to inject context\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"/no_think You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "    print(system_message)\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3f0d6-adb8-4468-bb99-e94a196c2b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "/no_think You are a helpful assistant. Use the following context in your response:\n",
      "\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task or problem into smaller, more manageable, and simpler subtasks. This approach allows for more effective planning, execution, and problem-solving. By decomposing a task, the complexity is reduced, and the solution can be developed in a step-by-step manner. Each subtask is typically smaller, more specific, and easier to tackle than the original task.\n",
      "\n",
      "Task decomposition is useful in various domains, including artificial intelligence, computer science, and project management. It enables:\n",
      "\n",
      "1. **Improved Planning**: By breaking down a task into smaller subtasks, it becomes easier to create a detailed plan and timeline for completing the task.\n",
      "2. **Enhanced Problem-Solving**: Decomposing a task helps identify the root cause of issues and allows for targeted solutions.\n",
      "3. **Better Resource Allocation**: Task decomposition facilitates efficient allocation of resources, as each subtask can be assigned to a specific resource or team.\n",
      "4. **Reduced Overhead**: By tackling smaller tasks, the overall workload is reduced, and the likelihood of errors is minimized.\n",
      "\n",
      "Task decomposition can be done using various techniques, such as:\n",
      "\n",
      "1. **Divide and Conquer**: Breaking down a task into smaller, independent subtasks that can be solved individually.\n",
      "2. **Tree Decomposition**: Representing the task as a tree, where each node represents a subtask, and the root node represents the original task.\n",
      "3. **Hierarchical Decomposition**: Breaking down a task into a series of hierarchical levels, with each level representing a set of subtasks.\n",
      "4. **Task Decomposition Frameworks**: Using pre-defined frameworks or templates to guide the decomposition process.\n",
      "\n",
      "Overall, task decomposition is a powerful technique for managing complex tasks, improving problem-solving, and increasing efficiency.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f90e9-08c7-4009-bb0f-70d90a2b398d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

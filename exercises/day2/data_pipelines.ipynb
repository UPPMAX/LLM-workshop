{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f956277",
   "metadata": {},
   "source": [
    "# Dataset preparation for SFT\n",
    "\n",
    "### Objective\n",
    "\n",
    "To understand chat template, see base vs instruct model responses and prepare a dataset for Supervised Finetuning.\n",
    "\n",
    "NOTE:\n",
    "Run this notebook on A40 (and above) GPU with `post_train_env` environment as it contains the necessary libraries to prepare our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221de627",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae87ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU - you will need to use a GPU to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64fc711",
   "metadata": {},
   "source": [
    "## Load Base and Instruct Models\n",
    "\n",
    "<details>\n",
    "<summary>Hints</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# If you are on CPU, use smaller models\n",
    "...\n",
    "# Load tokenizers\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "\n",
    "# Load models (use smaller precision for memory efficiency)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42604e68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load both base and instruct models for comparisonm\n",
    "\n",
    "# If you are on CPU, use smaller models\n",
    "USE_GPU = True\n",
    "\n",
    "if not USE_GPU:\n",
    "    base_model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"\n",
    "    instruct_model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/\"\n",
    "else:\n",
    "    base_model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B-Base/snapshots/d78a42f79198603e614095753484a04c10c2b940/\"\n",
    "    instruct_model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/\"\n",
    "\n",
    "# Load tokenizers\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(___) #FILL_HERE\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(___) #FILL_HERE\n",
    "\n",
    "# Load models (use smaller precision for memory efficiency)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ___, #FILL_HERE\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ___, #FILL_HERE\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954730e",
   "metadata": {},
   "source": [
    "## Explore Chat Template Formatting\n",
    "\n",
    "Our models follow ChatML format\n",
    "\n",
    "<details>\n",
    "<summary>Hints</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Create different types of conversations to test\n",
    "...\n",
    "\n",
    "for conv_type, messages in conversations.items():\n",
    "    print(f\"--- {conv_type.upper()} ---\")\n",
    "    \n",
    "    # Format without generation prompt (for completed conversations)\n",
    "    formatted_complete = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False \n",
    "    )\n",
    "    \n",
    "    # Format with generation prompt (for inference)\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "   ...\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde5e9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create different types of conversations to test\n",
    "conversations = {\n",
    "    \"simple_qa\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"with_system\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining technical concepts clearly.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"multi_turn\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you give me a simple example?\"},\n",
    "    ],\n",
    "    \n",
    "    \"reasoning_task\": [\n",
    "        {\"role\": \"user\", \"content\": \"Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "for conv_type, messages in conversations.items():\n",
    "    print(f\"--- {conv_type.upper()} ---\")\n",
    "    \n",
    "    # Format without generation prompt (for completed conversations)\n",
    "    formatted_complete = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=___ #FILL_HERE\n",
    "    )\n",
    "    \n",
    "    # Format with generation prompt (for inference)\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=___ #FILL_HERE\n",
    "    )\n",
    "    \n",
    "    print(\"Complete conversation format:\")\n",
    "    print(formatted_complete)\n",
    "    print(\"\\nWith generation prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03211f14",
   "metadata": {},
   "source": [
    "## Compare Base vs Instruct Model Response\n",
    "\n",
    "<details>\n",
    "<summary>Hints</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Test the same prompt on both models\n",
    "...\n",
    "# Prepare the prompt for base model (no chat template)\n",
    "...\n",
    "# Prepare the prompt for instruct model (with chat template)\n",
    "instruct_messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "instruct_formatted = instruct_tokenizer.apply_chat_template(\n",
    "    instruct_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "instruct_inputs = instruct_tokenizer(instruct_formatted, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate responses\n",
    "...\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891122d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test the same prompt on both models\n",
    "test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "# Prepare the prompt for base model (no chat template)\n",
    "base_inputs = base_tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Prepare the prompt for instruct model (with chat template)\n",
    "instruct_messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "instruct_formatted = instruct_tokenizer.___( #FILL_HERE\n",
    "    instruct_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "instruct_inputs = instruct_tokenizer(instruct_formatted, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate responses\n",
    "print(\"=== Model comparison ===\\n\")\n",
    "\n",
    "print(\"ðŸ¤– BASE MODEL RESPONSE:\")\n",
    "with torch.no_grad():\n",
    "    base_outputs = base_model.generate(\n",
    "        **base_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=base_tokenizer.eos_token_id\n",
    "    )\n",
    "    base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    print(base_response[len(test_prompt):])  # Show only the generated part\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ¤– INSTRUCT MODEL RESPONSE:\")\n",
    "with torch.no_grad():\n",
    "    instruct_outputs = instruct_model.generate(\n",
    "        **instruct_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=instruct_tokenizer.eos_token_id\n",
    "    )\n",
    "    instruct_response = instruct_tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    assistant_start = instruct_response.find(\"<|im_start|>assistant\\n\") + len(\"<|im_start|>assistant\\n\")\n",
    "    assistant_response = instruct_response[assistant_start:].split(\"<|im_end|>\")[0]\n",
    "    print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aca3ed",
   "metadata": {},
   "source": [
    "## Explore gsm8k dataset\n",
    "\n",
    "<details>\n",
    "<summary>Hints</summary>\n",
    "\n",
    "```python\n",
    "...\n",
    "# Convert to chat format\n",
    "def process_gsm8k(examples):\n",
    "    processed = []\n",
    "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a math tutor. Solve problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    return {\"messages\": processed}\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811d539",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"/mimer/NOBACKUP/groups/llm-workshop/datasets/openai___gsm8k\", split=\"train[:100]\")  # Small subset for demo\n",
    "print(f\"Original GSM8K example: {gsm8k[0]}\")\n",
    "\n",
    "# Convert to chat format\n",
    "def process_gsm8k(examples):\n",
    "    processed = []\n",
    "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a math tutor. Solve problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": ___}, #FILL_HERE\n",
    "            {\"role\": \"assistant\", \"content\": ___} #FILL_HERE\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    return {\"messages\": processed}\n",
    "\n",
    "gsm8k_processed = gsm8k.map(process_gsm8k, batched=True, remove_columns=gsm8k.column_names)\n",
    "print(f\"Processed example: {gsm8k_processed[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776f135",
   "metadata": {},
   "source": [
    "## Apply Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002deb47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to apply chat templates to processed datasets\n",
    "def apply_chat_template_to_dataset(dataset, tokenizer):\n",
    "    \"\"\"Apply chat template to dataset for training\"\"\"\n",
    "    \n",
    "    def format_messages(examples):\n",
    "        formatted_texts = []\n",
    "        \n",
    "        for messages in examples[\"messages\"]:\n",
    "            # Apply chat template\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False  # We want the complete conversation\n",
    "            )\n",
    "            formatted_texts.append(formatted_text)\n",
    "        \n",
    "        return {\"text\": formatted_texts}\n",
    "    \n",
    "    return dataset.map(format_messages, batched=True)\n",
    "\n",
    "# Apply to our processed GSM8K dataset\n",
    "gsm8k_formatted = apply_chat_template_to_dataset(gsm8k_processed, instruct_tokenizer)\n",
    "print(\"=== FORMATTED TRAINING DATA ===\")\n",
    "print(gsm8k_formatted[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cefdc6-c340-4341-81be-c9a24bf6e4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>LLM formats</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/dist/reset.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/dist/theme/white.css" id="theme">
  <link rel="stylesheet" href="/LLM-workshop/stylesheets/slides.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">LLM formats</h1>
</section>

<section id="overview" class="title-slide slide level2">
<h2>Overview</h2>
<ul>
<li>Formats of LLM models</li>
<li>Formats of numbers</li>
<li>Quantization of LLM</li>
<li>Quantization and performance</li>
</ul>
</section>

<section>
<section id="formats-of-llm-models" class="title-slide slide level2">
<h2>Formats of LLM models</h2>

</section>
<section id="so-you-want-to-use-a-llm-model" class="slide level3">
<h3>So you want to use a LLM model</h3>
<p><img data-src="figures/hf-search-models.png"
style="height=360px" /></p>
<aside class="notes">
<p>It is common for LLMs to be quantized after training. The
quantization of LLMs is relevant to the model’s compatibility with
different implementations, as well its its performance, both in terms of
accuracy and speed. This section will focus on explaining details of
different floating point number formats and quantization methods.</p>
</aside>
</section>
<section id="what-the-name-means" class="slide level3">
<h3>What the name means</h3>
<ul>
<li><code>Llama-3.3</code>: model (architecture)</li>
<li><code>70B</code>: size / number of parameters</li>
<li><code>Instruct</code>: fine-tuning</li>
<li><code>AWQ-INT4</code>: quantization</li>
<li><code>GGUF</code>: model format</li>
</ul>
<aside class="notes">
<p>Models architecture and size are often the first consideration when
working on LLM. But equally important are the format of models and the
quantization method. Modern acceleration devices cater for the lower
precision need of machine learning models, depending on the device you
want to run on, quantized models might give significant speed up.</p>
</aside>
</section>
<section id="file-formats-of-llms" class="slide level3">
<h3>File-formats of LLMs</h3>
<p><img data-src="figures/gguf.png" style="max-height:360px" /></p>
<p>The gguf file fomat (image from <a
href="https://huggingface.co/docs/hub/gguf">huggingface</a>)</p>
<aside class="notes" markdown="1">
<p>LLM models commonly consists of metadata, and the tensor
themselves.</p>
<p></asisde></p>
</section>
<section id="common-formats-of-llms" class="slide level3">
<h3>Common formats of LLMs</h3>
<ul>
<li>bin/pth/tf: “raw” ML library formats;</li>
<li>safetensors: used by huggingfacs;</li>
<li>ggml/gguf: developed by llama.cpp (supports many qunatization
formats);</li>
<li>llamafile: by mozilla, single-file format, executable.</li>
</ul>
<p>You can find detailed model information for some model formats, <a
href="https://huggingface.co/QuantStack/Qwen-Image-Edit-2509-GGUF?show_file_info=Qwen-Image-Edit-2509-Q2_K.gguf">example</a>.</p>
<aside class="notes">
<p>Models are published on different formats and they are optimized for
different usages. They can be converted to one another but has different
purposes.</p>
<p>raw formats used by ML formats might be handy for re-training. Some
of them contain pickled data so might execute arbitrary code
(unsafe).</p>
<p>Newer formats like GGUF/safetensors are suitable for common model
architectures (different engines will support them if the architecture
is known). They are memory-mapped, which are especially useful for <a
href="https://huggingface.co/docs/accelerate/package_reference/big_modeling#accelerate.disk_offload">disk
offloading</a>.</p>
</aside>
</section>
<section id="look-for-the-following" class="slide level3">
<h3>Look for the following</h3>
<ul>
<li>Quantization method;</li>
<li>Number format;</li>
<li>Hardware compatibility hints.</li>
</ul>
</section></section>
<section>
<section id="formats-of-numbers" class="title-slide slide level2">
<h2>Formats of numbers</h2>

</section>
<section id="why-do-we-care" class="slide level3">
<h3>Why do we care?</h3>
<ul>
<li>Quantization allow you to run larger models;</li>
<li>It might also eliminate expensive communication;</li>
<li>ML tolerates lower numerical precision;</li>
<li>Number formats also determines “distributioin of information”;</li>
</ul>
</section>
<section id="number-formats" class="slide level3">
<h3>Number formats</h3>
<p><img data-src="figures/float16.png" /></p>
</section>
<section id="floating-point-formats---cont.-1" class="slide level3">
<h3>Floating point formats - cont. 1</h3>
<p><img data-src="figures/number-format.svg" /></p>
</section>
<section id="floating-point-formats---cont.-2" class="slide level3">
<h3>Floating point formats - cont. 2</h3>
<p><img data-src="figures/number-ranges.png" /></p>
</section>
<section id="hardware-compatibility" class="slide level3">
<h3>Hardware Compatibility</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th>hardware accel.</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fp16/32/64</td>
<td>most gpus</td>
<td>IEEE 754</td>
</tr>
<tr class="even">
<td>fp8 <a
href="https://onnx.ai/onnx/technical/float8.html">(E4M3/E5M2)</a></td>
<td>hooper</td>
<td>Recent IEEE</td>
</tr>
<tr class="odd">
<td>bf16</td>
<td>most gpus</td>
<td>Google’s</td>
</tr>
<tr class="even">
<td>tf32</td>
<td>nvidia-gpus</td>
<td>Nvidia</td>
</tr>
<tr class="odd">
<td>int4/8</td>
<td>most GPUs</td>
<td></td>
</tr>
</tbody>
</table>
<p>See also <a
href="https://rocm.docs.amd.com/en/latest/reference/precision-support.html">Data
types support</a> by AMD RocM.</p>
<aside class="notes">
<p>Floating point number generally follows a IEEE standard format.
However, details like the representation of negative zeros (NZ) and
infinite values might be different.</p>
</aside>
</section>
<section id="rule-of-thumb" class="slide level3">
<h3>Rule of thumb</h3>
<ul>
<li>ML tasks favor a larger proportion of exponents;</li>
<li><a
href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">Google’s
bf16</a> (same range as fp32, less mantissa);</li>
<li>training usually done in fp32/16;</li>
<li>int4/8 is good for inference (on older GPUs).</li>
</ul>
</section></section>
<section>
<section id="quantization-methods" class="title-slide slide level2">
<h2>Quantization methods</h2>

</section>
<section id="quantization-target" class="slide level3">
<h3>Quantization target</h3>
<p><img data-src="figures/mixed_precision_hopper.jpg" /></p>
<ul>
<li>Weight/activation/mixed percision (w8a16);</li>
<li>KV-cache;</li>
<li>Non-uniform;</li>
</ul>
<aside class="notes">
<p>weights is usually the first thing to quantize, it is also the most
supported way of quantizing the model. Depending on the hardware, it
might or might not support converting the tensors between precision or
doing tensor operations natively.</p>
<p>For instance, FP8 is not officially support on Ampere GPUs (A40 and
A100). While there exist implementations that makes <a
href="https://docs.vllm.ai/en/v0.5.2/quantization/fp8.html">w8a16</a>
operations available, quantizating KV cache to FP8 currently <a
href="https://discuss.vllm.ai/t/kv-cache-quantizing/749">need hardware
support</a>.</p>
<p>Models can also been quantized <a
href="https://docs.vllm.ai/projects/llm-compressor/en/latest/examples/quantization_non_uniform/">non-uniformly</a></p>
</aside>
</section>
<section id="asymmetric-qunatization" class="slide level3">
<h3>(A)symmetric qunatization</h3>
<p><img data-src="figures/quantization_symmetry.webp" /></p>
<ul>
<li>linear transformation;</li>
<li>depend on original range;</li>
<li>position of zero.</li>
</ul>
<aside class="notes">
<p>One important aspect when quantizing the models is the distribution
of the model, the easiest way is to simply scale the parameters by a
factor.</p>
<p>To minimize loss of precision, we could map the parameters according
to the max/min values of the parameter, rather than the number-format
range. There, we need to choose whether we shift the zero point in the
transform (but introduces complexity in computation).</p>
</aside>
</section>
<section id="clipping" class="slide level3">
<h3>Clipping</h3>
<p><img data-src="figures/clipping.webp" /></p>
<aside class="notes">
<p>we can also choose to clip out the outlier to same more
precision.</p>
</aside>
</section>
<section id="calibration-for-weight-quantization" class="slide level3">
<h3>Calibration for weight quantization</h3>
<aside class="notes">
<p>For parameters of the model We can simply quantize them, since we
know their distribution. But given some small dataset but we can also
improved the accuracy but estimating how important each parameter it. A
popular way to do that is the GPTQ method.</p>
</aside>
<p><img data-src="figures/gptq.webp" /></p>
<p>Illustration of GPTQ method, where quantization are done to minimize
the error, weighed by according to the inverse Hessian
(sensitivity).</p>
</section>
<section id="calibration-for-activation-qunatization"
class="slide level3">
<h3>Calibration for activation qunatization</h3>
<p><img data-src="figures/dynamic_calibration.webp" /></p>
<p>Can be dynamic or static.</p>
<aside class="notes">
<p>To also quantize the activation function, we need to estimate the
range of activation, that has to be done by passing data to the model
and collect minima/maximi. We can do that either dynamically (during
inference) or statically (with a calibration set).</p>
</aside>
</section>
<section id="post-training-quantization-methods-ptq"
class="slide level3">
<h3>Post-training quantization methods (PTQ)</h3>
<ul>
<li>Weights and/or activation;</li>
<li>Calibration/accuracy trade off;</li>
<li>Not detailed here: sparsification.</li>
</ul>
<p><img data-src="figures/sparse-matrix.png" /></p>
<aside class="notes">
<p>Models may also be sparsified to reduce the required computation,
this is commonly known as weight pruning. But some GPUs also support
efficient evaluation of sparse matrices if the sparsity follow certain
pattern (example with <a
href="https://github.com/vllm-project/llm-compressor/blob/main/examples/sparse_2of4_quantization_fp8/README.md">llm-compressor</a>);</p>
<p>So far we covered mostly the so-called PTQ method when we do/can not
run the training (for a complete list with compatibility see <a
href="https://github.com/vllm-project/llm-compressor/blob/main/docs/guides/compression_schemes.md">vLLM
guide</a>).</p>
</aside>
</section>
<section id="quantization-aware-training-qat" class="slide level3">
<h3>Quantization aware training (QAT)</h3>
<p>QAT introduce quantization error during training;</p>
<p><img data-src="figures/qat.webp" style="max-height:100px;" /> <img
data-src="figures/qat_back.webp" style="max-height:300px;" /></p>
<aside class="notes">
<p>But we can also get higher accuracy by using the Quantization aware
training (QAT) method. There we do the training and perform the
quantization/dequantization; which this the first thing we gain is that
we can actually optimize the quantization parameters as part of the
training process.</p>
</aside>
</section>
<section id="quantization-aware-training-qat---cont."
class="slide level3">
<h3>Quantization aware training (QAT) - cont.</h3>
<p><img data-src="figures/qat_theory.webp" /></p>
<aside class="notes">
<p>The reason why it might work better, is that by introducing the
quantization error in the training process, we force the model to land
in a local minima where it is less sensitive to model parameters. So
even the original model performs worth, the quantized model works
better</p>
</aside>
</section></section>
<section>
<section id="summary" class="title-slide slide level2">
<h2>Summary</h2>

</section>
<section id="when-choosing-a-model" class="slide level3">
<h3>When choosing a model</h3>
<ul>
<li>Know the hardware/implementation compatibility;</li>
<li>Find the right model/format/qunatization;</li>
<li>Quantize if needed;</li>
<li>Look up/run benchmarks.</li>
</ul>
</section>
<section id="other-useful-links" class="slide level3">
<h3>Other useful links</h3>
<p>Benchmarks:</p>
<ul>
<li><a
href="https://huggingface.co/datasets/derekl35/quantization-benchmarks">derek135/quantization-benchmarks</a></li>
</ul>
</section></section>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/plugin/notes/notes.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/plugin/search/search.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/plugin/zoom/zoom.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.2.1/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>

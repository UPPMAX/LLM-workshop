
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>LLM formats - LLM Workshop</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2024%2024%22%3E%3Cpath%20d%3D%22M17%2013h-4v4h-2v-4H7v-2h4V7h2v4h4m-5-9A10%2010%200%200%200%202%2012a10%2010%200%200%200%2010%2010%2010%2010%200%200%200%2010-10A10%2010%200%200%200%2012%202%22/%3E%3C/svg%3E');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Lato";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM Workshop" class="md-header__button md-logo" aria-label="LLM Workshop" data-md-component="logo">
      
  <img src="../../logo/naiss_logo_white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM formats
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/UPPMAX/LLM-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../course_dates/" class="md-tabs__link">
        
  
  
    
  
  Course dates

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM Workshop" class="md-nav__button md-logo" aria-label="LLM Workshop" data-md-component="logo">
      
  <img src="../../logo/naiss_logo_white.png" alt="logo">

    </a>
    LLM Workshop
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/UPPMAX/LLM-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course_dates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course dates
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/UPPMAX/LLM-workshop/edit/master/docs/day1/llm_formats.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/UPPMAX/LLM-workshop/raw/master/docs/day1/llm_formats.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>LLM formats</h1>

<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Anchor link to this section for reference">&para;</a></h2>
<ul>
<li>Formats of LLM models</li>
<li>Formats of numbers</li>
<li>Quantization of LLM</li>
<li>Quantization and performance</li>
</ul>
<h2 id="formats-of-llm-models">Formats of LLM models<a class="headerlink" href="#formats-of-llm-models" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="so-you-want-to-use-a-llm-model">So you want to use a LLM model<a class="headerlink" href="#so-you-want-to-use-a-llm-model" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/hf-search-models.png" style="height=360px" /></p>
<aside class="notes">
<p>It is common for LLMs to be quantized after training. The quantization of
LLMs is relevant to the model's compatibility with different implementations, as
well its its performance, both in terms of accuracy and speed. This section will
focus on explaining details of different floating point number formats and
quantization methods.</p>
</aside>
<h3 id="what-the-name-means">What the name means<a class="headerlink" href="#what-the-name-means" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li><code>Llama-3.3</code>: model (architecture)</li>
<li><code>70B</code>: size / number of parameters</li>
<li><code>Instruct</code>: fine-tuning</li>
<li><code>AWQ-INT4</code>: quantization</li>
<li><code>GGUF</code>: model format</li>
</ul>
<aside class="notes">
<p>Models architecture and size are often the first consideration when working on
LLM.  But equally important are the format of models and the quantization
method. Modern acceleration devices cater for the lower precision need of
machine learning models, depending on the device you want to run on, quantized
models might give significant speed up.</p>
</aside>
<h3 id="file-formats-of-llms">File-formats of LLMs<a class="headerlink" href="#file-formats-of-llms" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/gguf.png" style="max-height:360px" /></p>
<p>The gguf file fomat (image from <a href="https://huggingface.co/docs/hub/gguf">huggingface</a>)</p>
<aside class="notes">
<p>LLM models commonly consists of metadata, and the tensor themselves.</p>
<p></asisde></p>
<h3 id="common-formats-of-llms">Common formats of LLMs<a class="headerlink" href="#common-formats-of-llms" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>bin/pth/tf: "raw" ML library formats;</li>
<li>safetensors: used by huggingfacs;</li>
<li>ggml/gguf: developed by llama.cpp (supports many qunatization formats);</li>
<li>llamafile: by mozilla, single-file format, executable.</li>
</ul>
<p>You can find detailed model information for some model formats,
<a href="https://huggingface.co/QuantStack/Qwen-Image-Edit-2509-GGUF?show_file_info=Qwen-Image-Edit-2509-Q2_K.gguf">example</a>.</p>
<aside class="notes">
<p>Models are published on different formats and they are optimized for different
usages. They can be converted to one another but has different purposes.</p>
<p>raw formats used by ML formats might be handy for re-training. Some of them
contain pickled data so might execute arbitrary code (unsafe).</p>
<p>Newer formats like GGUF/safetensors are suitable for common model architectures
(different engines will support them if the architecture is known). They are
memory-mapped, which are especially useful for <a href="https://huggingface.co/docs/accelerate/package_reference/big_modeling#accelerate.disk_offload">disk offloading</a>.</p>
</aside>
<h3 id="look-for-the-following">Look for the following<a class="headerlink" href="#look-for-the-following" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Quantization method;</li>
<li>Number format;</li>
<li>Hardware compatibility hints.</li>
</ul>
<h2 id="formats-of-numbers">Formats of numbers<a class="headerlink" href="#formats-of-numbers" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="why-do-we-care">Why do we care?<a class="headerlink" href="#why-do-we-care" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Quantization allow you to run larger models;</li>
<li>It might also eliminate expensive communication;</li>
<li>ML tolerates lower numerical precision;</li>
<li>Number formats also determines "distributioin of information";</li>
</ul>
<h3 id="number-formats">Number formats<a class="headerlink" href="#number-formats" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/float16.png" /></p>
<h3 id="floating-point-formats-cont-1">Floating point formats - cont. 1<a class="headerlink" href="#floating-point-formats-cont-1" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/number-format.svg" /></p>
<h3 id="floating-point-formats-cont-2">Floating point formats - cont. 2<a class="headerlink" href="#floating-point-formats-cont-2" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/number-ranges.png" /></p>
<h3 id="hardware-compatibility">Hardware Compatibility<a class="headerlink" href="#hardware-compatibility" title="Anchor link to this section for reference">&para;</a></h3>
<table>
<thead>
<tr>
<th></th>
<th>hardware accel.</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>fp16/32/64</td>
<td>most gpus</td>
<td>IEEE 754</td>
</tr>
<tr>
<td>fp8 <a href="https://onnx.ai/onnx/technical/float8.html">(E4M3/E5M2)</a></td>
<td>hooper</td>
<td>Recent IEEE</td>
</tr>
<tr>
<td>bf16</td>
<td>most gpus</td>
<td>Google's</td>
</tr>
<tr>
<td>tf32</td>
<td>nvidia-gpus</td>
<td>Nvidia</td>
</tr>
<tr>
<td>int4/8</td>
<td>most GPUs</td>
<td></td>
</tr>
</tbody>
</table>
<p>See also <a href="https://rocm.docs.amd.com/en/latest/reference/precision-support.html">Data types support</a> by AMD RocM.</p>
<aside class="notes">
<p>Floating point number generally follows a IEEE standard format. However, details
like the representation of negative zeros (NZ) and infinite values might be
different.</p>
</aside>
<h3 id="rule-of-thumb">Rule of thumb<a class="headerlink" href="#rule-of-thumb" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>ML tasks favor a larger proportion of exponents;</li>
<li><a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">Google's bf16</a> (same range as fp32, less mantissa);</li>
<li>training usually done in fp32/16;</li>
<li>int4/8 is good for inference (on older GPUs).</li>
</ul>
<h2 id="quantization-methods">Quantization methods<a class="headerlink" href="#quantization-methods" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="quantization-target">Quantization target<a class="headerlink" href="#quantization-target" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/mixed_precision_hopper.jpg" /></p>
<ul>
<li>Weight/activation/mixed percision (w8a16);</li>
<li>KV-cache;</li>
<li>Non-uniform;</li>
</ul>
<aside class="notes">
<p>weights is usually the first thing to quantize, it is also the most supported
way of quantizing the model. Depending on the hardware, it might or might not
support converting the tensors between precision or doing tensor operations
natively.</p>
<p>For instance, FP8 is not officially support on Ampere GPUs (A40 and A100). While
there exist implementations that makes <a href="https://docs.vllm.ai/en/v0.5.2/quantization/fp8.html">w8a16</a> operations available,
quantizating KV cache to FP8 currently <a href="https://discuss.vllm.ai/t/kv-cache-quantizing/749">need hardware
support</a>.</p>
<p>Models can also been quantized
<a href="https://docs.vllm.ai/projects/llm-compressor/en/latest/examples/quantization_non_uniform/">non-uniformly</a></p>
</aside>
<h3 id="asymmetric-qunatization">(A)symmetric qunatization<a class="headerlink" href="#asymmetric-qunatization" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/quantization_symmetry.webp" /></p>
<ul>
<li>linear transformation;</li>
<li>depend on original range;</li>
<li>position of zero.</li>
</ul>
<aside class="notes">
<p>One important aspect when quantizing the models is the distribution of the
model, the easiest way is to simply scale the parameters by a factor.</p>
<p>To minimize loss of precision, we could map the parameters according to the
max/min values of the parameter, rather than the number-format range. There, we
need to choose whether we shift the zero point in the transform (but introduces
complexity in computation).</p>
</aside>
<h3 id="clipping">Clipping<a class="headerlink" href="#clipping" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/clipping.webp" /></p>
<aside class="notes">
<p>we can also choose to clip out the outlier to same more precision.</p>
</aside>
<h3 id="calibration-for-weight-quantization">Calibration for weight quantization<a class="headerlink" href="#calibration-for-weight-quantization" title="Anchor link to this section for reference">&para;</a></h3>
<aside class="notes">
<p>For parameters of the model We can simply quantize them, since we know their
distribution. But given some small dataset but we can also improved the accuracy
but estimating how important each parameter it. A popular way to do that is the
GPTQ method.</p>
</aside>
<p><img alt="" src="../figures/gptq.webp" /></p>
<p>Illustration of GPTQ method, where quantization are done to minimize the error,
weighed by according to the inverse Hessian (sensitivity).</p>
<h3 id="calibration-for-activation-qunatization">Calibration for activation qunatization<a class="headerlink" href="#calibration-for-activation-qunatization" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/dynamic_calibration.webp" /></p>
<p>Can be dynamic or static.</p>
<aside class="notes">
<p>To also quantize the activation function, we need to estimate the range of
activation, that has to be done by passing data to the model and collect
minima/maximi. We can do that either dynamically (during inference) or
statically (with a calibration set).</p>
</aside>
<h3 id="post-training-quantization-methods-ptq">Post-training quantization methods (PTQ)<a class="headerlink" href="#post-training-quantization-methods-ptq" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Weights and/or activation;</li>
<li>Calibration/accuracy trade off;</li>
<li>Not detailed here: sparsification.</li>
</ul>
<p><img alt="" src="../figures/sparse-matrix.png" /></p>
<aside class="notes">
<p>Models may also be sparsified to reduce the required computation, this is
commonly known as weight pruning. But some GPUs also support efficient
evaluation of sparse matrices if the sparsity follow certain pattern (example
with <a href="https://github.com/vllm-project/llm-compressor/blob/main/examples/sparse_2of4_quantization_fp8/README.md">llm-compressor</a>);</p>
<p>So far we covered mostly the so-called PTQ method when we do/can not run the
training (for a complete list with compatibility see <a href="https://github.com/vllm-project/llm-compressor/blob/main/docs/guides/compression_schemes.md">vLLM guide</a>).</p>
</aside>
<h3 id="quantization-aware-training-qat">Quantization aware training (QAT)<a class="headerlink" href="#quantization-aware-training-qat" title="Anchor link to this section for reference">&para;</a></h3>
<p>QAT introduce quantization error during training;</p>
<p><img alt="" src="../figures/qat.webp" style="max-height:100px;" />
<img alt="" src="../figures/qat_back.webp" style="max-height:300px;" /></p>
<aside class="notes">
<p>But we can also get higher accuracy by using the Quantization aware training
(QAT) method. There we do the training and perform the
quantization/dequantization; which this the first thing we gain is that we can
actually optimize the quantization parameters as part of the training process.</p>
</aside>
<h3 id="quantization-aware-training-qat-cont">Quantization aware training (QAT) - cont.<a class="headerlink" href="#quantization-aware-training-qat-cont" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/qat_theory.webp" /></p>
<aside class="notes">
<p>The reason why it might work better, is that by introducing the quantization
error in the training process, we force the model to land in a local minima
where it is less sensitive to model parameters. So even the original model
performs worth, the quantized model works better</p>
</aside>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="when-choosing-a-model">When choosing a model<a class="headerlink" href="#when-choosing-a-model" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Know the hardware/implementation compatibility;</li>
<li>Find the right model/format/qunatization;</li>
<li>Quantize if needed;</li>
<li>Look up/run benchmarks.</li>
</ul>
<h3 id="other-useful-links">Other useful links<a class="headerlink" href="#other-useful-links" title="Anchor link to this section for reference">&para;</a></h3>
<p>Benchmarks:</p>
<ul>
<li><a href="https://huggingface.co/datasets/derekl35/quantization-benchmarks">derek135/quantization-benchmarks</a></li>
</ul>
</aside>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="https://www.naiss.se/" target="_blank" rel="noopener" title="NAISS" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.path", "navigation.footer", "navigation.expand", "search.suggest", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.action.edit", "content.action.view", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
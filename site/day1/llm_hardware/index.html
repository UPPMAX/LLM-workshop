
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>LLM and hardware - LLM Workshop</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2024%2024%22%3E%3Cpath%20d%3D%22M17%2013h-4v4h-2v-4H7v-2h4V7h2v4h4m-5-9A10%2010%200%200%200%202%2012a10%2010%200%200%200%2010%2010%2010%2010%200%200%200%2010-10A10%2010%200%200%200%2012%202%22/%3E%3C/svg%3E');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Lato";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#computations-in-llms" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM Workshop" class="md-header__button md-logo" aria-label="LLM Workshop" data-md-component="logo">
      
  <img src="../../logo/naiss_logo_white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM and hardware
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/UPPMAX/LLM-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../course_dates/" class="md-tabs__link">
        
  
  
    
  
  Course dates

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM Workshop" class="md-nav__button md-logo" aria-label="LLM Workshop" data-md-component="logo">
      
  <img src="../../logo/naiss_logo_white.png" alt="logo">

    </a>
    LLM Workshop
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/UPPMAX/LLM-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course_dates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course dates
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/UPPMAX/LLM-workshop/edit/master/docs/day1/llm_hardware.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/UPPMAX/LLM-workshop/raw/master/docs/day1/llm_hardware.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>LLM and hardware</h1>

<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Computations in LLMs</li>
<li>LLM on super-comupters</li>
</ul>
<h2 id="computations-in-llms">Computations in LLMs<a class="headerlink" href="#computations-in-llms" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="neural-networks">Neural networks<a class="headerlink" href="#neural-networks" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/neural_network_training.png" style="height:360px" /></p>
<ul>
<li>Learn patterns by adjusting parameters (weights);</li>
<li>Training = prediction → differentiation → update;</li>
<li>So far: mini-batch &amp; optimizer &amp; big → good.</li>
</ul>
<aside class="notes">
<p>Neural networks are building blocks of modern machine learning applications,
 the principle is simple, just like your regular gradient descent, you:</p>
<ul>
<li>compute the output of your model;</li>
<li>compute the <em>loss function</em> according to reference output</li>
<li>compute the gradient of loss with respect to your parameters;</li>
<li>update you parameters slightly in the direction that reduces the loss the most;</li>
</ul>
<p>Neural networks are however:</p>
<ul>
<li><strong>easy to differentiate:</strong> thanks to automatic differentiation;</li>
<li><strong>easy to parallelize:</strong> matrix multiplications can be done in parallel;</li>
<li><strong>easy to scale:</strong> training is done on small subsets of data per step.</li>
</ul>
<p>which makes it extermely easy to scale up, and show great performance when
 scaled up. [^1]</p>
<p>Note that during training, we need to store multiple copies of all model
 parameters (for gradients, optimizer states, etc.), which multiplies memory
 needs.</p>
<p>PS: If you are suprised that large models work better, you are not alone; see
 <a href="https://en.wikipedia.org/wiki/Double_descent">double descent</a></p>
</aside>
<h3 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/transformer_vs_rnn.png" style="height:360px" /></p>
<ul>
<li>Transformer computes <em>relationships</em> between tokens (attention);</li>
<li>tokens can be processed in parallel</li>
</ul>
<aside class="notes">
<p>Transformers is an innovation that makes a training a large language model
 practical.  Unlike RNNs or LSTMs, they do not rely on a hidden state that is
 carried sequentially.</p>
<p>Instead, the transformer computes <em>relationships</em> between all tokens in a
 sequence using the self-attention mechanism.  This means that during training,
 all tokens can be processed in parallel.</p>
<p>(Caching of) The relations between tokens will be crucial to inference
 performance, but for now, we can see transformer as just a composition of
 neural network blocks that predicts the next token with a sequence of previous
 ones.</p>
</aside>
<h3 id="training-of-llms">Training of LLMs<a class="headerlink" href="#training-of-llms" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/neural_network_training.png" style="height:350px;" /></p>
<ul>
<li>Just neural networkes that can be parallelized more efficiently;</li>
</ul>
<aside class="notes">

 Training of LLMs are not very different from what we talked about. But now we
 can have a rough view of the big picture, you have your data, you feed it to
 some memory, you let some processor work on it, get the gradient, the updated
 gradient, you put in another data, you continue.

</aside>

<h3 id="fine-tuninig-of-llms">Fine-tuninig of LLMs<a class="headerlink" href="#fine-tuninig-of-llms" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/fine_tuning.png" style="height:360px" /></p>
<ul>
<li>With specialized data (instruct, chat, etc);</li>
<li>Less memory usage by "freezing parameters"</li>
</ul>
<aside class="notes">
<p>Once a base model is trained, we usually fine-tune it on specific data
(instruct, chat, etc.).</p>
<p>From a computation point of view, fine-tuning is really the same task as
training, but you we can use some tricks to reduce the resource we need.</p>
<p>Above is a diagram of the LoRA (Low-Rank Adaptation) algorithm. Instead of
updating a full weight matrix, we consider updating it by a matrix product of
two small ones, this way we still need to do one copy of the big matrix, but the
backward path we just have the two low-rank matrices.</p>
</aside>
<h3 id="inference-of-llms">Inference of LLMs<a class="headerlink" href="#inference-of-llms" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/prefill_vs_decode.png" style="height:360px" /></p>
<ul>
<li>GPT-style inference: <em>pre-filling</em> and <em>decoding</em>;</li>
<li>Pre-filling: process the input prompt in parallel;</li>
<li>Decoding: generate new tokens one-by-one, using cached results.</li>
</ul>
<aside class="notes">
<p>Inference will need much less memory than training as we only need the forward
pass. But this is actually an interesting aspect of LLMs as compared to other
common machine learning tasks.</p>
<p>When inferencing with LLM, we are essentially two things:</p>
<ul>
<li><em>Pre-filling</em> — we process the entire prompt, this can be done in parallel
   efficiently;</li>
<li><em>decoding</em>: we generate one token at a time, but the intermediate results from
   previous can be cached as <em>key–value (KV) cache</em>, saving computation at the
   expense of memory.</li>
</ul>
<p>Think about what your task in mind, will it be more heavy in pre-filling or
decoding?</p>
</aside>
<h3 id="optimize-caches-for-inference">Optimize caches for inference<a class="headerlink" href="#optimize-caches-for-inference" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/paged_attention.gif" style="height:360px" /></p>
<ul>
<li>KV cache:</li>
<li>paged attention: indexed blockes of caches;</li>
<li>flash attention: fuse operations to reduce caches;</li>
</ul>
<p>more in-depth discussion of the technique where that visualization is from:
<a href="https://hamzaelshafie.bearblog.dev/paged-attention-from-first-principles-a-view-inside-vllm/">paged attention from first principles</a>.</p>
<aside class="notes">
<p>For this reason many effort in improving inference of LLMs has been put on
improving efficiency of memory accessing patterns and reducing the memory
needed. As an example, the paged attention mechanism groups adjacent tokens into
virtual memory "pages" like has been done in operating system kernels.</p>
<p>This allows us to efficiently use the fast memory on the GPUs. You can find more
examples and techniques in the blog linked.</p>
</aside>
<h3 id="key-takeaway">Key takeaway<a class="headerlink" href="#key-takeaway" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>LLMs/NNs benefit from massive parallelization;</li>
<li>Need for different tasks:</li>
<li>training: memory + compute + data throughput;</li>
<li>fine-tuninig: similar to training, cheaper;</li>
<li>pre-filling: compute;</li>
<li>decoding: memory;</li>
</ul>
<h2 id="llm-on-hpc-clusters">LLM on HPC clusters<a class="headerlink" href="#llm-on-hpc-clusters" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="llm-on-general-computers">LLM on general computers<a class="headerlink" href="#llm-on-general-computers" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Mostly about inference;</li>
<li>Quantization;</li>
<li>CPU offloading;</li>
<li>Memory-mapped file formats;</li>
</ul>
<h3 id="hpc-clusters">HPC clusters<a class="headerlink" href="#hpc-clusters" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/hpc_cluster.png" style="height:360px" /></p>
<ul>
<li>Racked computer nodes;</li>
<li>Parallel network storage;</li>
<li>Infiniband/RoCE networking;</li>
</ul>
<aside class="notes">
<p>HPC are designed for parallel computing; the hardware is
good at handing:</p>
<ul>
<li>fast communication between node;</li>
<li>fast access to storage (local or shared);</li>
<li>many CPUs/GPUs in one node</li>
</ul>
</aside>
<h3 id="alvis-hardware-compute">Alvis hardware - compute<a class="headerlink" href="#alvis-hardware-compute" title="Anchor link to this section for reference">&para;</a></h3>
<table>
<thead>
<tr>
<th style="text-align: right;">Data type</th>
<th>A100</th>
<th>A40</th>
<th>V100</th>
<th>T4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">FP64</td>
<td>9.7 | 19.5</td>
<td>0.58</td>
<td>7.8</td>
<td>0.25</td>
</tr>
<tr>
<td style="text-align: right;">FP32</td>
<td>19.5</td>
<td>37.4</td>
<td>15.7</td>
<td>8.1</td>
</tr>
<tr>
<td style="text-align: right;">TF32</td>
<td>156</td>
<td>74.8</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td style="text-align: right;">FP16</td>
<td>312</td>
<td>149.7</td>
<td>125</td>
<td>65</td>
</tr>
<tr>
<td style="text-align: right;">BF16</td>
<td>312</td>
<td>149.7</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td style="text-align: right;">Int8</td>
<td>624</td>
<td>299.3</td>
<td>64</td>
<td>130</td>
</tr>
<tr>
<td style="text-align: right;">Int4</td>
<td>1248</td>
<td>598.7</td>
<td>N/A</td>
<td>260</td>
</tr>
</tbody>
</table>
<!-- - _\*Performance on -->
<!--   [Tensor Core](https://en.wikipedia.org/wiki/Deep_learning_super_sampling#Architecture)._ -->
<!-- - _\*\*Up to a factor of two faster with -->
<!--   [sparsity](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)._ -->

<aside class="notes">
<p>Alvis was build for AI research, so it's equipped with latest (at the time) GPU
acceleration cards. They are capable of doing fast floating point operations
in reduced precision (see next section).</p>
</aside>
<h3 id="alvis-hardware-network-storage">Alvis hardware - network &amp; storage<a class="headerlink" href="#alvis-hardware-network-storage" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/gpu_direct.png" style="height:200px" /></p>
<ul>
<li>Fast storage: <a href="https://docs.weka.io/weka-system-overview/about/weka-system-functionality-features">WEKA file system</a>;</li>
<li>Infiniband: 100Gbit (A100 nodes);</li>
<li>Ethernet: 25Gbit (most other nodes);</li>
</ul>
<aside class="notes">
<p>Not only so, Alvis is also equipped with fast storage system backed by flash
storage; on the most powerful nodes (4xA100 GPUs) infiniband network that goes
directly to storage is available.</p>
<p>They were designed to facilitate fast loading of data, which is useful for any
training tasks. In the case of LLM, one should already benefit from that for
training.</p>
<p>It is worth noting that the while one typically do not need such fast storage
for inference, the LLM inference can actually take advantage of fast storage
backend, this is rather experimental, but take a look at the <a href="https://docs.lmcache.ai/kv_cache/storage_backends/infinistore.html">LMcache</a> package
if you want to optimize you inference tasks for real.</p>
</aside>
<h3 id="running-llms-on-supercomputers">Running LLMs on supercomputers<a class="headerlink" href="#running-llms-on-supercomputers" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>Most common bottleneck: <strong>memory</strong></li>
<li>Quantized models to fit larger models;</li>
<li>Parallelize the model across GPUs or nodes;</li>
</ul>
<aside class="notes">
<p>Supercomputers allow us to run larger LLMs because of not only the more powerful
nodes, but also the nodes are connected with fast internet connection. But we
still have the same issue as most high performance computing tasks, the compute
needs to access the memory and the data need to transferred.</p>
<p>Most commonly, when you try to run a large model (70B and 400B) parameters, you
need to split the models to many GPUs. You can choose to quantize the model, so
that the model use less memory, and you need less GPUs and less parallelization
issues. To this end, you will need to look up the compatibility between the
model, hardware and implementation.</p>
<p>You will also need to think about how do you parallelize the models, you have
the options:</p>
<ul>
<li>Tensor parallelism;</li>
<li>Pipeline parallelism;</li>
<li>Data parallelism</li>
</ul>
<p>So next we will introduce the different formats in LLMs, to give you an idea of
what it means when you download a certain model, and tomorrow we will walk through
the</p>
</aside>
<h3 id="tools-to-gain-information">Tools to gain information<a class="headerlink" href="#tools-to-gain-information" title="Anchor link to this section for reference">&para;</a></h3>
<p><img alt="" src="../figures/grafana.png" style="height:360px" /></p>
<ul>
<li>grafana (network utilization, temp disk);</li>
<li>nvtop, htop (CPU/GPU utilization, power draw);</li>
<li>nvidia nsight (advanced debugging and tracing);</li>
</ul>
<p>See details in C3SE documentation.</p>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Anchor link to this section for reference">&para;</a></h2>
<h3 id="take-home-messages">Take home messages<a class="headerlink" href="#take-home-messages" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>LLMs/neural networks benefit from massive parallelization;</li>
<li>Same issue of memeory vs. compute-bound;</li>
<li>Some optimization strategies;</li>
<li>Be aware of the troubleshooting tools!</li>
</ul>
<h3 id="useful-links">Useful links<a class="headerlink" href="#useful-links" title="Anchor link to this section for reference">&para;</a></h3>
<ul>
<li>nanotron has some in-depth discussion about the efficiency of model training;
  (<a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook</a>), as well as a <a href="https://huggingface.co/spaces/nanotron/predict_memory">prediction memory</a> estimation tool;</li>
<li>Alvis <a href="https://www.c3se.chalmers.se/about/Alvis/#gpu-hardware-details">hardware specifications</a>;</li>
<li>Alvis <a href="https://www.c3se.chalmers.se/documentation/submitting_jobs/monitoring/">monitoring tools</a>;</li>
</ul>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="https://www.naiss.se/" target="_blank" rel="noopener" title="NAISS" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.path", "navigation.footer", "navigation.expand", "search.suggest", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.action.edit", "content.action.view", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
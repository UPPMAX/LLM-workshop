{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec58f01-bc87-409e-aaa8-60b860528071",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dcada-5ebd-44c2-a1db-d0ab267b5882",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Quantization Techniques](#quantization-techniques)\n",
    "- [Linear Quantization](#linear)\n",
    "- [GPTQ Quantization](#gptq)\n",
    "- [Other Methods](#other-methods)\n",
    "- [Summary](#summary)\n",
    "- [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b25eca-0d1d-4973-b4d0-26a5bffaeb21",
   "metadata": {},
   "source": [
    "<a id=\"qunatization-techniques\"></a>\n",
    "## Quantization Techniques\n",
    "\n",
    "- Post trainging quantization (PTQ):\n",
    "    - Post training dynamic quantization: the range for each activation is computed on the fly at runtime.\n",
    "    - Post training static quantization: the range for each activation is computed in advance at quantization-time, typically by passing representative data thro\n",
    "        - Observers are put on activations to record their values.\n",
    "        - A certain number of forward passes on a calibration dataset is done (around 200 examples is enough).\n",
    "        - The ranges for each computation are computed according to some calibration technique.\n",
    "- Quantization aware training (QAT): the range for each activation is computed at training-time. They simulate the error induced by quantization to let the model\n",
    "\n",
    "Reference: https://huggingface.co/docs/optimum/concept_guides/quantization#calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a312-e461-4fc3-a53d-1dad7ca9891c",
   "metadata": {},
   "source": [
    "<a id=\"linear\"></a>\n",
    "## Linear quantization\n",
    "\n",
    "![](/LLM-workshop/day1/figures/quantization_symmetry.webp)\n",
    "\n",
    "- $x = S * (x_q - Z)$\n",
    "- When $Z = 0$: symmetrics quantization\n",
    "- It can be applied per tensor or per channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e0466-771d-44bb-ae87-a31b14650ac7",
   "metadata": {},
   "source": [
    "### Affine quantization in Quanto (Int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f851e89-0054-4d72-8959-9dc91a2f9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.quanto import QuantizedModelForCausalLM, qint8\n",
    "\n",
    "# https://www.geeksforgeeks.org/nlp/perplexity-for-llm-evaluation/\n",
    "def compute_perplexity_for_batch(model, tokenizer, input_texts):\n",
    "    inputs = tokenizer(\n",
    "        input_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    shift_logits = logits[:, :-1, :] \n",
    "    shift_labels = input_ids[:, 1:] \n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "    target_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    target_log_probs = target_log_probs * attention_mask[:, 1:].to(log_probs.dtype)\n",
    "    negative_log_likelihood = -target_log_probs.sum(dim=-1) / attention_mask[:, 1:].sum(dim=-1)\n",
    "    perplexities = torch.exp(negative_log_likelihood)\n",
    "    mean_perplexity_score = torch.mean(perplexities)\n",
    "\n",
    "    return {\n",
    "        \"perplexities\": perplexities.tolist(),\n",
    "        \"mean_perplexity\": mean_perplexity_score.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2abffa-9d14-4be9-892d-c6cec355620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.0179,  0.0066,  0.0247,  ..., -0.0087, -0.0117,  0.0201],\n",
      "        [ 0.0122,  0.0593,  0.0552,  ..., -0.0332, -0.0154,  0.0108],\n",
      "        [ 0.0178,  0.0155,  0.0344,  ..., -0.0386, -0.0386, -0.0276],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0352,  0.0713,  ..., -0.0718, -0.0265, -0.0287],\n",
      "        [ 0.0226, -0.0248,  0.0352,  ..., -0.0120, -0.0287, -0.0148],\n",
      "        [-0.0258, -0.0537, -0.0131,  ...,  0.0542,  0.0096, -0.0028]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(model)\n",
    "print(model.model.layers[0].self_attn.q_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333bc8e4-d097-44ab-a653-313acc14172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores for each text: [42.679718017578125, 16.073394775390625]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "example_texts = [\n",
    "    \"Once upon a time, there was a brave knight.\",\n",
    "    \"In a galaxy far, far away, a new adventure began.\"\n",
    "]\n",
    "\n",
    "# Compute perplexity scores for the batch of input texts\n",
    "results = compute_perplexity_for_batch(model, tokenizer, example_texts)\n",
    "print(f\"Perplexity scores for each text: {results['perplexities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838b7eaf-c5b1-4ca0-bb03-415fd77466a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "<class 'optimum.quanto.tensor.weights.qbytes.WeightQBytesTensor'>(tensor([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "        [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "        [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "        ...,\n",
      "        [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "        [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "        [-14, -30,  -7,  ...,  30,   5,  -2]], dtype=torch.int8), scale=tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        ...,\n",
      "        [0.0018],\n",
      "        [0.0007],\n",
      "        [0.0018]]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "qmodel = QuantizedModelForCausalLM.quantize(model, weights=qint8, exclude='lm_head')\n",
    "print(qmodel)\n",
    "print(qmodel.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "qmodel.save_pretrained('output/official/QLlama-3.2-1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a628c1ec-ec23-4bf1-8709-fc1ef6feb4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores for each text: [42.3848876953125, 16.23394012451172]\n"
     ]
    }
   ],
   "source": [
    "results = compute_perplexity_for_batch(qmodel, tokenizer, example_texts)\n",
    "print(f\"Perplexity scores for each text: {results['perplexities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae5e79-8c3e-497d-8cdd-9af98b8f1431",
   "metadata": {},
   "source": [
    "### Quanto integration in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6e3e47-6d83-4315-b143-015be5c0e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "<class 'optimum.quanto.tensor.weights.qbytes.WeightQBytesTensor'>(tensor([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "        [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "        [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "        ...,\n",
      "        [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "        [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "        [-14, -30,  -7,  ...,  30,   5,  -2]], dtype=torch.int8), scale=tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        ...,\n",
      "        [0.0018],\n",
      "        [0.0007],\n",
      "        [0.0018]]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, QuantoConfig\n",
    "\n",
    "quantization_config = QuantoConfig(weights=\"int8\", activations=None)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "print(model)\n",
    "print(model.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "# quanto quantized model cannot be serialized from transformers and cannot be saved\n",
    "# model.save_pretrained(\"output/transformers/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c3d49-4798-4097-ae5a-605d06a7f512",
   "metadata": {},
   "source": [
    "### Activation quantiztion / Calibration in Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23fc5694-3518-4956-9d8b-07f796666789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3516a99db3439e921cc9a66b3ecc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.quanto import quantize, freeze, qint8, Calibration, quantization_map\n",
    "from safetensors.torch import save_file\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\", use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    ").select(range(1024))\n",
    "\n",
    "quantize(model, weights=qint8, activations=qint8)\n",
    "with torch.no_grad(), Calibration(momentum=0.9):\n",
    "    model.eval()\n",
    "    for batch in calibration_dataset.iter(batch_size=2):\n",
    "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # good habit\n",
    "        del input_ids, attention_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4d22a0-c8db-454a-bba0-7a47f9de09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(\"output/calibration\", exist_ok=True)\n",
    "\n",
    "# Freeze integer weights\n",
    "freeze(model)\n",
    "\n",
    "# Serialize quantized model\n",
    "save_file(model.state_dict(), 'output/calibration/QLlama-3.2-1B/model.safetensors')\n",
    "# Store the quantized models quantization map\n",
    "with open('output/calibration/QLlama-3.2-1B/quantization_map.json', 'w') as f:\n",
    "    json.dump(quantization_map(model), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8e2380-1a79-4145-8d44-4af3ac571aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "from optimum.quanto import requantize\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "state_dict = load_file('output/calibration/QLlama-3.2-1B/model.safetensors')\n",
    "with open('output/calibration/QLlama-3.2-1B/quantization_map.json', 'r') as f:\n",
    "    quantization_map = json.load(f)\n",
    "\n",
    "# Create an empty model from your modeling code and requantize it\n",
    "config = AutoConfig.from_pretrained(\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/config.json\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "requantize(model, state_dict, quantization_map, device=torch.device('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde5ba1a-b106-4bfb-b317-786cfc3890a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea932f5-674f-4486-b658-589e64af2797",
   "metadata": {},
   "source": [
    "### Outlier problem\n",
    "\n",
    "- Quanto simply uses `absmax()` to calculate scale\n",
    "- Outlier would compress most values to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26311fb7-5665-46cd-84ca-a11ad6e60152",
   "metadata": {},
   "source": [
    "### LLM.int8() in Bitsandbytes\n",
    "\n",
    "- Save outlier in another tensor to keep information\n",
    "- Model is quantized on the fly without loading model in full precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf04ff-a416-4e0f-bb9b-aa1d24b9f03c",
   "metadata": {},
   "source": [
    "![](/LLM-workshop/day2/figures/bitsandbytes_int8.png)\n",
    "(Credit: [Dettmers+2022](https://arxiv.org/abs/2208.07339))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ce780-67ad-4b87-a34f-5b5a64a98d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config, \n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d70edba-6a99-4ab2-bfb1-53b61ae04903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight\n",
      "Parameter containing:\n",
      "Parameter(Int8Params([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "            [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "            [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "            ...,\n",
      "            [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "            [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "            [-14, -30,  -7,  ...,  30,   5,  -2]], device='cuda:0',\n",
      "           dtype=torch.int8))\n",
      "tensor([0.0515, 0.1079, 0.1436,  ..., 0.2256, 0.0898, 0.2305], device='cuda:0')\n",
      "1.4985504\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():  \n",
    "    if hasattr(param, \"SCB\"):\n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(param.SCB)\n",
    "        break\n",
    "print(model.get_memory_footprint() / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a416ed4-eb2a-4a57-bcf0-90fcc3f0a9cc",
   "metadata": {},
   "source": [
    "<a id=\"gptq\"></a>\n",
    "## GPTQ (Generalized Post-Training Quantization) Quantization\n",
    "\n",
    "- Error compensation: after quantizing a specific weight (or group of weights), GPTQ calculates the error introduced by this quantization step and immediately updates the remaining, not-yet-quantized weights in the layer to counteract that error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e3688-eee6-43ad-a1cb-66d09651a5af",
   "metadata": {},
   "source": [
    "### GPTQ in [GPTQModel](https://github.com/ModelCloud/GPTQModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97ea08d-0342-4b48-a8d2-82351766f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128004 (token='<|finetune_right_pad_id|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_mythopoetry_time_11_03_2025_11h_03m_44s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[96m0.30120248\u001b[0m | 1024        | 0.01000     | 2.109     | 3.436        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00808920\u001b[0m | 1024        | 0.01000     | 0.461     | 3.436        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[96m0.61797380\u001b[0m | 1024        | 0.01000     | 0.464     | 3.436        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00077160\u001b[0m | 1024        | 0.01000     | 0.465     | 2.474        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[96m0.51238197\u001b[0m | 1024        | 0.01000     | 0.474     | 2.884        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[96m0.64604568\u001b[0m | 1024        | 0.01000     | 0.471     | 2.884        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.00407102\u001b[0m | 1024        | 0.01000     | 2.088     | 7.244        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[96m0.49384868\u001b[0m | 1024        | 0.01000     | 0.468     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.02870745\u001b[0m | 1024        | 0.01000     | 0.467     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[96m0.99181962\u001b[0m | 1024        | 0.01000     | 0.468     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00305457\u001b[0m | 1024        | 0.01000     | 0.474     | 2.230        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[96m0.76506805\u001b[0m | 1024        | 0.01000     | 0.482     | 2.643        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[93m1.05032837\u001b[0m | 1024        | 0.01000     | 0.479     | 2.643        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[93m1.69528842\u001b[0m | 1024        | 0.01000     | 2.005     | 6.993        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[93m1.03624225\u001b[0m | 1024        | 0.01000     | 0.467     | 2.956        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.06973191\u001b[0m | 1024        | 0.01000     | 0.469     | 2.956        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[93m1.98996377\u001b[0m | 1024        | 0.01000     | 0.467     | 2.956        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.00341675\u001b[0m | 1024        | 0.01000     | 0.466     | 2.224        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[93m1.00305104\u001b[0m | 1024        | 0.01000     | 0.479     | 2.626        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[93m1.58601117\u001b[0m | 1024        | 0.01000     | 0.481     | 2.626        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.01273223\u001b[0m | 1024        | 0.01000     | 1.994     | 6.989        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[96m0.66815579\u001b[0m | 1024        | 0.01000     | 0.470     | 2.955        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.09228896\u001b[0m | 1024        | 0.01000     | 0.469     | 2.955        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[93m1.46644449\u001b[0m | 1024        | 0.01000     | 0.468     | 2.955        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.00657938\u001b[0m | 1024        | 0.01000     | 0.468     | 2.224        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[93m1.24055433\u001b[0m | 1024        | 0.01000     | 0.481     | 2.635        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[93m2.43714857\u001b[0m | 1024        | 0.01000     | 0.480     | 2.635        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.01896082\u001b[0m | 1024        | 0.01000     | 2.009     | 7.003        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[96m0.72263455\u001b[0m | 1024        | 0.01000     | 0.468     | 2.959        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.08654030\u001b[0m | 1024        | 0.01000     | 0.466     | 2.959        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[93m1.49871469\u001b[0m | 1024        | 0.01000     | 0.469     | 2.959        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.00970966\u001b[0m | 1024        | 0.01000     | 0.467     | 2.228        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[93m1.27198529\u001b[0m | 1024        | 0.01000     | 0.483     | 2.634        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[93m2.70081949\u001b[0m | 1024        | 0.01000     | 0.480     | 2.634        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.02205210\u001b[0m | 1024        | 0.01000     | 2.014     | 7.000        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[93m1.15391111\u001b[0m | 1024        | 0.01000     | 0.470     | 2.964        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.07711438\u001b[0m | 1024        | 0.01000     | 0.466     | 2.964        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[93m1.99201381\u001b[0m | 1024        | 0.01000     | 0.469     | 2.964        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.01008460\u001b[0m | 1024        | 0.01000     | 0.468     | 2.228        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[93m1.38432539\u001b[0m | 1024        | 0.01000     | 0.481     | 2.634        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[93m2.51981807\u001b[0m | 1024        | 0.01000     | 0.480     | 2.634        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.02659231\u001b[0m | 1024        | 0.01000     | 2.015     | 7.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[96m0.89122164\u001b[0m | 1024        | 0.01000     | 0.468     | 2.969        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.09944867\u001b[0m | 1024        | 0.01000     | 0.467     | 2.969        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[93m1.41462255\u001b[0m | 1024        | 0.01000     | 0.470     | 2.969        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.01492018\u001b[0m | 1024        | 0.01000     | 0.475     | 2.232        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[93m1.40714431\u001b[0m | 1024        | 0.01000     | 0.484     | 2.636        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[93m2.51485586\u001b[0m | 1024        | 0.01000     | 0.482     | 2.636        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.02716633\u001b[0m | 1024        | 0.01000     | 2.006     | 7.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[96m0.88762063\u001b[0m | 1024        | 0.01000     | 0.470     | 2.966        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[96m0.11456488\u001b[0m | 1024        | 0.01000     | 0.470     | 2.966        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[93m1.67333174\u001b[0m | 1024        | 0.01000     | 0.470     | 2.966        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.01489709\u001b[0m | 1024        | 0.01000     | 0.470     | 2.232        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[93m1.47728646\u001b[0m | 1024        | 0.01000     | 0.486     | 2.641        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[93m2.38897085\u001b[0m | 1024        | 0.01000     | 0.485     | 2.641        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.03020864\u001b[0m | 1024        | 0.01000     | 1.996     | 7.028        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[93m1.08520103\u001b[0m | 1024        | 0.01000     | 0.469     | 2.968        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[96m0.10937826\u001b[0m | 1024        | 0.01000     | 0.468     | 2.968        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[93m1.79538739\u001b[0m | 1024        | 0.01000     | 0.466     | 2.968        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.01826451\u001b[0m | 1024        | 0.01000     | 0.470     | 2.230        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[93m1.64138269\u001b[0m | 1024        | 0.01000     | 0.483     | 2.640        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[93m2.58196330\u001b[0m | 1024        | 0.01000     | 0.482     | 2.640        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.04072438\u001b[0m | 1024        | 0.01000     | 2.019     | 7.020        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[96m0.84914976\u001b[0m | 1024        | 0.01000     | 0.467     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[96m0.13325590\u001b[0m | 1024        | 0.01000     | 0.468     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[93m2.07214022\u001b[0m | 1024        | 0.01000     | 0.470     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.02500872\u001b[0m | 1024        | 0.01000     | 0.471     | 2.239        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[93m1.74224305\u001b[0m | 1024        | 0.01000     | 0.486     | 2.645        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[93m2.82709551\u001b[0m | 1024        | 0.01000     | 0.486     | 2.645        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.04959796\u001b[0m | 1024        | 0.01000     | 2.014     | 7.033        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[93m1.04439735\u001b[0m | 1024        | 0.01000     | 0.472     | 2.979        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[96m0.16073897\u001b[0m | 1024        | 0.01000     | 0.468     | 2.979        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[93m2.18384790\u001b[0m | 1024        | 0.01000     | 0.466     | 2.979        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.01789060\u001b[0m | 1024        | 0.01000     | 0.468     | 2.233        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[93m2.05724478\u001b[0m | 1024        | 0.01000     | 0.482     | 2.646        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[93m3.22427845\u001b[0m | 1024        | 0.01000     | 0.480     | 2.646        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.06237715\u001b[0m | 1024        | 0.01000     | 2.010     | 7.021        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[93m1.26090741\u001b[0m | 1024        | 0.01000     | 0.468     | 2.972        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[96m0.15488043\u001b[0m | 1024        | 0.01000     | 0.467     | 2.972        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[93m2.14639568\u001b[0m | 1024        | 0.01000     | 0.469     | 2.972        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.01430082\u001b[0m | 1024        | 0.01000     | 0.471     | 2.232        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[93m2.24924612\u001b[0m | 1024        | 0.01000     | 0.481     | 2.644        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[93m3.45779753\u001b[0m | 1024        | 0.01000     | 0.480     | 2.644        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.06777903\u001b[0m | 1024        | 0.01000     | 2.015     | 7.018        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[93m1.29090023\u001b[0m | 1024        | 0.01000     | 0.468     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[96m0.15683806\u001b[0m | 1024        | 0.01000     | 0.467     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[93m2.15606308\u001b[0m | 1024        | 0.01000     | 0.468     | 2.973        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.01496769\u001b[0m | 1024        | 0.01000     | 0.471     | 2.236        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[93m2.36392450\u001b[0m | 1024        | 0.01000     | 0.482     | 2.640        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[93m3.44941211\u001b[0m | 1024        | 0.01000     | 0.480     | 2.640        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.07523597\u001b[0m | 1024        | 0.01000     | 2.012     | 7.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[93m1.26744676\u001b[0m | 1024        | 0.01000     | 0.471     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[96m0.25983214\u001b[0m | 1024        | 0.01000     | 0.468     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[93m2.47879696\u001b[0m | 1024        | 0.01000     | 0.469     | 2.967        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.02049054\u001b[0m | 1024        | 0.01000     | 0.469     | 2.229        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[93m2.79143095\u001b[0m | 1024        | 0.01000     | 0.485     | 2.641        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[93m3.74195242\u001b[0m | 1024        | 0.01000     | 0.481     | 2.641        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[96m0.11398240\u001b[0m | 1024        | 0.01000     | 2.005     | 7.021        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[93m1.45823216\u001b[0m | 1024        | 0.01000     | 0.469     | 2.977        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[96m0.58275676\u001b[0m | 1024        | 0.01000     | 0.467     | 2.977        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[93m2.67891359\u001b[0m | 1024        | 0.01000     | 0.471     | 2.977        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.05191090\u001b[0m | 1024        | 0.01000     | 0.472     | 2.241        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[93m3.31205702\u001b[0m | 1024        | 0.01000     | 0.485     | 2.644        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[93m4.80369759\u001b[0m | 1024        | 0.01000     | 0.483     | 2.644        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[96m0.16251093\u001b[0m | 1024        | 0.01000     | 2.017     | 7.024        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[93m1.39100635\u001b[0m | 1024        | 0.01000     | 0.468     | 2.975        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[96m0.59320849\u001b[0m | 1024        | 0.01000     | 0.467     | 2.975        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[93m2.49978352\u001b[0m | 1024        | 0.01000     | 0.469     | 2.975        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[96m0.18476078\u001b[0m | 1024        | 0.01000     | 0.472     | 2.236        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[93m4.28467274\u001b[0m | 1024        | 0.01000     | 0.483     | 2.650        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[33m5.68962288\u001b[0m | 1024        | 0.01000     | 0.482     | 2.650        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[96m0.43330643\u001b[0m | 1024        | 0.01000     | 2.024     | 7.063        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.30120248', 'samples': '1024', 'damp': '0.01000', 'time': '2.109', 'fwd_time': '3.436'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00808920', 'samples': '1024', 'damp': '0.01000', 'time': '0.461', 'fwd_time': '3.436'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.61797380', 'samples': '1024', 'damp': '0.01000', 'time': '0.464', 'fwd_time': '3.436'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00077160', 'samples': '1024', 'damp': '0.01000', 'time': '0.465', 'fwd_time': '2.474'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.51238197', 'samples': '1024', 'damp': '0.01000', 'time': '0.474', 'fwd_time': '2.884'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.64604568', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.884'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00407102', 'samples': '1024', 'damp': '0.01000', 'time': '2.088', 'fwd_time': '7.244'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.49384868', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.02870745', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.99181962', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00305457', 'samples': '1024', 'damp': '0.01000', 'time': '0.474', 'fwd_time': '2.230'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.76506805', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.643'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '1.05032837', 'samples': '1024', 'damp': '0.01000', 'time': '0.479', 'fwd_time': '2.643'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '1.69528842', 'samples': '1024', 'damp': '0.01000', 'time': '2.005', 'fwd_time': '6.993'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '1.03624225', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.956'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.06973191', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.956'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '1.98996377', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.956'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00341675', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.224'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '1.00305104', 'samples': '1024', 'damp': '0.01000', 'time': '0.479', 'fwd_time': '2.626'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '1.58601117', 'samples': '1024', 'damp': '0.01000', 'time': '0.481', 'fwd_time': '2.626'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.01273223', 'samples': '1024', 'damp': '0.01000', 'time': '1.994', 'fwd_time': '6.989'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.66815579', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.955'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.09228896', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.955'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '1.46644449', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.955'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00657938', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.224'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '1.24055433', 'samples': '1024', 'damp': '0.01000', 'time': '0.481', 'fwd_time': '2.635'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '2.43714857', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.635'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.01896082', 'samples': '1024', 'damp': '0.01000', 'time': '2.009', 'fwd_time': '7.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.72263455', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.08654030', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '1.49871469', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00970966', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.228'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '1.27198529', 'samples': '1024', 'damp': '0.01000', 'time': '0.483', 'fwd_time': '2.634'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '2.70081949', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.634'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.02205210', 'samples': '1024', 'damp': '0.01000', 'time': '2.014', 'fwd_time': '7.000'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '1.15391111', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.964'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.07711438', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.964'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '1.99201381', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.964'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.01008460', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.228'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '1.38432539', 'samples': '1024', 'damp': '0.01000', 'time': '0.481', 'fwd_time': '2.634'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '2.51981807', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.634'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.02659231', 'samples': '1024', 'damp': '0.01000', 'time': '2.015', 'fwd_time': '7.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.89122164', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.969'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.09944867', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.969'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '1.41462255', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.969'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.01492018', 'samples': '1024', 'damp': '0.01000', 'time': '0.475', 'fwd_time': '2.232'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '1.40714431', 'samples': '1024', 'damp': '0.01000', 'time': '0.484', 'fwd_time': '2.636'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '2.51485586', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.636'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.02716633', 'samples': '1024', 'damp': '0.01000', 'time': '2.006', 'fwd_time': '7.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.88762063', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.966'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.11456488', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.966'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '1.67333174', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.966'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.01489709', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.232'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '1.47728646', 'samples': '1024', 'damp': '0.01000', 'time': '0.486', 'fwd_time': '2.641'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '2.38897085', 'samples': '1024', 'damp': '0.01000', 'time': '0.485', 'fwd_time': '2.641'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.03020864', 'samples': '1024', 'damp': '0.01000', 'time': '1.996', 'fwd_time': '7.028'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '1.08520103', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.968'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.10937826', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.968'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '1.79538739', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.968'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.01826451', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.230'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '1.64138269', 'samples': '1024', 'damp': '0.01000', 'time': '0.483', 'fwd_time': '2.640'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '2.58196330', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.640'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.04072438', 'samples': '1024', 'damp': '0.01000', 'time': '2.019', 'fwd_time': '7.020'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.84914976', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.13325590', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '2.07214022', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.02500872', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.239'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '1.74224305', 'samples': '1024', 'damp': '0.01000', 'time': '0.486', 'fwd_time': '2.645'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '2.82709551', 'samples': '1024', 'damp': '0.01000', 'time': '0.486', 'fwd_time': '2.645'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.04959796', 'samples': '1024', 'damp': '0.01000', 'time': '2.014', 'fwd_time': '7.033'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '1.04439735', 'samples': '1024', 'damp': '0.01000', 'time': '0.472', 'fwd_time': '2.979'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.16073897', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.979'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '2.18384790', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.979'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.01789060', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.233'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '2.05724478', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.646'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '3.22427845', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.646'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.06237715', 'samples': '1024', 'damp': '0.01000', 'time': '2.010', 'fwd_time': '7.021'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '1.26090741', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.972'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.15488043', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.972'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '2.14639568', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.972'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.01430082', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.232'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '2.24924612', 'samples': '1024', 'damp': '0.01000', 'time': '0.481', 'fwd_time': '2.644'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '3.45779753', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.644'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.06777903', 'samples': '1024', 'damp': '0.01000', 'time': '2.015', 'fwd_time': '7.018'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '1.29090023', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.15683806', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '2.15606308', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.01496769', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.236'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '2.36392450', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.640'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '3.44941211', 'samples': '1024', 'damp': '0.01000', 'time': '0.480', 'fwd_time': '2.640'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.07523597', 'samples': '1024', 'damp': '0.01000', 'time': '2.012', 'fwd_time': '7.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '1.26744676', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.25983214', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '2.47879696', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.967'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.02049054', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.229'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '2.79143095', 'samples': '1024', 'damp': '0.01000', 'time': '0.485', 'fwd_time': '2.641'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '3.74195242', 'samples': '1024', 'damp': '0.01000', 'time': '0.481', 'fwd_time': '2.641'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.11398240', 'samples': '1024', 'damp': '0.01000', 'time': '2.005', 'fwd_time': '7.021'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '1.45823216', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.977'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.58275676', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.977'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '2.67891359', 'samples': '1024', 'damp': '0.01000', 'time': '0.471', 'fwd_time': '2.977'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.05191090', 'samples': '1024', 'damp': '0.01000', 'time': '0.472', 'fwd_time': '2.241'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '3.31205702', 'samples': '1024', 'damp': '0.01000', 'time': '0.485', 'fwd_time': '2.644'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '4.80369759', 'samples': '1024', 'damp': '0.01000', 'time': '0.483', 'fwd_time': '2.644'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.16251093', 'samples': '1024', 'damp': '0.01000', 'time': '2.017', 'fwd_time': '7.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '1.39100635', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.975'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.59320849', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.975'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '2.49978352', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.975'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.18476078', 'samples': '1024', 'damp': '0.01000', 'time': '0.472', 'fwd_time': '2.236'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '4.28467274', 'samples': '1024', 'damp': '0.01000', 'time': '0.483', 'fwd_time': '2.650'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '5.68962288', 'samples': '1024', 'damp': '0.01000', 'time': '0.482', 'fwd_time': '2.650'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.43330643', 'samples': '1024', 'damp': '0.01000', 'time': '2.024', 'fwd_time': '7.063'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TritonV2QuantLinear, TorchQuantLinear]`          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TritonV2QuantLinear`.                               \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                         \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 4,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "quanto_qmap.json\n",
      "quantize_config.json\n",
      "README.md\n",
      "model.safetensors\n",
      "config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"do_sample\": true,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"temperature\": 0.6,\n",
      "    \"top_p\": 0.9,\n",
      "    \"transformers_version\": \"4.53.2\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"max_position_embeddings\": 131072,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 16,\n",
      "    \"num_key_value_heads\": 8,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 4,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": {\n",
      "        \"factor\": 32.0,\n",
      "        \"high_freq_factor\": 4.0,\n",
      "        \"low_freq_factor\": 1.0,\n",
      "        \"original_max_position_embeddings\": 8192,\n",
      "        \"rope_type\": \"llama3\"\n",
      "    },\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.53.2\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 128256\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2357.14MB, 2.30GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 984.55MB, 0.96GB                                   \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1372.59MB, 1.34GB - 58.23%                              \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    ").select(range(1024))[\"text\"]\n",
    "\n",
    "quant_config = QuantizeConfig(bits=4)\n",
    "\n",
    "model = GPTQModel.load(model_name, quant_config)\n",
    "\n",
    "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
    "model.quantize(calibration_dataset, batch_size=2)\n",
    "\n",
    "model.save(\"output/official/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c64ad-4d99-4d48-a9b2-fe513546df80",
   "metadata": {},
   "source": [
    "### GPTQ integration in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d70089-f733-4d84-9ca4-762ece64a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386de8b877e64f31bf5b0c88a64987b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en/c4-train.00000-of-01024.json.gz:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b83c8340f4625b095acbd554b88de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26fd50a72c546d28aea546eb7cd00f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 1/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 2/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 3/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 4/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 5/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 6/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 7/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 8/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 9/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 10/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 11/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 12/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 13/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 14/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 15/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 16/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Packing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (k_proj): TritonV2QuantLinear()\n",
      "          (o_proj): TritonV2QuantLinear()\n",
      "          (q_proj): TritonV2QuantLinear()\n",
      "          (v_proj): TritonV2QuantLinear()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (act_fn): SiLU()\n",
      "          (down_proj): TritonV2QuantLinear()\n",
      "          (gate_proj): TritonV2QuantLinear()\n",
      "          (up_proj): TritonV2QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "1032327296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output/transformers/QLlama-3.2-1B/tokenizer_config.json',\n",
       " 'output/transformers/QLlama-3.2-1B/special_tokens_map.json',\n",
       " 'output/transformers/QLlama-3.2-1B/chat_template.jinja',\n",
       " 'output/transformers/QLlama-3.2-1B/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "gptq_config=GPTQConfig(\n",
    "    bits=4,\n",
    "    dataset=\"c4\", # optimum will download 'en/c4-train.00000-of-01024.json.gz'\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "quantized_model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=gptq_config\n",
    ")\n",
    "print(quantized_model)\n",
    "print(quantized_model.get_memory_footprint())\n",
    "\n",
    "quantized_model.save_pretrained(\"output/transformers/QLlama-3.2-1B\")\n",
    "tokenizer.save_pretrained(\"output/transformers/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7b61e-9963-4666-a02b-80e049b6aa48",
   "metadata": {},
   "source": [
    "<a id=\"other-methods\"></a>\n",
    "## Other quantization methods supported in Transformers\n",
    "\n",
    "- https://huggingface.co/docs/transformers/quantization/overview\n",
    "\n",
    "| Quantization Method                        | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon)              | Intel GPU       | Torch compile() | Bits         | PEFT Fine Tuning | Serializable with ðŸ¤—Transformers | ðŸ¤—Transformers Support  | Link to library                                         |\n",
    "|--------------------------------------------|-------------------------|-----------------|----------|-----------|------------------------------------|-----------------|-----------------|--------------|------------------|----------------------------------|-------------------------|---------------------------------------------------------|\n",
    "| [AQLM](./aqlm)                             | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸŸ¢              | ðŸŸ¢              | 1/2          | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/Vahe1994/AQLM                        |\n",
    "| [AutoRound](./auto_round)                  | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸŸ¢              | ðŸ”´              | 2/3/4/8      | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/intel/auto-round                     |\n",
    "| [AWQ](./awq)                               | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸŸ¢        | ðŸ”´                                 | ðŸŸ¢              | ?               | 4            | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/casper-hansen/AutoAWQ                |\n",
    "| [bitsandbytes](./bitsandbytes)             | ðŸŸ¢                      | ðŸŸ¢              | ðŸŸ¢       | ðŸŸ¡        | ðŸŸ¡                                 | ðŸŸ¢              | ðŸŸ¢              | 4/8          | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n",
    "| [compressed-tensors](./compressed_tensors) | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 1/8          | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/neuralmagic/compressed-tensors       |\n",
    "| [EETQ](./eetq)                             | ðŸŸ¢                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ?               | 8            | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/NetEase-FuXi/EETQ                    |\n",
    "| [FP-Quant](./fp_quant)                     | ðŸŸ¢                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 4            | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/IST-DASLab/FP-Quant                  |\n",
    "| [GGUF / GGML (llama.cpp)](../gguf)         | ðŸŸ¢                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸŸ¢                                 | ðŸŸ¢              | ðŸ”´              | 1/8          | ðŸ”´               | [See Notes](../gguf)             | [See Notes](../gguf)    | https://github.com/ggerganov/llama.cpp                  |\n",
    "| [GPTQModel](./gptq)                        | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸŸ¢        | ðŸŸ¢                                 | ðŸŸ¢              | ðŸ”´              | 2/3/4/8      | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/ModelCloud/GPTQModel                 |\n",
    "| [AutoGPTQ](./gptq)                         | ðŸ”´                      | ðŸ”´              | ðŸŸ¢       | ðŸŸ¢        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 2/3/4/8      | ðŸŸ¢               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/AutoGPTQ/AutoGPTQ                    |\n",
    "| [HIGGS](./higgs)                           | ðŸŸ¢                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 2/4          | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/HanGuo97/flute                       |\n",
    "| [HQQ](./hqq)                               | ðŸŸ¢                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸŸ¢              | ðŸŸ¢              | 1/8          | ðŸŸ¢               | ðŸ”´                               | ðŸŸ¢                      | https://github.com/mobiusml/hqq/                        |\n",
    "| [optimum-quanto](./quanto)                 | ðŸŸ¢                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸŸ¢                                 | ðŸŸ¢              | ðŸŸ¢              | 2/4/8        | ðŸ”´               | ðŸ”´                               | ðŸŸ¢                      | https://github.com/huggingface/optimum-quanto           |\n",
    "| [FBGEMM_FP8](./fbgemm_fp8)                 | ðŸŸ¢                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸ”´              | 8            | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/pytorch/FBGEMM                       |\n",
    "| [torchao](./torchao)                       | ðŸŸ¢                      | ðŸŸ¢              | ðŸŸ¢       | ðŸ”´        | ðŸŸ¡                                 | ðŸŸ¢              |                 | 4/8          |                  | ðŸŸ¢ðŸ”´                             | ðŸŸ¢                      | https://github.com/pytorch/ao                           |\n",
    "| [VPTQ](./vptq)                             | ðŸ”´                      | ðŸ”´              | ðŸŸ¢       | ðŸŸ¡        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 1/8          | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/microsoft/VPTQ                       |\n",
    "| [FINEGRAINED_FP8](./finegrained_fp8)       | ðŸŸ¢                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸŸ¢              | ðŸ”´              | 8            | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      |                                                         |\n",
    "| [SpQR](./spqr)                             | ðŸ”´                      | ðŸ”´              | ðŸŸ¢       | ðŸ”´        | ðŸ”´                                 | ðŸ”´              | ðŸŸ¢              | 3            | ðŸ”´               | ðŸŸ¢                               | ðŸŸ¢                      | https://github.com/Vahe1994/SpQR/                       |\n",
    "| [Quark](./quark)                           | ðŸ”´                      | ðŸŸ¢              | ðŸŸ¢       | ðŸŸ¢        | ðŸŸ¢                                 | ðŸŸ¢              | ?               | 2/4/6/8/9/16 | ðŸ”´               | ðŸ”´                               | ðŸŸ¢                      | https://quark.docs.amd.com/latest/                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4c67e-e1c3-487b-91e6-5f6d3e636776",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "We have introduced\n",
    "1. Using `optimum-quanto` to linearly quantize llama3.2-1b to 8-bit\n",
    "2. Using `bitsandbyte` to linearly quantize llama3.2-1b to 8-bit and handle outlier\n",
    "3. Using `GPTQModel` to quantize llama3.2-1b to 4-bit with GPTQ method\n",
    "4. Saving quantized models and reloading them\n",
    "5. Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2966a01-70c6-4d6f-a363-fb0fe99d5f7c",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "## Reference\n",
    "\n",
    "- https://www.kaggle.com/code/aisuko/introduction-to-weight-quantization/notebook\n",
    "- https://www.kaggle.com/code/aisuko/quantization-methods\n",
    "- https://www.kaggle.com/code/aisuko/quantization-with-gptq\n",
    "- https://apxml.com/courses/practical-llm-quantization\n",
    "- https://github.com/huggingface/optimum-quanto\n",
    "- https://github.com/bitsandbytes-foundation/bitsandbytes\n",
    "- https://github.com/ModelCloud/GPTQModel\n",
    "- https://huggingface.co/docs/transformers/quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "gptq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

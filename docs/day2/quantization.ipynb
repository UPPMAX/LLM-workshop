{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec58f01-bc87-409e-aaa8-60b860528071",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dcada-5ebd-44c2-a1db-d0ab267b5882",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Outline\n",
    "\n",
    "- [Quantization Techniques](#quantization-techniques)\n",
    "- [Linear Quantization](#linear)\n",
    "- [GPTQ Quantization](#gptq)\n",
    "- [AWQ Quantization](#awq)\n",
    "- [Other Methods](#other-methods)\n",
    "- [Summary](#summary)\n",
    "- [Reference](#reference)\n",
    "\n",
    "***Note***: Some examples may take a lot of VRAM. You can restart the kernel once you hit OOM error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b25eca-0d1d-4973-b4d0-26a5bffaeb21",
   "metadata": {},
   "source": [
    "<a id=\"qunatization-techniques\"></a>\n",
    "## Quantization Techniques\n",
    "\n",
    "- Post trainging quantization (PTQ):\n",
    "    - Post training dynamic quantization: the range for each activation is computed on the fly at runtime.\n",
    "    - Post training static quantization: the range for each activation is computed in advance at quantization-time, typically by passing representative data through the model and recording the activation values.\n",
    "- Quantization aware training (QAT): the range for each activation is computed at training-time. They simulate the error induced by quantization to let the model be aware of quantization error\n",
    "\n",
    "Reference: https://huggingface.co/docs/optimum/concept_guides/quantization#calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a312-e461-4fc3-a53d-1dad7ca9891c",
   "metadata": {},
   "source": [
    "<a id=\"linear\"></a>\n",
    "## Linear quantization\n",
    "\n",
    "![](/LLM-workshop/day1/figures/quantization_symmetry.webp)\n",
    "![](figures/quantization_symmetry.webp)\n",
    "Image source: [Maarten Grootendorst](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)\n",
    "\n",
    "- $x = S * (x_q - Z)$\n",
    "- When $Z = 0$: symmetrics quantization\n",
    "- It can be applied per tensor or per channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e0466-771d-44bb-ae87-a31b14650ac7",
   "metadata": {},
   "source": [
    "### Affine quantization in Quanto (Int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f851e89-0054-4d72-8959-9dc91a2f9ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.quanto import QuantizedModelForCausalLM, qint8\n",
    "\n",
    "# https://www.geeksforgeeks.org/nlp/perplexity-for-llm-evaluation/\n",
    "def compute_perplexity_for_batch(model, tokenizer, input_texts):\n",
    "    inputs = tokenizer(\n",
    "        input_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    shift_logits = logits[:, :-1, :] \n",
    "    shift_labels = input_ids[:, 1:] \n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "    target_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    target_log_probs = target_log_probs * attention_mask[:, 1:].to(log_probs.dtype)\n",
    "    negative_log_likelihood = -target_log_probs.sum(dim=-1) / attention_mask[:, 1:].sum(dim=-1)\n",
    "    perplexities = torch.exp(negative_log_likelihood)\n",
    "    mean_perplexity_score = torch.mean(perplexities)\n",
    "\n",
    "    return {\n",
    "        \"perplexities\": perplexities.tolist(),\n",
    "        \"mean_perplexity\": mean_perplexity_score.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2abffa-9d14-4be9-892d-c6cec355620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.0179,  0.0066,  0.0247,  ..., -0.0087, -0.0117,  0.0201],\n",
      "        [ 0.0122,  0.0593,  0.0552,  ..., -0.0332, -0.0154,  0.0108],\n",
      "        [ 0.0178,  0.0155,  0.0344,  ..., -0.0386, -0.0386, -0.0276],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0352,  0.0713,  ..., -0.0718, -0.0265, -0.0287],\n",
      "        [ 0.0226, -0.0248,  0.0352,  ..., -0.0120, -0.0287, -0.0148],\n",
      "        [-0.0258, -0.0537, -0.0131,  ...,  0.0542,  0.0096, -0.0028]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(model)\n",
    "print(model.model.layers[0].self_attn.q_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333bc8e4-d097-44ab-a653-313acc14172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores for each text: [45.347049713134766, 16.073394775390625]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "example_texts = [\n",
    "    \"Once upon a time, there was a brave knight.\",\n",
    "    \"In a galaxy far, far away, a new adventure began.\"\n",
    "]\n",
    "\n",
    "# Compute perplexity scores for the batch of input texts\n",
    "results = compute_perplexity_for_batch(model, tokenizer, example_texts)\n",
    "print(f\"Perplexity scores for each text: {results['perplexities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838b7eaf-c5b1-4ca0-bb03-415fd77466a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "<class 'optimum.quanto.tensor.weights.qbytes.WeightQBytesTensor'>(tensor([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "        [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "        [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "        ...,\n",
      "        [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "        [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "        [-14, -30,  -7,  ...,  30,   5,  -2]], dtype=torch.int8), scale=tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        ...,\n",
      "        [0.0018],\n",
      "        [0.0007],\n",
      "        [0.0018]]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "qmodel = QuantizedModelForCausalLM.quantize(model, weights=qint8, exclude='lm_head')\n",
    "print(qmodel)\n",
    "print(qmodel.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "qmodel.save_pretrained('output/official/QLlama-3.2-1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a628c1ec-ec23-4bf1-8709-fc1ef6feb4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores for each text: [45.38690948486328, 16.23394012451172]\n"
     ]
    }
   ],
   "source": [
    "results = compute_perplexity_for_batch(qmodel, tokenizer, example_texts)\n",
    "print(f\"Perplexity scores for each text: {results['perplexities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae5e79-8c3e-497d-8cdd-9af98b8f1431",
   "metadata": {},
   "source": [
    "### Quanto integration in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6e3e47-6d83-4315-b143-015be5c0e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "<class 'optimum.quanto.tensor.weights.qbytes.WeightQBytesTensor'>(tensor([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "        [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "        [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "        ...,\n",
      "        [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "        [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "        [-14, -30,  -7,  ...,  30,   5,  -2]], dtype=torch.int8), scale=tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        ...,\n",
      "        [0.0018],\n",
      "        [0.0007],\n",
      "        [0.0018]]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, QuantoConfig\n",
    "\n",
    "quantization_config = QuantoConfig(weights=\"int8\", activations=None)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "print(model)\n",
    "print(model.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "# quanto quantized model cannot be serialized from transformers and cannot be saved\n",
    "# model.save_pretrained(\"output/transformers/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c3d49-4798-4097-ae5a-605d06a7f512",
   "metadata": {},
   "source": [
    "### Activation quantiztion / Calibration in Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23fc5694-3518-4956-9d8b-07f796666789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.quanto import quantize, freeze, qint8, Calibration, quantization_map\n",
    "from safetensors.torch import save_file\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\", use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    ").select(range(1024))\n",
    "\n",
    "quantize(model, weights=qint8, activations=qint8)\n",
    "with torch.no_grad(), Calibration(momentum=0.9):\n",
    "    model.eval()\n",
    "    for batch in calibration_dataset.iter(batch_size=2):\n",
    "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # good habit\n",
    "        del input_ids, attention_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4d22a0-c8db-454a-bba0-7a47f9de09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(\"output/calibration\", exist_ok=True)\n",
    "\n",
    "# Freeze integer weights\n",
    "freeze(model)\n",
    "\n",
    "# Serialize quantized model\n",
    "save_file(model.state_dict(), 'output/calibration/QLlama-3.2-1B/model.safetensors')\n",
    "# Store the quantized models quantization map\n",
    "with open('output/calibration/QLlama-3.2-1B/quantization_map.json', 'w') as f:\n",
    "    json.dump(quantization_map(model), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8e2380-1a79-4145-8d44-4af3ac571aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "from optimum.quanto import requantize\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "state_dict = load_file('output/calibration/QLlama-3.2-1B/model.safetensors')\n",
    "with open('output/calibration/QLlama-3.2-1B/quantization_map.json', 'r') as f:\n",
    "    quantization_map = json.load(f)\n",
    "\n",
    "# Create an empty model from your modeling code and requantize it\n",
    "config = AutoConfig.from_pretrained(\"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/config.json\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "requantize(model, state_dict, quantization_map, device=torch.device('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde5ba1a-b106-4bfb-b317-786cfc3890a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): QLinear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): QLinear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): QLinear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): QLinear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea932f5-674f-4486-b658-589e64af2797",
   "metadata": {},
   "source": [
    "### Outlier problem\n",
    "\n",
    "- Quanto simply uses `absmax()` to calculate scale\n",
    "- Outlier would compress most values to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26311fb7-5665-46cd-84ca-a11ad6e60152",
   "metadata": {},
   "source": [
    "### LLM.int8() in Bitsandbytes\n",
    "\n",
    "- Save outlier in another tensor to keep information\n",
    "- Model is quantized on the fly without loading model in full precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf04ff-a416-4e0f-bb9b-aa1d24b9f03c",
   "metadata": {},
   "source": [
    "![](/LLM-workshop/day2/figures/bitsandbytes_int8.png)\n",
    "![](figures/bitsandbytes_int8.png)\n",
    "Image source: [Dettmers+2022](https://arxiv.org/abs/2208.07339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ce780-67ad-4b87-a34f-5b5a64a98d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config, \n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d70edba-6a99-4ab2-bfb1-53b61ae04903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight\n",
      "Parameter containing:\n",
      "Parameter(Int8Params([[-44,  16,  61,  ..., -22, -29,  50],\n",
      "            [ 14,  70,  65,  ..., -39, -18,  13],\n",
      "            [ 16,  14,  30,  ..., -34, -34, -24],\n",
      "            ...,\n",
      "            [ 17,  20,  40,  ..., -40, -15, -16],\n",
      "            [ 32, -35,  50,  ..., -17, -41, -21],\n",
      "            [-14, -30,  -7,  ...,  30,   5,  -2]], device='cuda:0',\n",
      "           dtype=torch.int8))\n",
      "tensor([0.0515, 0.1079, 0.1436,  ..., 0.2256, 0.0898, 0.2305], device='cuda:0')\n",
      "1.4985504\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if hasattr(param, \"SCB\"):\n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(param.SCB)\n",
    "        break\n",
    "print(model.get_memory_footprint() / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a416ed4-eb2a-4a57-bcf0-90fcc3f0a9cc",
   "metadata": {},
   "source": [
    "<a id=\"gptq\"></a>\n",
    "## GPTQ (Generative Pre-trained Transformer Quantizer) Quantization\n",
    "\n",
    "- Process weights sequentially\n",
    "- Compensate error induced by currect step by updating the not-yet-quantized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e3688-eee6-43ad-a1cb-66d09651a5af",
   "metadata": {},
   "source": [
    "### GPTQ in [GPTQModel](https://github.com/ModelCloud/GPTQModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97ea08d-0342-4b48-a8d2-82351766f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128004 (token='<|finetune_right_pad_id|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_preexperiment_time_11_19_2025_18h_56m_52s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[96m0.30117315\u001b[0m | 1024        | 0.01000     | 1.217     | 3.466        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00808881\u001b[0m | 1024        | 0.01000     | 0.450     | 3.466        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[96m0.61798751\u001b[0m | 1024        | 0.01000     | 0.454     | 3.466        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00077137\u001b[0m | 1024        | 0.01000     | 0.456     | 2.546        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[96m0.51245892\u001b[0m | 1024        | 0.01000     | 0.463     | 2.947        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[96m0.64624822\u001b[0m | 1024        | 0.01000     | 0.460     | 2.947        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.00407354\u001b[0m | 1024        | 0.01000     | 2.052     | 7.199        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[96m0.49343121\u001b[0m | 1024        | 0.01000     | 0.454     | 3.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.02866399\u001b[0m | 1024        | 0.01000     | 0.453     | 3.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[96m0.99162263\u001b[0m | 1024        | 0.01000     | 0.454     | 3.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00303621\u001b[0m | 1024        | 0.01000     | 0.456     | 2.275        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[96m0.76688707\u001b[0m | 1024        | 0.01000     | 0.467     | 2.669        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[93m1.05201042\u001b[0m | 1024        | 0.01000     | 0.465     | 2.669        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[93m1.66648114\u001b[0m | 1024        | 0.01000     | 1.946     | 6.984        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[93m1.03801095\u001b[0m | 1024        | 0.01000     | 0.453     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.06991096\u001b[0m | 1024        | 0.01000     | 0.453     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[93m1.99752581\u001b[0m | 1024        | 0.01000     | 0.455     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.00342211\u001b[0m | 1024        | 0.01000     | 0.453     | 2.275        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[93m1.00037694\u001b[0m | 1024        | 0.01000     | 0.463     | 2.677        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[93m1.58194685\u001b[0m | 1024        | 0.01000     | 0.463     | 2.677        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.01264102\u001b[0m | 1024        | 0.01000     | 1.944     | 7.000        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[96m0.66864121\u001b[0m | 1024        | 0.01000     | 0.457     | 3.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.09188035\u001b[0m | 1024        | 0.01000     | 0.455     | 3.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[93m1.47139096\u001b[0m | 1024        | 0.01000     | 0.454     | 3.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.00652087\u001b[0m | 1024        | 0.01000     | 0.455     | 2.278        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[93m1.23837781\u001b[0m | 1024        | 0.01000     | 0.467     | 2.677        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[93m2.43490934\u001b[0m | 1024        | 0.01000     | 0.466     | 2.677        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.01894283\u001b[0m | 1024        | 0.01000     | 1.953     | 6.983        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[96m0.72414231\u001b[0m | 1024        | 0.01000     | 0.458     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.08650243\u001b[0m | 1024        | 0.01000     | 0.457     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[93m1.49932063\u001b[0m | 1024        | 0.01000     | 0.455     | 3.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.00972512\u001b[0m | 1024        | 0.01000     | 0.456     | 2.284        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[93m1.27032411\u001b[0m | 1024        | 0.01000     | 0.467     | 2.678        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[93m2.69963837\u001b[0m | 1024        | 0.01000     | 0.467     | 2.678        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[96m0.84886682\u001b[0m | 1024        | 0.01000     | 0.458     | 3.030        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[96m0.13304198\u001b[0m | 1024        | 0.01000     | 0.454     | 3.030        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[93m2.07612371\u001b[0m | 1024        | 0.01000     | 0.456     | 3.030        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.02503609\u001b[0m | 1024        | 0.01000     | 0.458     | 2.294        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[93m1.74318504\u001b[0m | 1024        | 0.01000     | 0.464     | 2.693        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[93m2.82753754\u001b[0m | 1024        | 0.01000     | 0.461     | 2.693        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.04964223\u001b[0m | 1024        | 0.01000     | 1.957     | 7.048        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[93m1.04286778\u001b[0m | 1024        | 0.01000     | 0.456     | 3.024        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[96m0.16099945\u001b[0m | 1024        | 0.01000     | 0.453     | 3.024        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[93m2.18314767\u001b[0m | 1024        | 0.01000     | 0.453     | 3.024        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.01785076\u001b[0m | 1024        | 0.01000     | 0.453     | 2.281        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[93m2.06139469\u001b[0m | 1024        | 0.01000     | 0.466     | 2.684        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[93m3.23025703\u001b[0m | 1024        | 0.01000     | 0.463     | 2.684        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.06254576\u001b[0m | 1024        | 0.01000     | 1.944     | 7.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[93m1.26511097\u001b[0m | 1024        | 0.01000     | 0.455     | 3.022        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[96m0.15488198\u001b[0m | 1024        | 0.01000     | 0.455     | 3.022        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[93m2.15340161\u001b[0m | 1024        | 0.01000     | 0.455     | 3.022        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.01433891\u001b[0m | 1024        | 0.01000     | 0.456     | 2.291        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[93m2.25294113\u001b[0m | 1024        | 0.01000     | 0.470     | 2.694        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[93m3.46270084\u001b[0m | 1024        | 0.01000     | 0.468     | 2.694        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.06779625\u001b[0m | 1024        | 0.01000     | 1.962     | 7.022        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[93m1.29417920\u001b[0m | 1024        | 0.01000     | 0.455     | 3.021        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[96m0.15706307\u001b[0m | 1024        | 0.01000     | 0.454     | 3.021        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[93m2.15997839\u001b[0m | 1024        | 0.01000     | 0.454     | 3.021        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.01520295\u001b[0m | 1024        | 0.01000     | 0.456     | 2.280        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[93m2.36928177\u001b[0m | 1024        | 0.01000     | 0.467     | 2.683        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[93m3.45540619\u001b[0m | 1024        | 0.01000     | 0.465     | 2.683        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.07528380\u001b[0m | 1024        | 0.01000     | 1.945     | 7.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[93m1.27076578\u001b[0m | 1024        | 0.01000     | 0.460     | 3.020        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[96m0.26116973\u001b[0m | 1024        | 0.01000     | 0.456     | 3.020        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[93m2.49135661\u001b[0m | 1024        | 0.01000     | 0.456     | 3.020        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.02025730\u001b[0m | 1024        | 0.01000     | 0.459     | 2.286        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[93m2.80010867\u001b[0m | 1024        | 0.01000     | 0.469     | 2.689        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[93m3.75511003\u001b[0m | 1024        | 0.01000     | 0.468     | 2.689        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[96m0.11440941\u001b[0m | 1024        | 0.01000     | 1.954     | 7.022        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[93m1.46573043\u001b[0m | 1024        | 0.01000     | 0.460     | 3.029        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[96m0.58527297\u001b[0m | 1024        | 0.01000     | 0.455     | 3.029        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[93m2.69790554\u001b[0m | 1024        | 0.01000     | 0.455     | 3.029        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.05254138\u001b[0m | 1024        | 0.01000     | 0.459     | 2.285        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[93m3.32393456\u001b[0m | 1024        | 0.01000     | 0.468     | 2.686        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[93m4.81758022\u001b[0m | 1024        | 0.01000     | 0.467     | 2.686        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[96m0.16269645\u001b[0m | 1024        | 0.01000     | 1.962     | 7.034        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[93m1.39538884\u001b[0m | 1024        | 0.01000     | 0.458     | 3.040        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[96m0.59370136\u001b[0m | 1024        | 0.01000     | 0.454     | 3.040        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[93m2.50607896\u001b[0m | 1024        | 0.01000     | 0.454     | 3.040        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[96m0.18327969\u001b[0m | 1024        | 0.01000     | 0.453     | 2.302        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[93m4.28998661\u001b[0m | 1024        | 0.01000     | 0.468     | 2.706        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[33m5.70833874\u001b[0m | 1024        | 0.01000     | 0.467     | 2.706        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[96m0.43429565\u001b[0m | 1024        | 0.01000     | 1.955     | 7.111        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.30117315', 'samples': '1024', 'damp': '0.01000', 'time': '1.217', 'fwd_time': '3.466'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00808881', 'samples': '1024', 'damp': '0.01000', 'time': '0.450', 'fwd_time': '3.466'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.61798751', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.466'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00077137', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '2.546'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.51245892', 'samples': '1024', 'damp': '0.01000', 'time': '0.463', 'fwd_time': '2.947'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.64624822', 'samples': '1024', 'damp': '0.01000', 'time': '0.460', 'fwd_time': '2.947'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00407354', 'samples': '1024', 'damp': '0.01000', 'time': '2.052', 'fwd_time': '7.199'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.49343121', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.02866399', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '3.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.99162263', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00303621', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '2.275'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.76688707', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.669'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '1.05201042', 'samples': '1024', 'damp': '0.01000', 'time': '0.465', 'fwd_time': '2.669'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '1.66648114', 'samples': '1024', 'damp': '0.01000', 'time': '1.946', 'fwd_time': '6.984'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '1.03801095', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.06991096', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '1.99752581', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00342211', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '2.275'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '1.00037694', 'samples': '1024', 'damp': '0.01000', 'time': '0.463', 'fwd_time': '2.677'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '1.58194685', 'samples': '1024', 'damp': '0.01000', 'time': '0.463', 'fwd_time': '2.677'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.01264102', 'samples': '1024', 'damp': '0.01000', 'time': '1.944', 'fwd_time': '7.000'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.66864121', 'samples': '1024', 'damp': '0.01000', 'time': '0.457', 'fwd_time': '3.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.09188035', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '1.47139096', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00652087', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '2.278'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '1.23837781', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.677'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '2.43490934', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.677'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.01894283', 'samples': '1024', 'damp': '0.01000', 'time': '1.953', 'fwd_time': '6.983'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.72414231', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.08650243', 'samples': '1024', 'damp': '0.01000', 'time': '0.457', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '1.49932063', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00972512', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '2.284'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '1.27032411', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.678'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '2.69963837', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.678'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.02200124', 'samples': '1024', 'damp': '0.01000', 'time': '1.959', 'fwd_time': '6.990'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '1.15480590', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.07708703', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '1.99378765', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.01000706', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '2.277'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '1.38294625', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.673'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '2.51848507', 'samples': '1024', 'damp': '0.01000', 'time': '0.464', 'fwd_time': '2.673'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.02658287', 'samples': '1024', 'damp': '0.01000', 'time': '1.957', 'fwd_time': '6.980'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.88846290', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '3.018'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.09963284', 'samples': '1024', 'damp': '0.01000', 'time': '0.457', 'fwd_time': '3.018'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '1.42152846', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.018'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.01500860', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '2.285'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '1.40616870', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.686'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '2.51148319', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.686'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.02717795', 'samples': '1024', 'damp': '0.01000', 'time': '1.951', 'fwd_time': '7.019'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.88643467', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '3.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.11398130', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '1.67166734', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.01493945', 'samples': '1024', 'damp': '0.01000', 'time': '0.457', 'fwd_time': '2.292'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '1.47412217', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.695'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '2.38286161', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.695'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.03029270', 'samples': '1024', 'damp': '0.01000', 'time': '1.961', 'fwd_time': '7.045'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '1.08758116', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.028'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.10918085', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.028'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '1.79880595', 'samples': '1024', 'damp': '0.01000', 'time': '0.457', 'fwd_time': '3.028'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.01832559', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '2.288'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '1.64151132', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.690'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '2.58264875', 'samples': '1024', 'damp': '0.01000', 'time': '0.463', 'fwd_time': '2.690'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.04075113', 'samples': '1024', 'damp': '0.01000', 'time': '1.955', 'fwd_time': '7.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.84886682', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '3.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.13304198', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '2.07612371', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '3.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.02503609', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '2.294'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '1.74318504', 'samples': '1024', 'damp': '0.01000', 'time': '0.464', 'fwd_time': '2.693'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '2.82753754', 'samples': '1024', 'damp': '0.01000', 'time': '0.461', 'fwd_time': '2.693'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.04964223', 'samples': '1024', 'damp': '0.01000', 'time': '1.957', 'fwd_time': '7.048'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '1.04286778', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '3.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.16099945', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '3.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '2.18314767', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '3.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.01785076', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '2.281'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '2.06139469', 'samples': '1024', 'damp': '0.01000', 'time': '0.466', 'fwd_time': '2.684'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '3.23025703', 'samples': '1024', 'damp': '0.01000', 'time': '0.463', 'fwd_time': '2.684'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.06254576', 'samples': '1024', 'damp': '0.01000', 'time': '1.944', 'fwd_time': '7.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '1.26511097', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.022'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.15488198', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.022'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '2.15340161', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.022'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.01433891', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '2.291'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '2.25294113', 'samples': '1024', 'damp': '0.01000', 'time': '0.470', 'fwd_time': '2.694'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '3.46270084', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.694'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.06779625', 'samples': '1024', 'damp': '0.01000', 'time': '1.962', 'fwd_time': '7.022'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '1.29417920', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.021'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.15706307', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.021'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '2.15997839', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.021'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.01520295', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '2.280'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '2.36928177', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.683'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '3.45540619', 'samples': '1024', 'damp': '0.01000', 'time': '0.465', 'fwd_time': '2.683'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.07528380', 'samples': '1024', 'damp': '0.01000', 'time': '1.945', 'fwd_time': '7.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '1.27076578', 'samples': '1024', 'damp': '0.01000', 'time': '0.460', 'fwd_time': '3.020'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.26116973', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '3.020'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '2.49135661', 'samples': '1024', 'damp': '0.01000', 'time': '0.456', 'fwd_time': '3.020'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.02025730', 'samples': '1024', 'damp': '0.01000', 'time': '0.459', 'fwd_time': '2.286'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '2.80010867', 'samples': '1024', 'damp': '0.01000', 'time': '0.469', 'fwd_time': '2.689'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '3.75511003', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.689'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.11440941', 'samples': '1024', 'damp': '0.01000', 'time': '1.954', 'fwd_time': '7.022'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '1.46573043', 'samples': '1024', 'damp': '0.01000', 'time': '0.460', 'fwd_time': '3.029'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.58527297', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.029'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '2.69790554', 'samples': '1024', 'damp': '0.01000', 'time': '0.455', 'fwd_time': '3.029'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.05254138', 'samples': '1024', 'damp': '0.01000', 'time': '0.459', 'fwd_time': '2.285'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '3.32393456', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.686'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '4.81758022', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.686'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.16269645', 'samples': '1024', 'damp': '0.01000', 'time': '1.962', 'fwd_time': '7.034'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '1.39538884', 'samples': '1024', 'damp': '0.01000', 'time': '0.458', 'fwd_time': '3.040'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.59370136', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.040'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '2.50607896', 'samples': '1024', 'damp': '0.01000', 'time': '0.454', 'fwd_time': '3.040'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.18327969', 'samples': '1024', 'damp': '0.01000', 'time': '0.453', 'fwd_time': '2.302'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '4.28998661', 'samples': '1024', 'damp': '0.01000', 'time': '0.468', 'fwd_time': '2.706'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '5.70833874', 'samples': '1024', 'damp': '0.01000', 'time': '0.467', 'fwd_time': '2.706'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.43429565', 'samples': '1024', 'damp': '0.01000', 'time': '1.955', 'fwd_time': '7.111'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TritonV2QuantLinear, TorchQuantLinear]`          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TritonV2QuantLinear`.                               \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                         \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 4,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "quant_log.csv\n",
      "special_tokens_map.json\n",
      "generation_config.json\n",
      "quanto_qmap.json\n",
      "quantize_config.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "README.md\n",
      "model.safetensors\n",
      "chat_template.jinja\n",
      "config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"do_sample\": true,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"temperature\": 0.6,\n",
      "    \"top_p\": 0.9,\n",
      "    \"transformers_version\": \"4.52.4\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"max_position_embeddings\": 131072,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 16,\n",
      "    \"num_key_value_heads\": 8,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 4,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": {\n",
      "        \"factor\": 32.0,\n",
      "        \"high_freq_factor\": 4.0,\n",
      "        \"low_freq_factor\": 1.0,\n",
      "        \"original_max_position_embeddings\": 8192,\n",
      "        \"rope_type\": \"llama3\"\n",
      "    },\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.52.4\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 128256\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2357.14MB, 2.30GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 984.55MB, 0.96GB                                   \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1372.59MB, 1.34GB - 58.23%                              \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    ").select(range(1024))[\"text\"]\n",
    "\n",
    "quant_config = QuantizeConfig(bits=4)\n",
    "\n",
    "model = GPTQModel.load(model_name, quant_config)\n",
    "\n",
    "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
    "model.quantize(calibration_dataset, batch_size=2);\n",
    "\n",
    "model.save(\"output/official/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c64ad-4d99-4d48-a9b2-fe513546df80",
   "metadata": {},
   "source": [
    "### GPTQ integration in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d70089-f733-4d84-9ca4-762ece64a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292f9dbd192046348f7bac031494fa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 1/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26201aebd5a9436aacc63beab5e327cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 1/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 2/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55b37c5910344a288e5d3c8842db5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 2/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 3/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3373d6164905471ca0bfb728eee38283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 3/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 4/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43ed7983a6c413e83820c7dfbaf70d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 4/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 5/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c371194b4f4f79a43cc08206790968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 5/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 6/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a8e3508d244311a81f1991b388b128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 6/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 7/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65602c7076b54ab5bbe0d20814763dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 7/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 8/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dd2196b39a44cf9c8d9a146004b9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 8/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 9/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d296c5cd748de93bca8f14ccd4de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 9/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 10/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b169f0fb1df4f7eaea8db40da5f5a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 10/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 11/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201f3d0a84cd44ee96ea937cb587334a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 11/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 12/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546232346ea64b9bb445d82a082f2902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 12/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 13/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dbb157938140018e5116e5325d7e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 13/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 14/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d178b1886f8842b09674fd17e6822813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 14/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 15/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54883425335f44baaf3c40420bea1356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 15/16...\n",
      "INFO:optimum.gptq.quantizer:Start quantizing block model.layers 16/16\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d867f43ea1f14c4e840b403f1e7ce0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.o_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.gate_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.up_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Quantizing mlp.down_proj in block 16/16...\n",
      "INFO:optimum.gptq.quantizer:Packing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.0.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.1.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.2.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.3.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.4.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.5.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.6.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.7.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.8.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.9.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.10.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.11.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.12.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.13.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.14.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.o_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.down_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.gate_proj\n",
      "INFO:optimum.gptq.quantizer:model.layers.15.mlp.up_proj\n",
      "INFO:optimum.gptq.quantizer:Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (k_proj): TritonV2QuantLinear()\n",
      "          (o_proj): TritonV2QuantLinear()\n",
      "          (q_proj): TritonV2QuantLinear()\n",
      "          (v_proj): TritonV2QuantLinear()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (act_fn): SiLU()\n",
      "          (down_proj): TritonV2QuantLinear()\n",
      "          (gate_proj): TritonV2QuantLinear()\n",
      "          (up_proj): TritonV2QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "1032327296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output/transformers/QLlama-3.2-1B/tokenizer_config.json',\n",
       " 'output/transformers/QLlama-3.2-1B/special_tokens_map.json',\n",
       " 'output/transformers/QLlama-3.2-1B/chat_template.jinja',\n",
       " 'output/transformers/QLlama-3.2-1B/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "gptq_config=GPTQConfig(\n",
    "    bits=4,\n",
    "    dataset=\"c4\", # optimum will download 'en/c4-train.00000-of-01024.json.gz'\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "quantized_model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=gptq_config\n",
    ")\n",
    "print(quantized_model)\n",
    "print(quantized_model.get_memory_footprint())\n",
    "\n",
    "quantized_model.save_pretrained(\"output/transformers/QLlama-3.2-1B\")\n",
    "tokenizer.save_pretrained(\"output/transformers/QLlama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c4091-5a0b-447f-a762-2f13846bddba",
   "metadata": {},
   "source": [
    "<a id=\"awq\"></a>\n",
    "## AWQ: Activation-aware Weight Quantization\n",
    "\n",
    "- Hypothesize actications determine the importance of each weights\n",
    "- Use calibration data to identify salient channel\n",
    "- Calculate per-channel scaling factors to reduce quantization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315177d-d8ce-4bad-8e93-7069ace4086b",
   "metadata": {},
   "source": [
    "### AWQ in [llmcompressor](https://github.com/vllm-project/llm-compressor)\n",
    "\n",
    "- [autoawq](https://github.com/casper-hansen/AutoAWQ) has been archived and llmcompressor developed by vLLM take over the function\n",
    "- llmcompressor support a few quantization methods:\n",
    "    - Simple PTQ\n",
    "    - GPTQ\n",
    "    - AWQ\n",
    "    - SmoothQuant\n",
    "    - SparseGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ab21-817f-48d3-af58-af06c6ab8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0959dc010ac489b887444264cb2682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-20T04:34:49.610464+0100 | reset | INFO - Compression lifecycle reset\n",
      "2025-11-20T04:34:49.613895+0100 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-11-20T04:34:49.693089+0100 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving mapping 1/4 (0 skipped): 100%|| 16/16 [00:00<00:00, 1068.75it/s]\n",
      "Resolving mapping 2/4 (15 skipped): 100%|| 16/16 [00:00<00:00, 2230.49it/s]\n",
      "Resolving mapping 3/4 (0 skipped): 100%|| 16/16 [00:00<00:00, 1327.02it/s]\n",
      "Resolving mapping 4/4 (0 skipped): 100%|| 16/16 [00:00<00:00, 2238.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-20T04:34:49.757582+0100 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-11-20T04:34:49.758165+0100 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing cache: 100%|| 256/256 [00:00<00:00, 1555.82it/s]\n",
      "(1/17): Calibrating: 100%|| 256/256 [00:02<00:00, 119.49it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(1/17): Propagating: 100%|| 256/256 [00:00<00:00, 351.90it/s]\n",
      "(2/17): Calibrating: 100%|| 256/256 [00:01<00:00, 136.26it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.86s/it]\n",
      "(2/17): Propagating: 100%|| 256/256 [00:00<00:00, 466.73it/s]\n",
      "(3/17): Calibrating: 100%|| 256/256 [00:01<00:00, 168.07it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.85s/it]\n",
      "(3/17): Propagating: 100%|| 256/256 [00:00<00:00, 484.16it/s]\n",
      "(4/17): Calibrating: 100%|| 256/256 [00:01<00:00, 215.13it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.86s/it]\n",
      "(4/17): Propagating: 100%|| 256/256 [00:00<00:00, 407.05it/s]\n",
      "(5/17): Calibrating: 100%|| 256/256 [00:01<00:00, 196.82it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(5/17): Propagating: 100%|| 256/256 [00:00<00:00, 432.69it/s]\n",
      "(6/17): Calibrating: 100%|| 256/256 [00:01<00:00, 169.70it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(6/17): Propagating: 100%|| 256/256 [00:00<00:00, 479.21it/s]\n",
      "(7/17): Calibrating: 100%|| 256/256 [00:01<00:00, 214.30it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(7/17): Propagating: 100%|| 256/256 [00:00<00:00, 438.11it/s]\n",
      "(8/17): Calibrating: 100%|| 256/256 [00:01<00:00, 170.15it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(8/17): Propagating: 100%|| 256/256 [00:00<00:00, 445.58it/s]\n",
      "(9/17): Calibrating: 100%|| 256/256 [00:01<00:00, 206.62it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.87s/it]\n",
      "(9/17): Propagating: 100%|| 256/256 [00:00<00:00, 425.35it/s]\n",
      "(10/17): Calibrating: 100%|| 256/256 [00:01<00:00, 180.31it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.88s/it]\n",
      "(10/17): Propagating: 100%|| 256/256 [00:00<00:00, 477.98it/s]\n",
      "(11/17): Calibrating: 100%|| 256/256 [00:01<00:00, 181.03it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.88s/it]\n",
      "(11/17): Propagating: 100%|| 256/256 [00:00<00:00, 424.24it/s]\n",
      "(12/17): Calibrating: 100%|| 256/256 [00:01<00:00, 181.42it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.89s/it]\n",
      "(12/17): Propagating: 100%|| 256/256 [00:00<00:00, 437.31it/s]\n",
      "(13/17): Calibrating: 100%|| 256/256 [00:01<00:00, 199.29it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.88s/it]\n",
      "(13/17): Propagating: 100%|| 256/256 [00:00<00:00, 434.82it/s]\n",
      "(14/17): Calibrating: 100%|| 256/256 [00:01<00:00, 202.33it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.89s/it]\n",
      "(14/17): Propagating: 100%|| 256/256 [00:00<00:00, 463.45it/s]\n",
      "(15/17): Calibrating: 100%|| 256/256 [00:01<00:00, 193.22it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.90s/it]\n",
      "(15/17): Propagating: 100%|| 256/256 [00:00<00:00, 451.70it/s]\n",
      "(16/17): Calibrating: 100%|| 256/256 [00:01<00:00, 176.80it/s]\n",
      "Smoothing: 100%|| 3/3 [00:08<00:00,  2.88s/it]\n",
      "(16/17): Propagating: 100%|| 256/256 [00:00<00:00, 400.77it/s]\n",
      "(17/17): Calibrating: 100%|| 256/256 [00:00<00:00, 283.77it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "(17/17): Propagating: 100%|| 256/256 [00:00<00:00, 377.33it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "Calibrating weights: 100%|| 327/327 [00:01<00:00, 255.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-20T04:37:45.957744+0100 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-20T04:37:46.445192+0100 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 215it [00:04, 47.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "\n",
    "model_name = \"/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/d2b9e36fa60db5ac69ce42498103dcbcfa836229/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Select number of samples. 256 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    ").select(range(1024))\n",
    "\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    AWQModifier(ignore=[\"lm_head\"], scheme=\"W4A16_ASYM\", targets=[\"Linear\"]),\n",
    "]\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_dataset,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    output_dir=\"output/llmcompressor/QLlama-3.2-1B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7b61e-9963-4666-a02b-80e049b6aa48",
   "metadata": {},
   "source": [
    "<a id=\"other-methods\"></a>\n",
    "## Other quantization methods supported in Transformers\n",
    "\n",
    "- https://huggingface.co/docs/transformers/quantization/overview\n",
    "\n",
    "| Quantization Method                        | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon) | Intel GPU | Torch compile() | Bits         | PEFT Fine Tuning | Serializable with Transformers | Transformers Support  | Link to library                                         |\n",
    "|--------------------------------------------|-------------------------|-----------------|----------|-----------|-----------------------|-----------|-----------------|--------------|------------------|--------------------------------------|-----------------------------|---------------------------------------------------------|\n",
    "| [AQLM](./aqlm)                             |                   |           |    |     |                 |     |           | 1/2          |            |                                |                       | https://github.com/Vahe1994/AQLM                        |\n",
    "| [AutoRound](./auto_round)                  |                   |           |    |     |                 |     |           | 2/3/4/8      |            |                                |                       | https://github.com/intel/auto-round                     |\n",
    "| [AWQ](./awq)                               |                   |           |    |     |                 |     | ?               | 4            |            |                                |                       | https://github.com/casper-hansen/AutoAWQ                |\n",
    "| [bitsandbytes](./bitsandbytes)             |                   |           |    |     |                 |     |           | 4/8          |            |                                |                       | https://github.com/bitsandbytes-foundation/bitsandbytes |\n",
    "| [compressed-tensors](./compressed_tensors) |                   |           |    |     |                 |     |           | 1/8          |            |                                |                       | https://github.com/neuralmagic/compressed-tensors       |\n",
    "| [EETQ](./eetq)                             |                   |           |    |     |                 |     | ?               | 8            |            |                                |                       | https://github.com/NetEase-FuXi/EETQ                    |\n",
    "| [FP-Quant](./fp_quant)                     |                   |           |    |     |                 |     |           | 4            |            |                                |                       | https://github.com/IST-DASLab/FP-Quant                  |\n",
    "| [GGUF / GGML (llama.cpp)](../gguf)         |                   |           |    |     |                 |     |           | 1/8          |            | [See Notes](../gguf)                 | [See Notes](../gguf)        | https://github.com/ggerganov/llama.cpp                  |\n",
    "| [GPTQModel](./gptq)                        |                   |           |    |     |                 |     |           | 2/3/4/8      |            |                                |                       | https://github.com/ModelCloud/GPTQModel                 |\n",
    "| [AutoGPTQ](./gptq)                         |                   |           |    |     |                 |     |           | 2/3/4/8      |            |                                |                       | https://github.com/AutoGPTQ/AutoGPTQ                    |\n",
    "| [HIGGS](./higgs)                           |                   |           |    |     |                 |     |           | 2/4          |            |                                |                       | https://github.com/HanGuo97/flute                       |\n",
    "| [HQQ](./hqq)                               |                   |           |    |     |                 |     |           | 1/8          |            |                                |                       | https://github.com/mobiusml/hqq/                        |\n",
    "| [optimum-quanto](./quanto)                 |                   |           |    |     |                 |     |           | 2/4/8        |            |                                |                       | https://github.com/huggingface/optimum-quanto           |\n",
    "| [FBGEMM_FP8](./fbgemm_fp8)                 |                   |           |    |     |                 |     |           | 8            |            |                                |                       | https://github.com/pytorch/FBGEMM                       |\n",
    "| [torchao](./torchao)                       |                   |           |    |     |                 |     |                 | 4/8          |                  |                          |                       | https://github.com/pytorch/ao                           |\n",
    "| [VPTQ](./vptq)                             |                   |           |    |     |                 |     |           | 1/8          |            |                                |                       | https://github.com/microsoft/VPTQ                       |\n",
    "| [FINEGRAINED_FP8](./finegrained_fp8)       |                   |           |    |     |                 |     |           | 8            |            |                                |                       |                                                         |\n",
    "| [SpQR](./spqr)                             |                   |           |    |     |                 |     |           | 3            |            |                                |                       | https://github.com/Vahe1994/SpQR/                       |\n",
    "| [Quark](./quark)                           |                   |           |    |     |                 |     | ?               | 2/4/6/8/9/16 |            |                                |                       | https://quark.docs.amd.com/latest/                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4c67e-e1c3-487b-91e6-5f6d3e636776",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "We have introduced\n",
    "1. Using `optimum-quanto` to linearly quantize llama3.2-1b in 8-bit\n",
    "2. Using `bitsandbyte` to linearly quantize llama3.2-1b in 8-bit and handle outlier\n",
    "3. Using `GPTQModel` to quantize llama3.2-1b with GPTQ method\n",
    "4. Using `llmcompressor` to quantize llama3.2-1b with AWQ method\n",
    "5. Saving quantized models and reloading them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2966a01-70c6-4d6f-a363-fb0fe99d5f7c",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "## Reference\n",
    "\n",
    "- https://www.kaggle.com/code/aisuko/introduction-to-weight-quantization/notebook\n",
    "- https://www.kaggle.com/code/aisuko/quantization-methods\n",
    "- https://www.kaggle.com/code/aisuko/quantization-with-gptq\n",
    "- https://apxml.com/courses/practical-llm-quantization\n",
    "- https://github.com/huggingface/optimum-quanto\n",
    "- https://github.com/bitsandbytes-foundation/bitsandbytes\n",
    "- https://github.com/ModelCloud/GPTQModel\n",
    "- https://huggingface.co/docs/transformers/quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

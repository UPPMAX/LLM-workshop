Pre and post course survey quetions per schedule item:
* Prompt engineering
    * I understand the differences in context and prompt engineering
    * I can optimize my prompt for the LLM I am using
* Tools, chain/tree-of-thought
    * I know how LLMs make use of tools
    * I know how LLMs can be forced to "think"
* Finetuning, RL
    * I know different finetuning techniques for LLMs
    * I can finetune an LLM on a dataset
* HyperParameter tuning
    * I know what various hyperparameters mean
    * I know how to optimize my hyperparameter selection
* Evaluation metrics
    * I broadly know which evaluation metrics to evaluate on for my task
    * I can evaluate my finetuned LLM
* Is there anything you would like to see less or more in the future LLM workshop? Or do you have any general feedback?: Textbox

Pre course survey questions only:
* I can navigate inside NAISS project dirs
* I can launch interactive sessions
* I can submit batch scripts
* I can build and use containers
* I can work (create/activate/modify) with a python virtual env
* I can code in Python: beginner, intermediate, proficient
* I have used ML/DL frameworks and libraries (TF/PyTorch/JAX/HF etc.): Yes, No, Not sure

Post course survey questions only: 
* I would recommend this workshop to others : Yes, No, Not sure

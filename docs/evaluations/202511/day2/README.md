# Day 2 Survey Analysis

## Overview

This analysis compares pre-course and post-course survey responses for Day 2 of the LLM workshop (November 2025).

## Confidence Rating Improvements

The following table shows the average confidence ratings (scale 1-5) for each learning outcome, comparing pre-course expectations with post-course assessments:

| Learning Outcome | Pre-Course Avg | Post-Course Avg | Improvement |
|------------------|----------------|-----------------|-------------|
| I know the common stages of a simple RAG | 2.37 | 4.73 | +2.36 |
| I know some popular libraries for quantizing models | 2.05 | 3.64 | +1.58 |
| I know some popular libraries for parallelizing LLMs | 1.95 | 3.45 | +1.51 |
| I know how RAG is used by AI Agents | 2.32 | 3.73 | +1.41 |
| I can use inference engine/server to infer from multimodal LLMs | 2.53 | 3.91 | +1.38 |
| I know how to choose a scheme for a certain LLM task | 1.89 | 3.27 | +1.38 |
| I know different ways to parallelize LLM training | 2.21 | 3.55 | +1.33 |
| I know when to use RAG for my usecase | 2.63 | 3.91 | +1.28 |
| I know the common stages of a pre-training pipeline | 2.79 | 3.91 | +1.12 |
| I know the common stages of a post-training pipeline | 2.74 | 3.82 | +1.08 |
| I know different ways to perform quantization | 2.42 | 3.36 | +0.94 |

## Distribution Shifts

Detailed distribution of confidence ratings before and after the course:

### I know the common stages of a simple RAG

**Pre-course distribution** (n=19):
- Rating 1:  8 ( 42.1%) ████████
- Rating 2:  3 ( 15.8%) ███
- Rating 3:  2 ( 10.5%) ██
- Rating 4:  5 ( 26.3%) █████
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  0 (  0.0%) 
- Rating 3:  0 (  0.0%) 
- Rating 4:  3 ( 27.3%) █████
- Rating 5:  8 ( 72.7%) ██████████████

### I know some popular libraries for quantizing models

**Pre-course distribution** (n=19):
- Rating 1:  9 ( 47.4%) █████████
- Rating 2:  4 ( 21.1%) ████
- Rating 3:  2 ( 10.5%) ██
- Rating 4:  4 ( 21.1%) ████
- Rating 5:  0 (  0.0%) 

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  3 ( 27.3%) █████
- Rating 3:  1 (  9.1%) █
- Rating 4:  4 ( 36.4%) ███████
- Rating 5:  3 ( 27.3%) █████

### I know some popular libraries for parallelizing LLMs

**Pre-course distribution** (n=19):
- Rating 1: 11 ( 57.9%) ███████████
- Rating 2:  2 ( 10.5%) ██
- Rating 3:  2 ( 10.5%) ██
- Rating 4:  4 ( 21.1%) ████
- Rating 5:  0 (  0.0%) 

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  3 ( 27.3%) █████
- Rating 3:  2 ( 18.2%) ███
- Rating 4:  4 ( 36.4%) ███████
- Rating 5:  2 ( 18.2%) ███

### I know how RAG is used by AI Agents

**Pre-course distribution** (n=19):
- Rating 1:  9 ( 47.4%) █████████
- Rating 2:  2 ( 10.5%) ██
- Rating 3:  2 ( 10.5%) ██
- Rating 4:  5 ( 26.3%) █████
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  2 ( 18.2%) ███
- Rating 3:  0 (  0.0%) 
- Rating 4:  8 ( 72.7%) ██████████████
- Rating 5:  1 (  9.1%) █

### I can use inference engine/server to infer from multimodal LLMs

**Pre-course distribution** (n=19):
- Rating 1:  7 ( 36.8%) ███████
- Rating 2:  2 ( 10.5%) ██
- Rating 3:  5 ( 26.3%) █████
- Rating 4:  3 ( 15.8%) ███
- Rating 5:  2 ( 10.5%) ██

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  0 (  0.0%) 
- Rating 3:  4 ( 36.4%) ███████
- Rating 4:  4 ( 36.4%) ███████
- Rating 5:  3 ( 27.3%) █████

### I know how to choose a scheme for a certain LLM task

**Pre-course distribution** (n=19):
- Rating 1: 11 ( 57.9%) ███████████
- Rating 2:  2 ( 10.5%) ██
- Rating 3:  3 ( 15.8%) ███
- Rating 4:  3 ( 15.8%) ███
- Rating 5:  0 (  0.0%) 

**Post-course distribution** (n=11):
- Rating 1:  2 ( 18.2%) ███
- Rating 2:  0 (  0.0%) 
- Rating 3:  3 ( 27.3%) █████
- Rating 4:  5 ( 45.5%) █████████
- Rating 5:  1 (  9.1%) █

### I know different ways to parallelize LLM training

**Pre-course distribution** (n=19):
- Rating 1:  9 ( 47.4%) █████████
- Rating 2:  3 ( 15.8%) ███
- Rating 3:  1 (  5.3%) █
- Rating 4:  6 ( 31.6%) ██████
- Rating 5:  0 (  0.0%) 

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  2 ( 18.2%) ███
- Rating 3:  3 ( 27.3%) █████
- Rating 4:  4 ( 36.4%) ███████
- Rating 5:  2 ( 18.2%) ███

### I know when to use RAG for my usecase

**Pre-course distribution** (n=19):
- Rating 1:  8 ( 42.1%) ████████
- Rating 2:  0 (  0.0%) 
- Rating 3:  3 ( 15.8%) ███
- Rating 4:  7 ( 36.8%) ███████
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  2 ( 18.2%) ███
- Rating 3:  0 (  0.0%) 
- Rating 4:  6 ( 54.5%) ██████████
- Rating 5:  3 ( 27.3%) █████

### I know the common stages of a pre-training pipeline

**Pre-course distribution** (n=19):
- Rating 1:  3 ( 15.8%) ███
- Rating 2:  4 ( 21.1%) ████
- Rating 3:  7 ( 36.8%) ███████
- Rating 4:  4 ( 21.1%) ████
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  0 (  0.0%) 
- Rating 3:  3 ( 27.3%) █████
- Rating 4:  6 ( 54.5%) ██████████
- Rating 5:  2 ( 18.2%) ███

### I know the common stages of a post-training pipeline

**Pre-course distribution** (n=19):
- Rating 1:  3 ( 15.8%) ███
- Rating 2:  5 ( 26.3%) █████
- Rating 3:  6 ( 31.6%) ██████
- Rating 4:  4 ( 21.1%) ████
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  0 (  0.0%) 
- Rating 3:  4 ( 36.4%) ███████
- Rating 4:  5 ( 45.5%) █████████
- Rating 5:  2 ( 18.2%) ███

### I know different ways to perform quantization

**Pre-course distribution** (n=19):
- Rating 1:  7 ( 36.8%) ███████
- Rating 2:  2 ( 10.5%) ██
- Rating 3:  6 ( 31.6%) ██████
- Rating 4:  3 ( 15.8%) ███
- Rating 5:  1 (  5.3%) █

**Post-course distribution** (n=11):
- Rating 1:  0 (  0.0%) 
- Rating 2:  3 ( 27.3%) █████
- Rating 3:  3 ( 27.3%) █████
- Rating 4:  3 ( 27.3%) █████
- Rating 5:  2 ( 18.2%) ███

## Key Takeaways from Participant Feedback

Selected feedback from participants:

1. "Very good course so far! Maybe for the onboarding session, make sure participants can do simple tasks e.g. running jupyter notebook, copying a file etc. to save time during exercises?"

2. "Very well prepared and inclusive training!"

3. "I feel like these surveys are too long and skipped answering yesterday due to that. I would also appreciate being able to answer that I did not participate, since I missed the datapipeline session.    I really like Jayants teaching style, the instructions for doing the exercises has been clear enough to follow. Some other exercises were more difficult to follow and assumed I would know where the files were stored etc.     Overall I'm very happy with your workshop!"

4. "I had difficulty following the optimizer section, but partly because I needed more time with the first exercise to get started."

5. "Great workshop."

6. "A lot of things in this course were not so relevant to me. I am not advanced enough to do training / quantization / parallelization. Still some things in the course looks helpful towards my goal of automating the tedious work I need to do, something like:    - https://github.com/langchain-ai/deep_research_from_scratch/, so I am less limited by ChatGPT Plus plan I have to buy myself    - RAG with reasonably complicated PDF, also looking into figures/diagrams    - combining the above, so whatever ~100 papers to deep research finds about "immune checkpoints" it will download to my local disk, double check for relevance, and then answer my further questions using all these ~100 papers"

7. "The first answer is because there was a lot of confusion with the working directory and setup. The first day participants were asked to create a directory under /mimer/.../llm-workshop and if I am not wrong also to clone the workshop repo inside the personal directory we just created. And then during the workshop we were asked to use the HOME directory. It was very confusing understanding which directory you were referring to. Maybe I missed something but since many were in the same situation I would recommend you to set a main directory with all files needed for the workshop. Then, in the main directory create a script to automate the creation and copying process of the files that should be personal to follow the course, for example mkdir /mimer/.../llm-workshop/$USER; cp /mimer/.../llm-workshop/[set_of_paths] /mimer/.../llm-workshop/$USER/[set_of_paths]; cd /mimer/.../llm-workshop/[set_of_paths] ~/protocol/jupyter/[set_of_paths] . And then telling participants to execute that script at the beginning of each day (in case people don't join all days, or even after each break asking if there is anyone who have just joined). This would make the workshop easier to follow. Apart from the confusion the workshop is very good and I would recommend it for it. Thanks"

## Summary

- **Total learning outcomes tracked**: 11
- **Average improvement across all outcomes**: +1.40
- **Learning outcomes with positive improvement**: 11/11
- **Highest improvement**: "I know the common stages of a simple RAG" (+2.36)

---
tags:
  - Inference
---

# Accessing LLMs

## Quick start

### LM Studio

- Select directory (and download model)
- Chat
- Serving mode
    - REST API
    - lms Client

### vLLM

- Offline inference
- Serving engine

### Others

- ollama + open webui



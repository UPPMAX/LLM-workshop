{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to a NAISS LLM Workshop","text":""},{"location":"#wondering-how-llms-work","title":"Wondering how LLMs work?","text":"<p>\u26a0\ufe0f This website is still in training. Check back after a few more epochs!</p> <p>Course dates Schedule</p>"},{"location":"course_dates/","title":"Course dates","text":"Week Date Times Item 46 Wednesday 11th Nov 2025 24:00 Registration deadline 47 Monday 17th Nov 2025 11:00-12:00 Onboarding 47 Wednesday 19th Nov 2025 10:00-16:00 Day 1 47 Thursday 20th Nov 2025 10:00-16:00 Day 2 47 Friday 21th Nov 2025 10:00-16:00 Day 3 <ul> <li>Registration link: https://indico.chalmers.se/event/375/</li> <li>Zoom-link is available on registration page and is also sent to you automatically upon registration</li> </ul>","tags":["dates","days","calendar"]},{"location":"prerequisites/","title":"Prerequisites","text":"<p>Warning</p> <p>Getting an account can take a few days and if you're not accessing through your University getting access to the Chalmers VPN can also take a few days. Start on this in time.</p> <p>This LLM-Workshop is an advanced course in the sense that we expect basic familiarity with some concepts we will be using. We will cover a very brief refresher for the most important ones, but this will often not be sufficient to follow along if you are completely new to the topic.</p> <p>Additionally, there will be some steps to go through to get access and set up your environment on the cluster we will be using for this workshop.</p> <ol> <li>Complete getting access<ul> <li>This can take some time so start ASAP</li> <li>The relevant project to request access to is  <li>There is a recording of an old Alvis intro here</li> <li>Access the cluster and clone this repository in suitable location.<ul> <li>Note that connections are only allowed through SUNET</li> </ul> </li> <li>Prepare the workshop container.</li> <li>Prepare the interactive runtime.</li> <li>Submit the sanity check.</li>"},{"location":"prerequisites/#cloning-the-repository","title":"Cloning the repository","text":"<p>Start by connecting e.g. <code>ssh &lt;CID&gt;@alvis2.c3se.chalmers.se</code>. When you are on the cluster clone the repository.</p> <pre><code>cd path/of/your/choice  # optional\ngit clone https://github.com/UPPMAX/LLM-workshop.git\ncd LLM-workshop\n</code></pre>"},{"location":"prerequisites/#prepare-the-workshop-container","title":"Prepare the workshop container","text":"<p>The easy option is to use the container from project storage:</p> <pre><code>cd LLM-workshop/excercises/\nln -s /mimer/NOBACKUP/groups/llm_workshop/llm_workshop.sif .\n</code></pre> <p>If you don't have access to that you can build the container with:</p> <pre><code>cd LLM-workshop/excercises/\napptainer build llm_workshop.sif llm_workshop.def\n</code></pre>"},{"location":"prerequisites/#prepare-the-interactive-runtime","title":"Prepare the interactive runtime","text":"<p>Environments in the interactive apps are set-up with specific files that are used when launching the interactive app. See Alvis documentation.</p> <p>For this you can do something like: <pre><code>cp /portal/vscode/codeserver-container.sh ~/portal/vscode/llm-workshop.sh\nnano ~/portal/vscode/llm-workshop.sh  # edit this file to use ~/LLM-workshop/excercises/llm_workshop.sif\n</code></pre></p> <p>Finally try launching the VSCode interactive app on https://alvis.c3se.chalmers.se/, try it out and remember to cancel the job when you are done.</p>"},{"location":"prerequisites/#run-the-sanity-check","title":"Run the sanity check","text":"<pre><code>cd LLM-workshop/excercises/\nsbatch llm_workshop\n</code></pre> <p>Check that it has completed successfully with <code>sacct -X</code> (<code>STATE</code> should be <code>COMPLETED</code>).</p>"},{"location":"schedule/","title":"Schedule","text":"","tags":["schedule","timetable"]},{"location":"schedule/#tentative-schedule","title":"Tentative schedule","text":"<p>~6hr/day including breaks and lunch</p> <p>Day 1  </p> Item Time Instructor  Overview of the landscape and proper resources TDB Viktor  Brief introduction to public LLMs TDB Jayant  Accessing LLMs / Quick inference TDB Chia-Jung / Yunqi   LLM and hardware TDB Yunqi  LLM formats TDB Yunqi <p>Day 2  </p> Item Time Instructor  Datapipeline TDB Jayant  How to quantize (and other tools?) TDB Chia-Jung  Model parallelism TDB Yunqi  Multi-modalities TDB Chia-Jung  RAG TDB Jayant <p>Day 3  </p> Item Time Instructor  Prompt engineering TDB Jayant  Tools, chain/tree-of-thought TDB Viktor  Finetuning, RL TDB Jayant  HyperParameter tuning TDB Viktor  Evaluation metrics TDB Viktor","tags":["schedule","timetable"]},{"location":"day1/access_llm/","title":"Quick Start to Access LLMs","text":"","tags":["Inference"]},{"location":"day1/access_llm/#lm-studio","title":"LM Studio","text":"<p>LM Studio is a desktop app for developing and experimenting with LLMs. It has a friendly user interface and suitable for private usage. In this tutorial, we will use it to show some key concepts in LLM inference.</p> <p>We have deployed it on Alvis, you can find it in <code>Menu &gt; C3SE &gt; LM Studio</code>.</p> <p>Important</p> <p>LM Studio supports limited file format and may not scale well on clusters. Don't use it for productive work.</p> <p> </p>","tags":["Inference"]},{"location":"day1/access_llm/#basic-inference","title":"Basic inference","text":"<p>Once you start LM studio, it brings you to a chat window. On top of the chat window, you can see a drop-down list allowing you to select/download models.</p> <p> </p> <p>Before downloading any models, it is important to select a directory to save downloaded models. Click the folder icon in the sidebar, you can find that it saves models into your home directory by default. You can change the path to any directory where you have downloaded models. If you haven't downloaded any model, you had better set the path to a directory under your storage project. Otherwise, you run out of file/space quota easily.</p> <p> </p> <p>Once you set the path, you can go back to the chat window to download/load models and start a chat.</p> <p> </p>","tags":["Inference"]},{"location":"day1/access_llm/#openai-compatible-api-server","title":"OpenAI-Compatible API Server","text":"<p>Besides of the chat window, LM Studio also supports OpenAI compatible API server to handle HTTP requests. The server can be launched from the GUI by  toggling the option in Developer tab in the sidebar. Once you start the server, you can send HTTP requests to the listed endpoints. </p> <p></p> <p>In the figure, it shows there are four endpoints:</p> <ul> <li><code>/v1/models</code></li> <li><code>/v1/chat/completions</code></li> <li><code>/v1/completions</code></li> <li><code>/v1/embeddings</code></li> </ul> <p>You can test the API by sending HTTP request from your terminal by curl. For example:</p> <pre><code># Request for available models\n$ curl http://localhost:1234/v1/models\n{\n  \"data\": [\n    {\n      \"id\": \"llama-3.3-70b-instruct\",\n      \"object\": \"model\",\n      \"owned_by\": \"organization_owner\"\n    },\n    {\n      \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n      \"object\": \"model\",\n      \"owned_by\": \"organization_owner\"\n    }\n  ],\n  \"object\": \"list\"\n}\n</code></pre> <pre><code># Chat\n$ curl http://localhost:1234/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n    \"model\": \"llama-3.3-70b-instruct\",\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"why is the sky blue\" }\n    ]\n}'\n\n{\n  \"id\": \"chatcmpl-stubx36wa8neg1u8jo5re\",\n  \"object\": \"chat.completion\",\n  \"created\": 1746801158,\n  \"model\": \"llama-3.3-70b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of light by small particles or molecules in the atmosphere.\\n\\nHere's what happens:\\n\\n1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\\n2. **Light encounters tiny molecules**: The light encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2) in the atmosphere.\\n3. **Shorter wavelengths scatter more**: These small molecules scatter the shorter wavelengths of light, like blue and violet, more than the longer wavelengths, like red and orange. This is because the smaller molecules are more effective at scattering the higher-energy, shorter-wavelength light.\\n4. **Blue light is scattered in all directions**: As a result of this scattering, the blue light is dispersed in all directions, reaching our eyes from all parts of the sky.\\n5. **Our eyes perceive the sky as blue**: Since we see more blue light being scattered in all directions, our brains interpret the color of the sky as blue.\\n\\nThere are some additional factors that can affect the color of the sky:\\n\\n* **Dust and pollution**: Tiny particles in the atmosphere, like dust, smoke, or pollutants, can scatter light in different ways, making the sky appear more hazy or gray.\\n* **Water vapor**: Water molecules in the air can also scatter light, which is why the sky often appears more blue on dry days.\\n* **Time of day and sun position**: The color of the sky can change depending on the time of day and the position of the sun. During sunrise and sunset, the sky can take on hues of red, orange, and pink due to the scattering of light by atmospheric particles.\\n\\nIn summary, the sky appears blue because of the way that tiny molecules in the atmosphere scatter sunlight, favoring shorter wavelengths like blue and violet over longer wavelengths like red and orange.\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 40,\n    \"completion_tokens\": 406,\n    \"total_tokens\": 446\n  },\n  \"stats\": {},\n  \"system_fingerprint\": \"llama-3.3-70b-instruct\"\n}\n</code></pre> <p>More information can be found in the official document</p>","tags":["Inference"]},{"location":"day1/access_llm/#command-line-tools","title":"Command line tools","text":"<p>Once you have ever stared LM studio, it automatically installs a command line tool into you home directory: <code>~/.lmstudio/bin/lms</code>. With the tool, you can do the same operations as what you can do in the GUI. You can also see the models you have loaded from the GUI in the terminal</p> <pre><code>$ ~/.lmstudio/bin/lms --help\nlms &lt;subcommand&gt;\n\nwhere &lt;subcommand&gt; can be one of:\n\n- status - Prints the status of LM Studio\n- server - Commands for managing the local server\n- ls - List all downloaded models\n- ps - List all loaded models\n- get - Searching and downloading a model from online.\n- load - Load a model\n- unload - Unload a model\n- create - Create a new project with scaffolding\n- log - Log operations. Currently only supports streaming logs from LM Studio via `lms log stream`\n- import - Import a model file into LM Studio\n- flags - Set or get experiment flags\n- bootstrap - Bootstrap the CLI\n- version - Prints the version of the CLI\n\nFor more help, try running `lms &lt;subcommand&gt; --help`\n</code></pre> <pre><code>$ ~/.lmstudio/bin/lms status\n\n   \u250c Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                                           \u2502\n   \u2502   Server:  ON  (Port: 1234)               \u2502\n   \u2502                                           \u2502\n   \u2502   Loaded Models                           \u2502\n   \u2502     \u00b7 llama-3.3-70b-instruct - 42.52 GB   \u2502\n   \u2502                                           \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>$ ~/.lmstudio/bin/lms ps\n\n   LOADED MODELS\n\nIdentifier: llama-3.3-70b-instruct\n  \u2022 Type:  LLM\n  \u2022 Path: lmstudio-community/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf\n  \u2022 Size: 42.52 GB\n  \u2022 Architecture: Llama\n</code></pre>","tags":["Inference"]},{"location":"day1/access_llm/#advanced-settings","title":"Advanced settings","text":"<p>In LM Studio GUI, you can find advanced setting in the Developer tab. You can set the <code>temperature</code>, <code>top K</code>, <code>top P</code> values, etc in the inference setting. There are also parameters about performance, like GPU offload, CPU Thread, KV cache, etc.</p> <p> </p>","tags":["Inference"]},{"location":"day1/access_llm/#vllm","title":"vLLM","text":"<p>vLLM is a fast and easy-to-use library for LLM inference and serving.</p> <p>vLLM itself doesn't have a GUI interface, but it is efficient for LLM inference and allow users to load LLMs to multiple GPU and multiple nodes. It can scale well on clusters like Alvis.</p> <p>There are two main entrypoints in vLLM, OpenAI-Compatible API Server and LLM class. The former one is implemented by the <code>AsyncLLMEngine</code> class while the latter one is based on <code>LLMEngine</code> class.</p>","tags":["Inference"]},{"location":"day1/access_llm/#openai-compatible-api-server_1","title":"OpenAI-Compatible API Server","text":"<p>A typical way to use it is using the command line to serve models in OpenAI-compatible API servers. For example:</p> <p><pre><code>$ vllm serve openai/gpt-oss-20b --port 8000 --async-scheduling --quantization mxfp4\n</code></pre> will serve <code>gpt-oss-20b</code> model on <code>http://localhost:8000</code>.</p> <p>More arguments can be found here or in <code>vllm serve --help</code>.</p> <p>Once the vLLM server get launched successfully. The following APIs are available:</p> <pre><code>/v1/models, Methods: GET\n/v1/responses, Methods: POST\n/v1/responses/{response_id}, Methods: GET\n/v1/responses/{response_id}/cancel, Methods: POST\n/v1/chat/completions, Methods: POST\n/v1/completions, Methods: POST\n/v1/embeddings, Methods: POST\n/v1/score, Methods: POST\n/v1/audio/transcriptions, Methods: POST\n/v1/audio/translations, Methods: POST\n/v1/rerank, Methods: POST\n/v2/rerank, Methods: POST\n</code></pre> <p>You also get some APIs from vLLM itself such as:</p> <pre><code>/openapi.json, Methods: HEAD, GET\n/docs, Methods: HEAD, GET\n/health, Methods: GET\n/tokenize, Methods: POST\n/detokenize, Methods: POST\n...\n</code></pre>","tags":["Inference"]},{"location":"day1/access_llm/#offline-inference-llm-class","title":"Offline inference (LLM class)","text":"<p>As a python package, vLLM also provide <code>LLM</code> python class, which can be imported into python scripts and load models to do inference. For example:</p> <pre><code>from vllm import LLM\n\n# Initialize the vLLM engine.\nllm = LLM(model=\"facebook/opt-125m\")\n</code></pre> <p>The LLM class can accept many arguments and most of them are shared with the available arguments for <code>vllm serve</code>. However, some features are limited in <code>AsyncLLMEngine</code>, such as pipeline parallelism, and not supported in LLM class.</p> <p>Once a LLM instance is created, users can call the methods such as <code>chat</code> and <code>generate</code> as calling APIs in OpenAI-Compatible API server. Here is an example:</p> <pre><code>from vllm import LLM, SamplingParams\n\nsampling_params = SamplingParams(\n    temperature=0.6,\n    max_tokens=128,\n    top_p=0.9,\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Why is the sky blue?\"},\n]\n\nllm = LLM(\n    model=\"openai/gpt-oss-20b\",\n    tensor_parallel_size=4,\n    quantization=\"mxfp4\"\n)\n\noutput = llm.chat(messages, sampling_params, use_tqdm=False)\nprint(output[0].outputs[0].text)\n</code></pre> <p>More examples can be found in vLLM document.</p>","tags":["Inference"]},{"location":"day1/access_llm/#other-tools","title":"Other Tools","text":"<ul> <li>Transformer</li> <li>ollama + open webui</li> </ul>","tags":["Inference"]},{"location":"day1/introduction/","title":"LLM Workshop - Introduction","text":"","tags":["Introduction"]},{"location":"day1/introduction/#overview","title":"Overview","text":"<ul> <li>History of AI</li> <li>Compute and AI</li> <li>Ethics and concerns</li> <li>Introducing the workshop hardware</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#history-of-ai","title":"History of AI","text":"<ul> <li>How has AI developed over time?</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#ai-as-a-term-is-coined","title":"AI as a term is coined","text":"<p>We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. [...] An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.</p> <ul> <li>Darthmout Summer Research Project</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#early-nlp-1956-1966","title":"Early NLP \u2013 1956-1966","text":"<ul> <li>Rule based, lots of manual effort</li> <li>Lots of LISP</li> <li>Used for:<ul> <li>Information retrieval</li> <li>Basic chat-bots e.g. Eliza</li> <li>Limited translation systems</li> </ul> </li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#ai-winter-i-1974-1980","title":"AI Winter I \u2013 1974-1980","text":"<ul> <li>NLP Winter started even earlier</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#statistical-nlp-1980s","title":"Statistical NLP \u2013 1980s","text":"<ul> <li>Using statistics of the corpus</li> <li>Bag-of-words, N-grams</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#ai-winter-ii-1990s-early-2000s","title":"AI Winter II \u2013 1990s, early 2000s","text":"<ul> <li>The word \"AI\" is not a buzzword</li> <li>Research continues under other names</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#deep-learning-2012-","title":"Deep Learning \u2013 2012-","text":"<ul> <li>2012: AlexNet has less than 25% error on ImageNet challenge</li> <li>2017: Transformer architecture (Attention Is All You Need, retrospectic)</li> <li>2019: GPT-2 released. Surprisingly good.</li> <li>2020: GPT-3 released. Surprisingly still improving.</li> <li>2022: ChatGPT (GPT-3.5) released. General public starts to take notice.</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#deep-learning-transformer-architecture","title":"Deep Learning \u2013 Transformer architecture","text":"No recurrent connections,   thus more parallelizable.","tags":["Introduction"]},{"location":"day1/introduction/#deep-learning-attention-mechanism","title":"Deep Learning \u2013 Attention mechanism","text":"<ul> <li>Scaled Dot-Product Attention $$ \\mathrm{Attention}(V, K, Q) = \\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_K}}\\right)V $$</li> <li>Cross-attention: \\(Q = X_\\mathrm{dec} W_Q\\), \\(K = X_\\mathrm{enc} W_K\\) and \\(V = X_\\mathrm{enc} W_V\\)</li> <li>Self-attention: Same \\(X\\) used for all matrices</li> <li>In decoder, self-attention masks future tokens</li> <li>Autoregressive unimodal LLMs usually decoder only</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#compute-and-ai","title":"Compute and AI","text":"<ul> <li>What has changed?</li> <li>The bitter lesson by Richard Sutton, 2019</li> </ul> <p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.</p>","tags":["Introduction"]},{"location":"day1/introduction/#compute-use-over-time","title":"Compute use over time","text":"<ul> <li>Compute, but also data, architecture and algorithms</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#compute-and-performance","title":"Compute and performance","text":"","tags":["Introduction"]},{"location":"day1/introduction/#what-was-new-with-chatgpt","title":"What was new with ChatGPT?","text":"<ul> <li>Base models are pure language models</li> <li>Chat models are:<ul> <li>Instruct tuned (supervised)</li> <li>Reinforcement Learning with Human Feedback</li> </ul> </li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#rlhf","title":"RLHF","text":"<ul> <li>Enables RL when no clear scoring function available</li> <li>Relatively little human input needed</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#further-scaling","title":"Further scaling","text":"","tags":["Introduction"]},{"location":"day1/introduction/#ethics-and-issues","title":"Ethics and issues","text":"<ul> <li>Societal concerns</li> <li>Misuse concerns</li> <li>Misalignment concerns</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#societal-concerns","title":"Societal concerns","text":"<ul> <li>Perpetuated bias</li> <li>Confident falsehood and sycophancy</li> <li>Copyright &amp; IP issues</li> <li>Distribution of wealth and the job market<ul> <li>E.g. GDPval benchmark</li> </ul> </li> <li>Climate footprint<ul> <li>Word for word comparisson, AI is cheaper</li> <li>But, AI can generate a lot more text (rel. Jevons paradox</li> </ul> </li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#misuse-concerns","title":"Misuse concerns","text":"<ul> <li>Mass spear phishing, disinformation campaigns, ...</li> <li>Cyberattacks<ul> <li>Finding and exploiting vulnerabilities</li> </ul> </li> <li>Enabling bad actors<ul> <li>Dual use of artificial-intelligence-powered drug discovery</li> </ul> </li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#misalignment","title":"Misalignment","text":"<ul> <li>RLHF is only a step in the right direction</li> <li>Goodhart's law</li> <li>Misaligned leadership<ul> <li>What future are they aiming for?</li> </ul> </li> <li>Superintelligence </li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#introduction-to-the-hardware-for-this-workshop","title":"Introduction to the hardware for this workshop","text":"<ul> <li>Main reference: Alvis introduction material</li> <li>Compute clusters</li> <li>GPUs as compute accelerator</li> <li>Multi-GPU</li> <li>Containers</li> <li>Batch queue system</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#compute-clusters","title":"Compute clusters","text":"","tags":["Introduction"]},{"location":"day1/introduction/#the-compute-node","title":"The compute node","text":"<ul> <li>Speed-up by parallelization</li> <li>Feeding data to GPU memory (VRAM) often bottleneck</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#software","title":"Software","text":"<ul> <li>Default software environment intentionally sparse</li> <li>Use modules or containers to run software</li> <li>(Follow our recommendations when installing Python packages)</li> <li>We will use containers in this course</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#software-containers","title":"Software \u2013 containers","text":"<ul> <li>Apptainer containers</li> <li>A single file for your software and all dependencies</li> <li>(Building containers)</li> <li>Running software in a container <pre><code>apptainer exec your_container.sif python your_code.py\n</code></pre></li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#slurm","title":"SLURM","text":"<ul> <li>Batch queueing system</li> <li>Allocates resources in a fair and effective manner</li> <li>Resources are finite so expect queue times occasionally</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#slurm-workflow-preparing-job","title":"SLURM workflow \u2013 Preparing job","text":"","tags":["Introduction"]},{"location":"day1/introduction/#slurm-workflow-submitting-job-to-queue","title":"SLURM workflow \u2013 Submitting job to queue","text":"","tags":["Introduction"]},{"location":"day1/introduction/#slurm-workflow-job-starts","title":"SLURM workflow \u2013 Job starts","text":"","tags":["Introduction"]},{"location":"day1/introduction/#partial-command-overview","title":"Partial command overview","text":"<ul> <li>View queued and running jobs <code>squeue [--me]</code></li> <li>View previous jobs <code>sacct</code></li> <li>Submit jobs <code>sbatch &lt;JOBSCRIPT&gt;</code></li> <li>Cancel queued or running jobs <code>scancel &lt;JOBID&gt;</code></li> <li>More complete overview at SLURM documentation and Alvis intro</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#summary-of-introduction","title":"Summary of Introduction","text":"<ul> <li>Compute a key component to the success of LLMs</li> <li>Use and development of AI is not without its issues</li> <li>The hardware you can access</li> <li>Containers for accessing software</li> <li>SLURM batch queue system for running things on the cluster</li> </ul>","tags":["Introduction"]},{"location":"day1/introduction/#excercise","title":"Excercise","text":"<ol> <li>Finish prerequisites</li> <li>Navigate to your instance of <code>LLM-workshop</code></li> <li>Do a <code>git pull</code> to get the latest changes</li> <li>Launch an interactive session through https://alvis.c3se.chalmers.se/</li> <li>Modify and then submit <code>LLM-workshop/excercises/introduction/hello-llms.sh</code> TODO</li> </ol>","tags":["Introduction"]},{"location":"day1/llm_formats/","title":"LLM formats","text":""},{"location":"day1/llm_formats/#overview","title":"Overview","text":"<ul> <li>Formats of LLM models</li> <li>Formats of numbers</li> <li>Quantization of LLM</li> <li>Quantization and performance</li> </ul>"},{"location":"day1/llm_formats/#formats-of-llm-models","title":"Formats of LLM models","text":""},{"location":"day1/llm_formats/#so-you-want-to-use-a-llm-model","title":"So you want to use a LLM model","text":"<p>It is common for LLMs to be quantized after training. The quantization of LLMs is relevant to the model's compatibility with different implementations, as well its its performance, both in terms of accuracy and speed. This section will focus on explaining details of different floating point number formats and quantization methods.</p>"},{"location":"day1/llm_formats/#what-the-name-means","title":"What the name means","text":"<ul> <li><code>Llama-3.3</code>: model (architecture)</li> <li><code>70B</code>: size / number of parameters</li> <li><code>Instruct</code>: fine-tuning</li> <li><code>AWQ-INT4</code>: quantization</li> <li><code>GGUF</code>: model format</li> </ul> <p>Models architecture and size are often the first consideration when working on LLM.  But equally important are the format of models and the quantization method. Modern acceleration devices cater for the lower precision need of machine learning models, depending on the device you want to run on, quantized models might give significant speed up.</p>"},{"location":"day1/llm_formats/#file-formats-of-llms","title":"File-formats of LLMs","text":"<p>The gguf file fomat (image from huggingface)</p> <p>LLM models commonly consists of metadata, and the tensor themselves.</p> <p></p>"},{"location":"day1/llm_formats/#common-formats-of-llms","title":"Common formats of LLMs","text":"<ul> <li>bin/pth/tf: \"raw\" ML library formats;</li> <li>safetensors: used by huggingfacs;</li> <li>ggml/gguf: developed by llama.cpp (supports many qunatization formats);</li> <li>llamafile: by mozilla, single-file format, executable.</li> </ul> <p>You can find detailed model information for some model formats, example.</p> <p>Models are published on different formats and they are optimized for different usages. They can be converted to one another but has different purposes.</p> <p>raw formats used by ML formats might be handy for re-training. Some of them contain pickled data so might execute arbitrary code (unsafe).</p> <p>Newer formats like GGUF/safetensors are suitable for common model architectures (different engines will support them if the architecture is known). They are memory-mapped, which are especially useful for disk offloading.</p>"},{"location":"day1/llm_formats/#look-for-the-following","title":"Look for the following","text":"<ul> <li>Quantization method;</li> <li>Number format;</li> <li>Hardware compatibility hints.</li> </ul>"},{"location":"day1/llm_formats/#formats-of-numbers","title":"Formats of numbers","text":""},{"location":"day1/llm_formats/#why-do-we-care","title":"Why do we care?","text":"<ul> <li>Quantization allow you to run larger models;</li> <li>It might also eliminate expensive communication;</li> <li>ML tolerates lower numerical precision;</li> <li>Number formats also determines \"distributioin of information\";</li> </ul>"},{"location":"day1/llm_formats/#number-formats","title":"Number formats","text":""},{"location":"day1/llm_formats/#floating-point-formats-cont-1","title":"Floating point formats - cont. 1","text":""},{"location":"day1/llm_formats/#floating-point-formats-cont-2","title":"Floating point formats - cont. 2","text":""},{"location":"day1/llm_formats/#hardware-compatibility","title":"Hardware Compatibility","text":"hardware accel. note fp16/32/64 most gpus IEEE 754 fp8 (E4M3/E5M2) hooper Recent IEEE bf16 most gpus Google's tf32 nvidia-gpus Nvidia int4/8 most GPUs <p>See also Data types support by AMD RocM.</p> <p>Floating point number generally follows a IEEE standard format. However, details like the representation of negative zeros (NZ) and infinite values might be different.</p>"},{"location":"day1/llm_formats/#rule-of-thumb","title":"Rule of thumb","text":"<ul> <li>ML tasks favor a larger proportion of exponents;</li> <li>Google's bf16 (same range as fp32, less mantissa);</li> <li>training usually done in fp32/16;</li> <li>int4/8 is good for inference (on older GPUs).</li> </ul>"},{"location":"day1/llm_formats/#quantization-methods","title":"Quantization methods","text":""},{"location":"day1/llm_formats/#quantization-target","title":"Quantization target","text":"<ul> <li>Weight/activation/mixed percision (w8a16);</li> <li>KV-cache;</li> <li>Non-uniform;</li> </ul> <p>weights is usually the first thing to quantize, it is also the most supported way of quantizing the model. Depending on the hardware, it might or might not support converting the tensors between precision or doing tensor operations natively.</p> <p>For instance, FP8 is not officially support on Ampere GPUs (A40 and A100). While there exist implementations that makes w8a16 operations available, quantizating KV cache to FP8 currently need hardware support.</p> <p>Models can also been quantized non-uniformly</p>"},{"location":"day1/llm_formats/#asymmetric-qunatization","title":"(A)symmetric qunatization","text":"<ul> <li>linear transformation;</li> <li>depend on original range;</li> <li>position of zero.</li> </ul> <p>One important aspect when quantizing the models is the distribution of the model, the easiest way is to simply scale the parameters by a factor.</p> <p>To minimize loss of precision, we could map the parameters according to the max/min values of the parameter, rather than the number-format range. There, we need to choose whether we shift the zero point in the transform (but introduces complexity in computation).</p>"},{"location":"day1/llm_formats/#clipping","title":"Clipping","text":"<p>we can also choose to clip out the outlier to same more precision.</p>"},{"location":"day1/llm_formats/#calibration-for-weight-quantization","title":"Calibration for weight quantization","text":"<p>For parameters of the model We can simply quantize them, since we know their distribution. But given some small dataset but we can also improved the accuracy but estimating how important each parameter it. A popular way to do that is the GPTQ method.</p> <p></p> <p>Illustration of GPTQ method, where quantization are done to minimize the error, weighed by according to the inverse Hessian (sensitivity).</p>"},{"location":"day1/llm_formats/#calibration-for-activation-qunatization","title":"Calibration for activation qunatization","text":"<p>Can be dynamic or static.</p> <p>To also quantize the activation function, we need to estimate the range of activation, that has to be done by passing data to the model and collect minima/maximi. We can do that either dynamically (during inference) or statically (with a calibration set).</p>"},{"location":"day1/llm_formats/#post-training-quantization-methods-ptq","title":"Post-training quantization methods (PTQ)","text":"<ul> <li>Weights and/or activation;</li> <li>Calibration/accuracy trade off;</li> <li>Not detailed here: sparsification.</li> </ul> <p>Models may also be sparsified to reduce the required computation, this is commonly known as weight pruning. But some GPUs also support efficient evaluation of sparse matrices if the sparsity follow certain pattern (example with llm-compressor);</p> <p>So far we covered mostly the so-called PTQ method when we do/can not run the training (for a complete list with compatibility see vLLM guide).</p>"},{"location":"day1/llm_formats/#quantization-aware-training-qat","title":"Quantization aware training (QAT)","text":"<p>QAT introduce quantization error during training;</p> <p> </p> <p>But we can also get higher accuracy by using the Quantization aware training (QAT) method. There we do the training and perform the quantization/dequantization; which this the first thing we gain is that we can actually optimize the quantization parameters as part of the training process.</p>"},{"location":"day1/llm_formats/#quantization-aware-training-qat-cont","title":"Quantization aware training (QAT) - cont.","text":"<p>The reason why it might work better, is that by introducing the quantization error in the training process, we force the model to land in a local minima where it is less sensitive to model parameters. So even the original model performs worth, the quantized model works better</p>"},{"location":"day1/llm_formats/#summary","title":"Summary","text":""},{"location":"day1/llm_formats/#when-choosing-a-model","title":"When choosing a model","text":"<ul> <li>Know the hardware/implementation compatibility;</li> <li>Find the right model/format/qunatization;</li> <li>Quantize if needed;</li> <li>Look up/run benchmarks.</li> </ul>"},{"location":"day1/llm_formats/#other-useful-links","title":"Other useful links","text":"<p>Benchmarks:</p> <ul> <li>derek135/quantization-benchmarks</li> </ul>"},{"location":"day1/llm_hardware/","title":"LLM and hardware","text":""},{"location":"day1/llm_hardware/#overview","title":"Overview","text":"<ul> <li>Computations in LLMs</li> <li>LLM on super-comupters</li> </ul>"},{"location":"day1/llm_hardware/#computations-in-llms","title":"Computations in LLMs","text":""},{"location":"day1/llm_hardware/#neural-networks","title":"Neural networks","text":"<ul> <li>Learn patterns by adjusting parameters (weights);</li> <li>Training = prediction \u2192 differentiation \u2192 update;</li> <li>So far: mini-batch &amp; optimizer &amp; big \u2192 good.</li> </ul> <p>Neural networks are building blocks of modern machine learning applications,  the principle is simple, just like your regular gradient descent, you:</p> <ul> <li>compute the output of your model;</li> <li>compute the loss function according to reference output</li> <li>compute the gradient of loss with respect to your parameters;</li> <li>update you parameters slightly in the direction that reduces the loss the most;</li> </ul> <p>Neural networks are however:</p> <ul> <li>easy to differentiate: thanks to automatic differentiation;</li> <li>easy to parallelize: matrix multiplications can be done in parallel;</li> <li>easy to scale: training is done on small subsets of data per step.</li> </ul> <p>which makes it extermely easy to scale up, and show great performance when  scaled up. [^1]</p> <p>Note that during training, we need to store multiple copies of all model  parameters (for gradients, optimizer states, etc.), which multiplies memory  needs.</p> <p>PS: If you are suprised that large models work better, you are not alone; see  double descent</p>"},{"location":"day1/llm_hardware/#transformer","title":"Transformer","text":"<ul> <li>Transformer computes relationships between tokens (attention);</li> <li>tokens can be processed in parallel</li> </ul> <p>Transformers is an innovation that makes a training a large language model  practical.  Unlike RNNs or LSTMs, they do not rely on a hidden state that is  carried sequentially.</p> <p>Instead, the transformer computes relationships between all tokens in a  sequence using the self-attention mechanism.  This means that during training,  all tokens can be processed in parallel.</p> <p>(Caching of) The relations between tokens will be crucial to inference  performance, but for now, we can see transformer as just a composition of  neural network blocks that predicts the next token with a sequence of previous  ones.</p>"},{"location":"day1/llm_hardware/#training-of-llms","title":"Training of LLMs","text":"<ul> <li>Just neural networkes that can be parallelized more efficiently;</li> </ul>    Training of LLMs are not very different from what we talked about. But now we  can have a rough view of the big picture, you have your data, you feed it to  some memory, you let some processor work on it, get the gradient, the updated  gradient, you put in another data, you continue."},{"location":"day1/llm_hardware/#fine-tuninig-of-llms","title":"Fine-tuninig of LLMs","text":"<ul> <li>With specialized data (instruct, chat, etc);</li> <li>Less memory usage by \"freezing parameters\"</li> </ul> <p>Once a base model is trained, we usually fine-tune it on specific data (instruct, chat, etc.).</p> <p>From a computation point of view, fine-tuning is really the same task as training, but you we can use some tricks to reduce the resource we need.</p> <p>Above is a diagram of the LoRA (Low-Rank Adaptation) algorithm. Instead of updating a full weight matrix, we consider updating it by a matrix product of two small ones, this way we still need to do one copy of the big matrix, but the backward path we just have the two low-rank matrices.</p>"},{"location":"day1/llm_hardware/#inference-of-llms","title":"Inference of LLMs","text":"<ul> <li>GPT-style inference: pre-filling and decoding;</li> <li>Pre-filling: process the input prompt in parallel;</li> <li>Decoding: generate new tokens one-by-one, using cached results.</li> </ul> <p>Inference will need much less memory than training as we only need the forward pass. But this is actually an interesting aspect of LLMs as compared to other common machine learning tasks.</p> <p>When inferencing with LLM, we are essentially two things:</p> <ul> <li>Pre-filling \u2014 we process the entire prompt, this can be done in parallel    efficiently;</li> <li>decoding: we generate one token at a time, but the intermediate results from    previous can be cached as key\u2013value (KV) cache, saving computation at the    expense of memory.</li> </ul> <p>Think about what your task in mind, will it be more heavy in pre-filling or decoding?</p>"},{"location":"day1/llm_hardware/#optimize-caches-for-inference","title":"Optimize caches for inference","text":"<ul> <li>KV cache:</li> <li>paged attention: indexed blockes of caches;</li> <li>flash attention: fuse operations to reduce caches;</li> </ul> <p>more in-depth discussion of the technique where that visualization is from: paged attention from first principles.</p> <p>For this reason many effort in improving inference of LLMs has been put on improving efficiency of memory accessing patterns and reducing the memory needed. As an example, the paged attention mechanism groups adjacent tokens into virtual memory \"pages\" like has been done in operating system kernels.</p> <p>This allows us to efficiently use the fast memory on the GPUs. You can find more examples and techniques in the blog linked.</p>"},{"location":"day1/llm_hardware/#key-takeaway","title":"Key takeaway","text":"<ul> <li>LLMs/NNs benefit from massive parallelization;</li> <li>Need for different tasks:</li> <li>training: memory + compute + data throughput;</li> <li>fine-tuninig: similar to training, cheaper;</li> <li>pre-filling: compute;</li> <li>decoding: memory;</li> </ul>"},{"location":"day1/llm_hardware/#llm-on-hpc-clusters","title":"LLM on HPC clusters","text":""},{"location":"day1/llm_hardware/#llm-on-general-computers","title":"LLM on general computers","text":"<ul> <li>Mostly about inference;</li> <li>Quantization;</li> <li>CPU offloading;</li> <li>Memory-mapped file formats;</li> </ul>"},{"location":"day1/llm_hardware/#hpc-clusters","title":"HPC clusters","text":"<ul> <li>Racked computer nodes;</li> <li>Parallel network storage;</li> <li>Infiniband/RoCE networking;</li> </ul> <p>HPC are designed for parallel computing; the hardware is good at handing:</p> <ul> <li>fast communication between node;</li> <li>fast access to storage (local or shared);</li> <li>many CPUs/GPUs in one node</li> </ul>"},{"location":"day1/llm_hardware/#alvis-hardware-compute","title":"Alvis hardware - compute","text":"Data type A100 A40 V100 T4 FP64 9.7 | 19.5 0.58 7.8 0.25 FP32 19.5 37.4 15.7 8.1 TF32 156 74.8 N/A N/A FP16 312 149.7 125 65 BF16 312 149.7 N/A N/A Int8 624 299.3 64 130 Int4 1248 598.7 N/A 260 <p>Alvis was build for AI research, so it's equipped with latest (at the time) GPU acceleration cards. They are capable of doing fast floating point operations in reduced precision (see next section).</p>"},{"location":"day1/llm_hardware/#alvis-hardware-network-storage","title":"Alvis hardware - network &amp; storage","text":"<ul> <li>Fast storage: WEKA file system;</li> <li>Infiniband: 100Gbit (A100 nodes);</li> <li>Ethernet: 25Gbit (most other nodes);</li> </ul> <p>Not only so, Alvis is also equipped with fast storage system backed by flash storage; on the most powerful nodes (4xA100 GPUs) infiniband network that goes directly to storage is available.</p> <p>They were designed to facilitate fast loading of data, which is useful for any training tasks. In the case of LLM, one should already benefit from that for training.</p> <p>It is worth noting that the while one typically do not need such fast storage for inference, the LLM inference can actually take advantage of fast storage backend, this is rather experimental, but take a look at the LMcache package if you want to optimize you inference tasks for real.</p>"},{"location":"day1/llm_hardware/#running-llms-on-supercomputers","title":"Running LLMs on supercomputers","text":"<ul> <li>Most common bottleneck: memory</li> <li>Quantized models to fit larger models;</li> <li>Parallelize the model across GPUs or nodes;</li> </ul> <p>Supercomputers allow us to run larger LLMs because of not only the more powerful nodes, but also the nodes are connected with fast internet connection. But we still have the same issue as most high performance computing tasks, the compute needs to access the memory and the data need to transferred.</p> <p>Most commonly, when you try to run a large model (70B and 400B) parameters, you need to split the models to many GPUs. You can choose to quantize the model, so that the model use less memory, and you need less GPUs and less parallelization issues. To this end, you will need to look up the compatibility between the model, hardware and implementation.</p> <p>You will also need to think about how do you parallelize the models, you have the options:</p> <ul> <li>Tensor parallelism;</li> <li>Pipeline parallelism;</li> <li>Data parallelism</li> </ul> <p>So next we will introduce the different formats in LLMs, to give you an idea of what it means when you download a certain model, and tomorrow we will walk through the</p>"},{"location":"day1/llm_hardware/#tools-to-gain-information","title":"Tools to gain information","text":"<ul> <li>grafana (network utilization, temp disk);</li> <li>nvtop, htop (CPU/GPU utilization, power draw);</li> <li>nvidia nsight (advanced debugging and tracing);</li> </ul> <p>See details in C3SE documentation.</p>"},{"location":"day1/llm_hardware/#summary","title":"Summary","text":""},{"location":"day1/llm_hardware/#take-home-messages","title":"Take home messages","text":"<ul> <li>LLMs/neural networks benefit from massive parallelization;</li> <li>Same issue of memeory vs. compute-bound;</li> <li>Some optimization strategies;</li> <li>Be aware of the troubleshooting tools!</li> </ul>"},{"location":"day1/llm_hardware/#useful-links","title":"Useful links","text":"<ul> <li>nanotron has some in-depth discussion about the efficiency of model training;   (The Ultra-Scale Playbook), as well as a prediction memory estimation tool;</li> <li>Alvis hardware specifications;</li> <li>Alvis monitoring tools;</li> </ul>"},{"location":"day1/public_llms/","title":"Brief introduction to publicly available LLMs","text":"Learning outcomes <ul> <li>To understand the different categories that LLM comes in</li> <li>To know which matrices to look at for your particular usecase</li> </ul>","tags":["Public llm","Introduction"]},{"location":"day1/public_llms/#arena","title":"Arena","text":"Narrowing performance gap on MMLU benchmark (Apr 2022 - July 2025) with human domain experts at 89.8% <p>Open-weight models are catching up with closed source models steadily<sup>1</sup><sup>2</sup>. However, creating high-quality benchmarks is an active area of research as the existing ones are beginning to plateau. </p>","tags":["Public llm","Introduction"]},{"location":"day1/public_llms/#categories","title":"Categories","text":"<ul> <li>LLMs come in wide-range of \"openness\".</li> <li>Public != Open.</li> <li>\u201cPublicly Available\u201d means that the model checkpoints can be publicly accessible (terms can still apply) while \u201cClosed Source\u201d means the opposite.</li> </ul> Category (ordered by openness) Weights available? Inference Fine\u2011tuning Redistribute weights / derivatives Typical license Examples Open Source (OSI\u2011compatible) \u2705 Full \u2705 \u2705 \u2705 Apache\u20112.0 / MIT Mistral 7B ; OLMo 2 ; Alpaca Open Weights (restricted / gated) \u2705 Full \u2705 \u26a0\ufe0f License\u2011bound (e.g., research\u2011only / carve\u2011outs) \u274c Usually not allowed Custom terms (Llama / Gemma / RAIL) Llama\u202f3 (Meta Llama 3 Community License); Gemma\u202f2 (Gemma Terms of Use); BLOOM (OpenRAIL) Adapter\u2011only / Delta releases \u26a0\ufe0f Partial (adapters/deltas) \u2705 (after applying) \u2705 (adapters) \u2705 Adapters (base license applies) Mixed LoRA adapters over a base model Proprietary API + FT \u274c \u26a0\ufe0f API-only \u26a0\ufe0f API\u2011only (no weights export) \u274c Vendor ToS OpenAI (GPT\u20114.1, o4\u2011mini FT/RFT); Cohere (Command R/R+ FT); Anthropic (Claude\u202f3 Haiku FT via Bedrock) Proprietary API\u2011only \u274c \u26a0\ufe0f API-only \u274c \u274c Vendor ToS Google Gemini API","tags":["Public llm","Introduction"]},{"location":"day1/public_llms/#leaderboard","title":"Leaderboard","text":"<p> Open LLM Leaderboard</p> <p>Other notable leaderboards:  - HELM (Holistic Evaluation of Language Models by Stanford)  - LMArena (focus on open-weight models by UC Berkeley)</p>","tags":["Public llm","Introduction"]},{"location":"day1/public_llms/#benchmarks-to-consider","title":"Benchmarks to consider","text":"<p>Focus on a small set of comparable metrics (most appear on the Open LLM Leaderboard or model cards):</p> <p>Core capability benchmarks (higher is better unless noted)</p> <ul> <li>MMLU-Pro<sup>3</sup>: general academic/world knowledge </li> <li>GPQA<sup>4</sup>: Q&amp;A dataset designed by domain experts (PhD-level))</li> <li>MuSR<sup>5</sup>: Reasoning with very long contexts (up to 100K tokens)</li> <li>MATH<sup>6</sup>: high-school competition math problems</li> <li>IFEval<sup>7</sup>: Testing ability to strictly follow instructions</li> <li>BBH<sup>8</sup>: reasoning &amp; commonsense</li> </ul> Category Benchmarks (examples) Orgs with open weights that report them General academic / world knowledge MMLU, MMLU-Pro, CMMLU Meta (LLaMA), Mistral, Cohere, DeepSeek Domain expert level GPQA, CEval, CMMLU Meta (LLaMA papers mention expert subsets), Cohere (Command evals), DeepSeek (reports CEval/CMMLU/GPQA) Reasoning with long context MuSR, LongBench / long-context evals Mistral (Mixtral with long context, reported evals), DeepSeek (long-context benchmarks in tech report) High-school competition / advanced math GSM8K, MATH, AIME Meta (MATH, GSM8K), Mistral (GSM8K, MATH), Cohere (GSM8K), DeepSeek (MATH, GSM8K, AIME) Instruction following / alignment IFEval, instruction eval suites Meta (instruction-tuned LLaMA), Cohere (Command-R+ evals), DeepSeek (instruction following evals) Reasoning &amp; commonsense BBH, HellaSwag, Winogrande, PiQA, ARC, DROP Meta (HellaSwag, BBH), Mistral (HellaSwag, Winogrande), Cohere (commonsense evals), DeepSeek (HellaSwag, BBH, PiQA, Winogrande, ARC, DROP) Code completion &amp; debugging HumanEval, MBPP, LeetCode, Codeforces Meta (HumanEval), Mistral (HumanEval, MBPP), Cohere (HumanEval, MBPP), DeepSeek (HumanEval, MBPP, LeetCode) <p>Note: Mulitlingual and multimodal benchmarks are not covered here in detail.</p> Detailed benchmark coverage per open-weight model provider Benchmark Meta (LLaMA) Mistral Cohere (Command-R+) DeepSeek MMLU / MMLU-Pro / CMMLU \u2705 \u2705 \u2705 \u2705 GPQA / CEval (expert Q&amp;A) \u26aa (GPQA subsets in papers) \u26aa (less common) \u2705 \u2705 MuSR / LongBench / long-context evals \u26aa (not main focus, context \u226432k) \u2705 (Mixtral-8x22B long context) \u26aa \u2705 GSM8K (math word problems) \u2705 \u2705 \u2705 \u2705 MATH (competition-level) \u2705 \u2705 \u26aa \u2705 AIME (advanced math) \u26aa \u26aa \u26aa \u2705 IFEval / Instruction evals \u2705 \u26aa \u2705 \u2705 BBH (BigBench Hard) \u2705 \u26aa \u26aa \u2705 HellaSwag \u2705 \u2705 \u26aa \u2705 Winogrande \u26aa \u2705 \u26aa \u2705 PiQA \u26aa \u26aa \u26aa \u2705 ARC (AI2 Reasoning Challenge) \u26aa \u26aa \u26aa \u2705 DROP (reading comp / commonsense) \u26aa \u26aa \u26aa \u2705 HumanEval (code completion) \u2705 \u2705 \u2705 \u2705 MBPP (Python problems) \u26aa \u2705 \u2705 \u2705 LeetCode / Codeforces evals \u26aa \u26aa \u26aa \u2705 <p>\u2705 = reported officially in model card / tech report / benchmarks page</p> <p>\u26aa = not a primary benchmark for that org (either not reported or only mentioned indirectly)</p> <p>So what models for look for, while we do our research spanning couple of years?  - Look for Chinese model makers. This year has been there's.</p> <ol> <li> <p>The path forward for large language models in medicine is open. Nature \u21a9</p> </li> <li> <p>Closed-source vs. open-weight models LinkedIn \u21a9</p> </li> <li> <p>MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. arXiv \u21a9</p> </li> <li> <p>GPQA: A High-Quality Dataset for Evaluating Question Answering in Specialized Domains. arXiv \u21a9</p> </li> <li> <p>MuSR: A Benchmark for Evaluating Mathematical Understanding and Symbolic Reasoning in Large Language Models. arXiv \u21a9</p> </li> <li> <p>MATH: Measuring Mathematical Problem Solving With the MATH Dataset. arXiv \u21a9</p> </li> <li> <p>IFEval: Instruction-Following Evaluation for Large Language Models. arXiv \u21a9</p> </li> <li> <p>BBH: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv \u21a9</p> </li> </ol>","tags":["Public llm","Introduction"]},{"location":"day2/data_pipelines/","title":"Data Pipelines","text":"Learning outcomes <ul> <li>Understand the key components of data pipelines for LLMs  </li> <li>Perform data preparation for a pre-training task</li> </ul> <ul> <li> <p>Well-designed data pipelines determine LLM quality, safety, and training throughput.</p> </li> <li> <p>Success of all frontier models like GPT-5 relies heavily on quality data and constructing effecient pipelines around it that reduces training compute and improves the capabilities of the model that we desire it to have. </p> </li> </ul> <p>Smol LLM training team</p> <p>From our experience, and though it might disappoint architecture enthusiasts, the biggest performance gains usually come from data curation.</p>","tags":["Data pipelines"]},{"location":"day2/data_pipelines/#pre-training","title":"Pre-training","text":"<p>Goal: Assemble large, diverse, governed corpora and feed tokens efficiently to the model to learn general-purpose representations. LLM learns in self-supervised fashion.</p> <p>Raw data, is often messy and unsuitable for learning linguistic semantics. It typically exists in diverse formats like HTML, PDFs, spreadsheets etc, requiring extensive preprocessing to make it usable for training. Challenge lies in preserving the content and structure during this lossy process of data cleaning.</p> <ul> <li> <p>Data acquisition and licensing (1)</p> </li> <li> <p>Content extraction, normalization and detection (2)</p> </li> <li> <p>Deduplication (3)</p> </li> <li> <p>Quality filtering, decontamination (4)</p> </li> <li> <p>Tokenization and training (5)</p> </li> <li> <p>Mixture building and sampling (6)</p> </li> <li> <p>Storage and sharding (7)</p> </li> <li> <p>Training and observability (8)</p> </li> <li> <p>Continuous Evaluation (9)</p> </li> <li> <p>Data governance and ethics (10)</p> </li> </ul> <ol> <li>web crawl, books, Github code, academic papers. PII and copyright governance. Web crawl : CC </li> <li>language ID, Unicode cleanup, boilerplate removal, doc boundaries. Language classifiers: GlotLID, Fasttext</li> <li>exact and near-dup (MinHash/SimHash); mitigate contamination and overfitting.</li> <li>heuristic/classifier filters (toxicity, spam), quality scoring, temperature sampling.</li> <li>train vocab (BPE/Unigram), pre-tokenize, pack sequences respecting EOD. Tokenization: HF Tokenizers, </li> <li>domain/language balance, curriculum, up/down-sampling.</li> <li>Arrow/Parquet/WebDataset, deterministic sharding, resume-safe streaming. Arrow: HF Datasets</li> <li>Remove overlaps with evals; versioning, lineage, dashboards. Checkpoint/logging: HF Trainer</li> <li>benchmark language understanding, reasoning, QA etc. Bias, stereotype, toxicity and answer safety checks.</li> <li>Ethical charter, inspection tools for data composition, licensing, artifact release for reproducibility and further research. </li> </ol> Full reproduction of the FineWeb dataset fineweb.py Resources <ul> <li> <p>LLM papers on data and data pipelines:</p> <ul> <li>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</li> <li>CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</li> <li>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only</li> <li>Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</li> <li>RedPajama: an Open Dataset for Training Large Language Models</li> </ul> </li> <li> <p>End-to-end Data preprocessing libraries:</p> <ul> <li>HF datatrove</li> <li>Nvidia Curator</li> <li>Webdataset</li> </ul> </li> <li> <p>Classic NLP data preprocessing libararies:</p> <ul> <li>Explosion spaCy</li> <li>NLTK</li> <li>StanfordNLP Stanza</li> </ul> </li> </ul>","tags":["Data pipelines"]},{"location":"day2/data_pipelines/#post-training","title":"Post-training","text":"<p>Goal: Align base LLMs to new tasks or improve its existing abilities in chat-based dialogs, structured tasks or domain-specific data.</p> <p>This aligning is done via supervised fine-tuning and preference optimization (reward modelling or RLHF/RLAIF).</p> <ul> <li> <p>Collection and cleaning (1)</p> </li> <li> <p>Curation (2)</p> </li> <li> <p>Transformation (3)</p> </li> <li> <p>Validation (4)</p> </li> </ul> <ol> <li>Instruction\u2013response pairs, multi-turn formatting style and tool-use coverage. Synthetic data generation.  </li> <li>PII filtering and annotation by humans or AI. Ranking and scoring by humans or smaller models for Reward modelling. Deduplication.  </li> <li>Tokenization, formatting into chat templates, sharding and packing for effecient GPU training. Tokenizers: HF Fast-tokenizer</li> <li>Schema validataion (e.g. via pydantic), quality checks, benchmarking and collecting stats.</li> </ol> Resources <ul> <li> <p>LLM papers on data and data pipelines:</p> <ul> <li>Training language models to follow instructions with human feedback</li> <li>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</li> <li>Constitutional AI: Harmlessness from AI Feedback</li> <li>LIMA: Less Is More for Alignment</li> <li>Self-Instruct: Aligning Language Models with Self-Generated Instructions</li> </ul> </li> </ul> <p>https://learn.deeplearning.ai/courses/pretraining-llms/lesson/xfpqx/data-preparation https://learn.deeplearning.ai/courses/pretraining-llms/lesson/wqgv4/packaging-data-for-pretraining</p>","tags":["Data pipelines"]},{"location":"day2/multimodal/","title":"Multi-Modality","text":"","tags":["multi-modality","inference"]},{"location":"day2/multimodal/#introduction","title":"Introduction","text":"<ul> <li>architecture? https://arxiv.org/abs/2405.17927</li> </ul>","tags":["multi-modality","inference"]},{"location":"day2/multimodal/#inference-in-vllm","title":"Inference in vLLM","text":"<p>The way to use multimodal models is similar to using normal LLMs. VLMs can also be hosted as OpenAI-compatible API servers or loaded as local model instances. The main difference is the content of data to be sent to the model. Instead of text-only messages like <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"...\"},\n    ...\n]\n</code></pre> The content has to be expanded with data type and the data match with the type. For example, to send an image, the content may look like <pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What is shown in the image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://...\"}}\n        ]\n    }\n]\n</code></pre> Instead of sending a URL in the content, a base64-encoded can also be sent to models, such as  <pre><code>import base64\nwith open(\"image.jpg\", \"rb\") as image_file:\n    data = base64.b64encode(image_file.read())\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What is shown in the image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"data\"}}\n        ]\n    }\n]\n</code></pre></p> <p>A complete example:</p> <pre><code>#!/bin/bash\n#SBATCH --gpus-per-node=A100:4\n#SBATCH --time=2:00:00\n\nVLLM_PORT=$(find_ports) # get a random port\nNGPUS=${SLURM_GPUS_ON_NODE:-1}\n\nexport APPTAINERENV_VLLM_DISABLE_COMPILE_CACHE=1\n\n### Llama-3.2-11B-Vision-Instruct\n# https://github.com/vllm-project/vllm/issues/27198\nVLLM_SIF=/apps/containers/vLLM/vllm-0.9.1.sif\nHF_MODEL=/mimer/NOBACKUP/Datasets/LLM/huggingface/hub/models--neuralmagic--Llama-3.2-11B-Vision-Instruct-quantized.w4a16/snapshots/7f66874ab1a17131069ffede32f5efaad2cb80b5/\nMODEL_NAME=$(echo \"$HF_MODEL\" | sed -n 's#.*/models--\\([^/]*\\)--\\([^/]*\\)/.*#\\1/\\2#p')\n\nvllm_opts=\"--tensor-parallel-size=$NGPUS\"\nvllm_opts+=\" --max-model-len=10000\"\nvllm_opts+=\" --no-use-tqdm-on-load\"\nvllm_opts+=\" --enable-auto-tool-choice\"\nvllm_opts+=\" --tool-call-parser llama3_json\"\nvllm_opts+=\" --gpu-memory-utilization 0.5\"\nvllm_opts+=\" --max-num-seqs=16\"  # multimodal\nvllm_opts+=\" --enforce-eager\"  # multimodal\nvllm_opts+=\" --limit-mm-per-prompt.image 2\"  # multimodal\nvllm_opts+=\" --limit-mm-per-prompt.video 1\"  # multimodal\nvllm_opts+=\" --allowed-local-media-path=${HOME}\"  # multimodal\n\napptainer exec $VLLM_SIF \\\n    vllm serve ${HF_MODEL} \\\n        --served-model-name $MODEL_NAME \\\n        --port ${VLLM_PORT} \\\n        ${vllm_opts} \\\n        &gt; vllm.out 2&gt; vllm.err &amp;\nVLLM_PID=$!\n\nif timeout 600 bash -c \"tail -f vllm.err | grep -q 'Application startup complete'\"; then\n    echo \"vLLM is running. Sending test request\"\nelse\n    echo \"vLLM doesn't seem to start, aborting\"\n    echo \"Terminating VLLM\" &amp;&amp; kill -15 ${VLLM_PID}\n    exit\nfi\n\ncurl http://localhost:$VLLM_PORT/v1/models | jq .\n\nBASE64IMG=$(base64 M87BH.jpg)\ncat &gt; payload.json &lt;&lt; EOF\n{\n    \"model\": \"neuralmagic/Llama-3.2-11B-Vision-Instruct-quantized.w4a16\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is shown in the image?\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64, $BASE64IMG\"}}\n            ]\n        }\n    ],\n    \"temperature\": 0\n}\nEOF\ncurl http://localhost:$VLLM_PORT/v1/chat/completions \\\n    -H \"Content-Type: application/json; charset=utf-8\" \\\n    -d @payload.json | jq\n\nIMGURL=\"https://upload.wikimedia.org/wikipedia/commons/4/4f/Black_hole_-_Messier_87_crop_max_res.jpg\"\ncat &gt; payload.json &lt;&lt; EOF\n{\n    \"model\": \"neuralmagic/Llama-3.2-11B-Vision-Instruct-quantized.w4a16\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is shown in the image?\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"$IMGURL\"}}\n            ]\n        }\n    ],\n    \"temperature\": 0\n}\nEOF\ncurl http://localhost:$VLLM_PORT/v1/chat/completions \\\n    -H \"Content-Type: application/json; charset=utf-8\" \\\n    -d @payload.json | jq\n</code></pre>","tags":["multi-modality","inference"]},{"location":"day2/parallelization_schemes/","title":"LLM parallelization schemes","text":""},{"location":"day2/parallelization_schemes/#motivation","title":"Motivation","text":"<ul> <li>Parallelization is necessary;</li> <li>Good vs. bad parallelization.</li> </ul>"},{"location":"day2/parallelization_schemes/#strategies","title":"Strategies","text":""},{"location":"day2/parallelization_schemes/#data-parallelism-dp","title":"Data parallelism (DP)","text":"<ul> <li>Trivial to implement;</li> <li>Syncing overhead for training;</li> <li>Memory cost due to replication.</li> </ul>"},{"location":"day2/parallelization_schemes/#overlapping-in-dp","title":"Overlapping in DP","text":"<ul> <li>Overlapping reduces communication overhead.</li> </ul>"},{"location":"day2/parallelization_schemes/#sharding-in-dp","title":"Sharding in DP","text":"<ul> <li>Sharding reduces memory usage.</li> </ul>"},{"location":"day2/parallelization_schemes/#tensor-parallelism","title":"Tensor parallelism","text":"<ul> <li>Row vs. column version;</li> <li>Expensive communication.</li> </ul>"},{"location":"day2/parallelization_schemes/#tensor-parallelism-cont","title":"Tensor parallelism (cont.)","text":"<ul> <li>Tensor parallelism reduces memory usage;</li> <li>Efficiency reduce with inter-node parallelization;</li> </ul>"},{"location":"day2/parallelization_schemes/#pipeline-parallelism","title":"Pipeline parallelism","text":"<ul> <li>Training: bubbling problem;</li> <li>Combine with DP to for better performance;</li> </ul>"},{"location":"day2/parallelization_schemes/#pipeline-parallelism-cont","title":"Pipeline parallelism (cont.)","text":"<ul> <li>More strategies exist;</li> <li>Balancing, bubble, memory and communication;</li> <li>Implement is not trivial.</li> </ul>"},{"location":"day2/parallelization_schemes/#expert-moe-parallelism","title":"Expert (MoE) parallelism","text":"<ul> <li>Activate only subsets of experts per token;</li> <li>Reduces compute cost for huge models.</li> </ul>"},{"location":"day2/parallelization_schemes/#hybrid-3d-parallelism","title":"Hybrid / 3D parallelism","text":"<ul> <li>For really large models one need to combine the technique;</li> </ul>"},{"location":"day2/parallelization_schemes/#implementations","title":"Implementations","text":""},{"location":"day2/parallelization_schemes/#distributed-computing-mpi","title":"Distributed computing - MPI","text":"<pre><code># just a simple script\n...\nhvd.init()\nmodel = Model(...)\noptimizer = hvd.DistributedOptimizer()\nmodel.compile( ... )\nmodel.fit( ... )\n...\n\n# and run in a job script\nsrun python train.py\n</code></pre> <ul> <li>General-purpose HPC communication;</li> <li>Integrates well with the cluster;</li> <li>Not so popular in AI/ML.</li> </ul>"},{"location":"day2/parallelization_schemes/#distributed-computing-ray","title":"Distributed computing - Ray","text":"<pre><code>ray.init()\n\n@ray.remote\ndef preprocess(data):\n    ...\n\n@ray.remote\ndef train(model_name, dataset):\n    ...\n\ncleaned = [preprocess.remote(...) for data in dataset ]\ntrained_models = [train.remote(...) for data in cleand]\n\nresults = ray.get(trained_models)\n</code></pre> <ul> <li>Python-native distributed orchestration;</li> </ul>"},{"location":"day2/parallelization_schemes/#distributed-computing-ray-cont","title":"Distributed computing - Ray (cont.)","text":"<pre><code># start ray head\nsrun -J \"head ray node-step-%J\" \\\n  apptainer exec ${SIF_IMAGE} ${RAY_CMD_HEAD} &amp;\nRAY_HEAD_PID=$!\n\n# start ray worker\nsrun -J \"worker ray node-step-%J\" \\\n  apptainer exec ${SIF_IMAGE} ${RAY_CMD_WORKER} &amp;\nsleep 10\n\n# start the actual script\napptainer exec ${SIF_IMAGE} vllm serve ${HF_MODEL} \\\n  --port ${API_PORT} ${vllm_opts}\n</code></pre> <ul> <li>Run in server/client mode;</li> <li>Needs more work to configure.</li> </ul>"},{"location":"day2/parallelization_schemes/#popular-training-frameworks","title":"Popular training frameworks","text":"<ul> <li>PyTorch DDP: standard (basic) DP in PyTorch;</li> <li>PyTorch FSDP: improved with sharding;</li> <li>DeepSpeed: implements advanced schemes;</li> <li>Megatron-LM: Nvidia's  of 3D parallelism; </li> <li>Other options: Colossal-AI, FairScale, ...</li> </ul>"},{"location":"day2/parallelization_schemes/#popular-inference-frameworks","title":"Popular inference frameworks","text":"<ul> <li>vLLM: PagedAttention and dynamic batching;</li> <li>DeepSpeed: fused attention;</li> <li>Triton: NVIDIA\u2019s inference platform;</li> <li>Other frameworks: ???</li> </ul>"},{"location":"day2/parallelization_schemes/#summary","title":"Summary","text":""},{"location":"day2/parallelization_schemes/#take-home-message","title":"Take home-message","text":"<ul> <li>Enough memory: use data parallelism;</li> <li>On a single node: prefer tensor parallelism;</li> <li>On many nodes: user pipeline parallelism;</li> <li>For training: Ultrascale playbook.</li> <li>For inference: use a inference engine.</li> </ul>"},{"location":"day2/rag/","title":"Retrieval Augmented Generation (RAG)","text":"Learning outcomes <ul> <li>Understand the key components of RAG applications by looking at what popular open-source RAG libraries provide</li> <li>Perform a simple RAG task </li> </ul> <ul> <li> <p>LLMs are not trained on your personal data or fairly recent data.</p> </li> <li> <p>RAG can help provide richer and accurate responses based on external knowledge.</p> </li> <li> <p>It incurs significantly lower computation cost compared to long-context LLMs.</p> </li> <li> <p>We will learn RAG through the lens of popular open-source RAG libraries, viz. LangChain and LlamaIndex. </p> </li> </ul>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#basics","title":"Basics","text":"RAG figures Naive RAG Naive Retrieval System","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#stages-in-rag","title":"Stages in RAG \ud83d\udd04","text":"<p>Atomic unit</p> <ul> <li>LangChain's atomic unit is a Document.</li> <li>LlamaIndex's atomic unit is a Node. A collection of Nodes consitutes a Document.</li> </ul> <ol> <li> <p>LlamaIndex ex.: SimpleDirectoryReader class LangChain ex.: document_loaders module, langchain_text_splitters module</p> </li> <li> <p>LlamaIndex ex.: VectorStoreIndex class LangChain ex.: Embeddings class</p> </li> <li> <p>LlamaIndex ex.: StorageContext LangChain ex.: VectorStore</p> </li> <li> <p>LlamaIndex ex.: RetrieverQueryEngine class LangChain ex.: Retriever class  </p> </li> <li> <p>LlamaIndex ex.: LLM-Evaluator LangChain ex.: LangSmith, QAEvalChain</p> </li> <li> <p>Retrieval techniques</p> </li> <li>QA/chat</li> <li>Misc: Reranker model, GraphRAG, RAPTOR, EraRAG, multimodal</li> </ol> <p>https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction</p> Resources \ud83d\udcda <ul> <li> <p>Recommended papers on RAG:</p> <ul> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li> <li>REALM: Retrieval-Augmented Language Model Pre-Training</li> <li>Dense Passage Retrieval for Open-Domain Question Answering</li> <li>Improving language models by retrieving from trillions of tokens</li> </ul> </li> <li> <p>Popular libraries and software suite:</p> <ul> <li>llama_index</li> <li>LangChain</li> <li>RAGFlow</li> </ul> </li> </ul> <p>When and when not to use RAG \u2696\ufe0f</p> <ul> <li> <p>It was found<sup>1</sup> that RAG lags behing Long-Context LLMs in the following scenarios: (1)</p> <ul> <li>Query requiring multi-step reasoning.</li> <li>General queries to which embeddings model does not perform well.</li> <li>Long and complex queries.</li> <li>Implicit queries requiring the reader to connect the dots.</li> </ul> </li> <li> <p>Way easier than just fine-tuning on personal data.</p> </li> <li>Allows smaller models with shorter context memory to be on par with larger models. Therefore, saving compute and memory cost on GPUs.</li> </ul> <p> </p> <ol> <li> </li> </ol>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#loading","title":"Loading \ud83d\udce5","text":"<p>Loading/Parsing data from source and creating well-formatted Documents with metadata. (1) This step includes splitting texts such that it can be embedded into lower dimensions. <pre><code>flowchart LR\n  A[Text/image + metadata] --&gt; B[Chunking/Splitting] --&gt; C[Document]</code></pre> </p> <p>Document parsing sequence</p> <ul> <li>LangChain creates Documents first and then performs chunking.</li> <li>LlamaIndex performs chunking first, that becomes a Node and then creates Documents of multiple Nodes.</li> </ul>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#indexing","title":"Indexing \ud83d\udcca","text":"<p>Creating data structure and/or reducing dimensions of the data for easy querying of data.(2)</p> <pre><code>flowchart LR\n  A[Document] --&gt; B[Embeddings]</code></pre>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#storing","title":"Storing \ud83d\udcbe","text":"<p>Storing Documents, metadata and embeddings in a persistant manner (Ex. Vector Stores). (3)</p> <pre><code>flowchart LR\n  A[Document] --&gt; C[Vector Store/Storage Context]\n  B[Embeddings] --&gt; C</code></pre>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#querying","title":"Querying \u2753","text":"<p>Retrieving relavent Documents for a user Query and feeding it to LLM for added context. (4)</p> <pre><code>flowchart LR\n  A[Vector Store/Storage Context] --&gt; D[LLM + tools]\n  B[Query] --&gt; D\n  C[Prompt] --&gt; D\n  D --&gt; E[Response]</code></pre>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#evaluation","title":"Evaluation \ud83d\udcc8","text":"<p>Trace inspection, meterics, comparisons to test if full pipeline gives desired results. (5)</p>","tags":["rag","retrieval augmented generation"]},{"location":"day2/rag/#agentic-rag","title":"Agentic RAG \ud83e\udd16","text":"<ul> <li>An LLM-powered agent decides when and how to retrieve during reasoning. This gives more flexibility in the decision making process by the system but low control over it by the engineer.</li> <li>Router</li> <li>Tool calling</li> <li>Multistep reasoning with tools</li> </ul> <p>(More about Agents will be covered in Day 3.)</p> <p>https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/yd6nd/introduction</p> <p>Some more useful techniques in retrieval pipelines \ud83d\udee0\ufe0f</p> <ul> <li>Rerankers<ul> <li>article on rerankers</li> </ul> </li> <li>GraphRAG (Knowledge graphs?)</li> <li>RAPTOR</li> <li>EraRAG</li> </ul> <ol> <li> <p>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach arXiv \u21a9</p> </li> </ol>","tags":["rag","retrieval augmented generation"]},{"location":"day3/evaluation_metrics/","title":"Evaluation metrics","text":""},{"location":"day3/fine_tuning/","title":"Post-training","text":"<p>https://arxiv.org/html/2408.13296v1#Ch5.S1</p> <ul> <li>Consists of Fine-tuning and/or RL.</li> </ul> <p>Do you even need to post-train your model? * When simple prompting and instruction following fails. * Query KB using RAG if KB keeps changing. * Domain knowelege is needed heavily and base model does not perform well? Continue your pre-training for inject more knowledge. * BUT if you want model to follow almost all your instructions tightly and have improved targeted capabilities, reasoning, use fine-tuning/post-training.</p>","tags":["fine tuning","lora","qlora"]},{"location":"day3/fine_tuning/#fine-tuning","title":"Fine-tuning","text":"<p>https://learn.deeplearning.ai/courses/post-training-of-llms/lesson/ynmgf/introduction-to-post-training https://learn.deeplearning.ai/courses/post-training-of-llms/lesson/erg07/basics-of-sft</p>","tags":["fine tuning","lora","qlora"]},{"location":"day3/fine_tuning/#rl","title":"RL","text":"<p>https://learn.deeplearning.ai/courses/post-training-of-llms/lesson/jeg0d/basics-of-online-rl</p>","tags":["fine tuning","lora","qlora"]},{"location":"day3/hyperparameter_tuning/","title":"Hyperparameter tuning","text":"<ul> <li>What could be tuned?</li> <li>Overview of basic tuning techniques:<ul> <li>Grid search (not recommended)</li> <li>Random search</li> <li>BayesOpt, ... (what others, don't need full coverage)</li> </ul> </li> <li>Simple example with job-arrays</li> <li>(For small tasks use HTC framework like HyperQueue)</li> <li>More complex example using Optuna</li> </ul>"},{"location":"day3/prompt_engineering/","title":"Prompt Engineering","text":"<p>https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/dfbds/introduction</p> <p>A few words about Context Engineering</p> <ul> <li>Differs from Prompt Engineering as we are more careful at filling the context window of our LLMs in industrial application settings.</li> <li>This requires careful design of the following components:<ul> <li>System prompt/instructions</li> <li>User inputs</li> <li>Short term memory or chat history</li> <li>Long term memory</li> <li>Information retrieval from knowledge base</li> <li>Tools and their definitions</li> <li>Responses from tools</li> <li>Structured outputs</li> <li>Global state/context</li> </ul> </li> <li>Read more here</li> </ul>","tags":["prompt","engineering"]},{"location":"day3/tools/","title":"Tools and reasoning (Should this just focus on MCPs instead?)","text":"<ul> <li>Overview of tools and function calling, examples<ul> <li>Claude plays pokemon</li> <li>Web search</li> <li>Code execution (perhaps not include this)</li> <li>Database queries</li> <li>Image generation</li> </ul> </li> <li>What is needed to set something like this up?<ul> <li>MCP</li> </ul> </li> <li>Excercise: Set-up tool use through prompting (or have an already tuned model?)</li> <li>Reasoning<ul> <li>Chain of thought / Tree of thought https://www.promptingguide.ai/techniques/tot</li> <li>DeepSeek-R1 style reasoning</li> </ul> </li> <li>What is needed to set something like this up?</li> <li>Excercise: Set-up ToT through prompting</li> </ul>"},{"location":"evaluations/202511/day1/questions/","title":"Questions","text":"<p>Pre and post course survey quetions per schedule item: * Overview of the landscape and proper resources     * I understand the overall architecture of Alvis compute cluster     * I know some history and origins of current generative model with attention mechanism * Brief introduction to public LLMs     * I understand the categories in which LLMs are released     * I know what to look for when selecting a LM for my use case  * Accessing LLMs / Quick inference     * I know the tools available on Alvis to quick start with LLMs     * I can perform inference using LLM inference engine/server * LLM and hardware     * I know what makes LLMs compute heavy     * I know what makes LLMs memory heavy     * I know how LLMs perform next token predictions in general     * I know some tricks used to reduce computation and memory footprint in LLMs * LLM formats     * I know understand LLM nomenclature     * I know commonly used formats in which LLMs are saved on disk     * I can interpret quantization types in LLMs     * I know which GPU to go for which type of quantization</p> <p>Pre course survey questions only: * I can navigate inside NAISS project dirs * I can launch interactive sessions * I can submit batch scripts * I can build and use containers * I can work (create/activate/modify) with a python virtual env * I can code in Python: beginner, intermediate, proficient * I have used ML/DL frameworks and libraries (TF/PyTorch/JAX/HF etc.): Yes, No, Not sure</p> <p>Post course survey questions only:  * I would recommend this workshop to others : Yes, No, Not sure</p>"},{"location":"evaluations/202511/day2/questions/","title":"Questions","text":"<p>Pre and post course survey questions per schedule item: * Datapipeline     * I know the common stages of pre-training pipeline     * I know the common stages of post-training pipeline * How to quantize * Model parallelism     * I know different ways to parallelize LLM training     * I know the tools to perform parallelization * Multi-modalities     * I can use inference engine/server to infer from multimodal LLMs * RAG     * I know the common stages of a simple RAG     * I know when to use RAG for my usecase     * I know how RAG is used by AI Agents</p> <p>Pre course survey questions only: * I can navigate inside NAISS project dirs * I can launch interactive sessions * I can submit batch scripts * I can build and use containers * I can work (create/activate/modify) with a python virtual env * I can code in Python: beginner, intermediate, proficient * I have used ML/DL frameworks and libraries (TF/PyTorch/JAX/HF etc.): Yes, No, Not sure</p> <p>Post course survey questions only:  * I would recommend this workshop to others : Yes, No, Not sure</p>"},{"location":"evaluations/202511/day3/questions/","title":"Questions","text":"<p>Pre and post course survey quetions per schedule item: * Prompt engineering     * I understand the differences in context and prompt engineering     * I can optimize my prompt for the LLM I am using * Tools, chain/tree-of-thought     * I know how LLMs make use of tools     * I know how LLMs can be forced to \"think\" * Finetuning, RL     * I know different finetuning techniques for LLMs     * I can finetune an LLM on a dataset * HyperParameter tuning     * I know what various hyperparameters mean     * I know how to optimize my hyperparameter selection * Evaluation metrics     * I broadly know which evaluation metrics to evaluate on for my task     * I can evaluate my finetuned LLM</p> <p>Pre course survey questions only: * I can navigate inside NAISS project dirs * I can launch interactive sessions * I can submit batch scripts * I can build and use containers * I can work (create/activate/modify) with a python virtual env * I can code in Python: beginner, intermediate, proficient * I have used ML/DL frameworks and libraries (TF/PyTorch/JAX/HF etc.): Yes, No, Not sure</p> <p>Post course survey questions only:  * I would recommend this workshop to others : Yes, No, Not sure</p>"}]}